textes
"Please post your personal projects, startups, product placements, collaboration needs, blogs etc.Please mention the payment and pricing requirements for products and services.Please do not post link shorteners, link aggregator websites , or auto-subscribe links.\--Any abuse of trust will lead to bans.Encourage others who create new posts for questions to post here instead!Thread will stay alive until next one so keep posting after the date in the title.\--Meta: This is an experiment. If the community doesnt like this, we will cancel it. This is to encourage those in the community to promote their work by not spamming the main threads..
**For Job Postings** please use this template>Hiring: \[Location\], Salary:\[\], \[Remote | Relocation\], \[Full Time | Contract | Part Time\]    and \[Brief overview, what you're looking for\]**For Those looking for jobs** please use this template>Want to be Hired: \[Location\], Salary Expectation:\[\], \[Remote | Relocation\], \[Full Time | Contract | Part Time\]  Resume: \[Link to resume\] and \[Brief overview, what you're looking for\]&#x200B;Please remember that this community is geared towards those with experience..
I've been working on a gradient boosting implementation that handles two problems I kept running into with XGBoost/LightGBM in production:1. **Performance collapse on extreme imbalance (under 1% positive class)**2. **Silent degradation when data drifts (sensor drift, behavior changes, etc.)**Key ResultsImbalanced data (Credit Card Fraud - 0.2% positives):**- PKBoost: 87.8% PR-AUC****- LightGBM: 79.3% PR-AUC****- XGBoost: 74.5% PR-AUC**Under realistic drift (gradual covariate shift):\- PKBoost: 86.2% PR-AUC (‚àí2.0% degradation)\- XGBoost: 50.8% PR-AUC (‚àí31.8% degradation)\- LightGBM: 45.6% PR-AUC (‚àí42.5% degradation) **What's Different**The main innovation is using Shannon entropy in the split criterion alongside gradients. Each split maximizes:Gain = GradientGain + Œª¬∑InformationGainwhere Œª adapts based on class imbalance. This explicitly optimizes for information gain on the minority class instead of just minimizing loss.Combined with:\- Quantile-based binning (robust to scale shifts)\- Conservative regularization (prevents overfitting to majority)\- PR-AUC early stopping (focuses on minority performance)The architecture is inherently more robust to drift without needing online adaptation. Trade-offsThe good:\- Auto-tunes for your data (no hyperparameter search needed)\- Works out-of-the-box on extreme imbalance\- Comparable inference speed to XGBoostThe honest:\- \~2-4x slower training (45s vs 12s on 170K samples)\- Slightly behind on balanced data (use XGBoost there)\- Built in Rust, so less Python ecosystem integration Why I'm SharingThis started as a learning project (built from scratch in Rust), but the drift resilience results surprised me. I haven't seen many papers addressing this - most focus on online learning or explicit drift detection.Looking for feedback on:\- Have others seen similar robustness from conservative regularization?\- Are there existing techniques that achieve this without retraining?\- Would this be useful for production systems, or is 2-4x slower training a dealbreaker? Links\- GitHub: [https://github.com/Pushp-Kharat1/pkboost](https://github.com/Pushp-Kharat1/pkboost)\- Benchmarks include: Credit Card Fraud, Pima Diabetes, Breast Cancer, Ionosphere\- MIT licensed, \~4000 lines of RustHappy to answer questions about the implementation or share more detailed results. Also open to PRs if anyone wants to extend it (multi-class support would be great).\---**Edit**: Built this on a 4-core Ryzen 3 laptop with 8GB RAM, so the benchmarks should be reproducible on any hardware.**Edit**: The Python library is now avaible for use, for furthur details, please check the Python folder in the Github Repo for Usage, Or Comment if any questions or issues.
Hi all!I was asked to review a paper about application of ML to Parkinson's disease diagnosis.I have spotted some weak points, but I wouls like to know what would you look at when reviewing a ML paper.Thank you very much in advance!!.
Hey everyone,As you might know, the CVPR deadline is getting close, and I‚Äôm planning to submit there for the first time. I‚Äôd really appreciate any advice on how to approach the writing, what are the best styles, tones, or structures that make a strong impression?Also, if you have tips on how to present the ‚Äústory‚Äù of the paper effectively, I‚Äôd love to hear them.Thanks in advance!.
I‚Äôve been diving into how people build datasets for code-related ML research ‚Äî things like program synthesis, code reasoning, SWE-bench-style evaluation, or DPO/RLHF.From what I‚Äôve seen, **most projects still rely on scraping or synthetic generation, with a lot of manual cleanup and little reproducibility.**Even published benchmarks vary wildly in annotation quality and documentation.So I‚Äôm curious:1. How are you collecting or validating your datasets for code-focused experiments?2. Are you using public data, synthetic generation, or human annotation pipelines?3. What‚Äôs been the hardest part ‚Äî scale, quality, or reproducibility?**I‚Äôve been studying this problem closely and have been experimenting with a small side project to make dataset creation easier for researchers (happy to share more if anyone‚Äôs interested).**Would love to hear what‚Äôs worked ‚Äî or totally hasn‚Äôt ‚Äî in your experience :) .
Google have just announced the 2025 recipients. What are the criteria to get this fellowship?[https://research.google/programs-and-events/phd-fellowship/recipients/](https://research.google/programs-and-events/phd-fellowship/recipients/).
Hey everyone... I‚Äôm exploring an idea: an **AI that lets you build, debug, and update ML models by chatting** ‚Äî like a *Copilot for ML engineers* or a *no-code ML builder* for non-tech users.After talking to a few ML devs, feedback was split ‚Äî some find it useful, others say ‚Äúeveryone‚Äôs just using LLMs and RAG now.‚ÄùCurious what you think:* Do you still face pain maintaining or improving traditional ML models?* Would a conversational AI that handles data cleaning, training, and tuning help?Honest takes appreciated :).
I am just curious for working on World Models. Do we always require robot intervention  or it can be done via only training and testing data? I want to select this topic for phd research.  Does anyone give me suggestion? how they look into this domain?.
Wanted to share some learnings we had optimizing and deploying Qwen-Image-Edit at scale to replace Nano-Banana. The goal was to generate a product catalogue of 1.2m images, which would have cost $46k with Nano-Banana or GPT-Image-Edit.Qwen-Image-Edit being Apache 2.0 allows you to fine-tune and apply a few tricks like compilation, lightning lora and quantization to cut costs.The base model takes \~15s to generate an image which would mean we would need 1,200,000\*15/60/60=5,000 compute hours.Compilation of the PyTorch graph + applying a lightning LoRA cut inference down to \~4s per image which resulted in \~1,333 compute hours.I'm a big fan of open source models, so wanted to share the details in case it inspires you to own your own weights in the future.[https://www.oxen.ai/blog/how-we-cut-inference-costs-from-46k-to-7-5k-fine-tuning-qwen-image-edit](https://www.oxen.ai/blog/how-we-cut-inference-costs-from-46k-to-7-5k-fine-tuning-qwen-image-edit).
Hello i am a student currently working on my project skin cancer multiclass classification using clinical images(non-dermascopic) and have merged clinical images from 3 datasets(pad ufes,milk 10k,HIBA dataset) but the issue is that i am really stuck as i cant get the scores above 0.60 recall for some class and other is  stuck at 0.30. i dont know if this is a cleaning issue or not choosing the optimum augmentation techniques and the model. It would bereally helpfull if i could get some help thankyou!.
Hey everyoneI‚Äôm exploring the idea of setting up a GPU cluster in Angola to provide affordable AI compute (A100s and 5090s). Power costs here are extremely low, and there‚Äôs direct Tier-3 connectivity to South America and Europe, mostly southern below 100 ms.Before going further, I wanted to gauge interest would researchers, indie AI teams, or small labs consider renting GPU time if prices were around 30‚Äì40 % lower than typical cloud platforms?For US users running batching, scraping, or other non real time workloads where latency isn‚Äôt critical but cost efficiency is.Still early stage, just trying to understand the demand and what kind of workloads people would actually use it for. Any feedback is a must, ty..
This week I made a series of adjustments, including making the environment's core compatible with Libretro cores, which are software renderers. Now you can train Reinforcement Learning with PS2, Wii, Game Cube, PS1, SNES, and other games!If anyone is interested in collaborating, we're open to ideas!!! And also to anyone who wants to code ;)Here's the link to the repository: [https://github.com/paulo101977/sdlarch-rl](https://github.com/paulo101977/sdlarch-rl)Here's the link to my channel: [https://www.youtube.com/@AIPlaysGod?sub\_confirmation=1](https://www.youtube.com/@AIPlaysGod?sub_confirmation=1).
Built a side project to solve GPU sharing conflicts in the lab: **Chronos****The problem**: 1 GPU, 5 grad students, constant resource conflicts.**The solution**: Time-based partitioning with auto-expiration.    from chronos import Partitioner        with Partitioner().create(device=0, memory=0.5, duration=3600) as p:        train_model()  # Guaranteed 50% GPU for 1 hour, auto-cleanup\- Works on any GPU (NVIDIA, AMD, Intel, Apple Silicon)\- < 1% overhead\- Cross-platform\- Apache 2.0 licensed**Performance:** 3.2ms partition creation, stable in 24h stress tests.Built this weekends because existing solutions . Would love feedback if you try it!**Install**: pip install chronos-gpu**Repo**: [github.com/oabraham1/chronos](http://github.com/oabraham1/chronos).
GPTQ is a simplified modification of the OBQ method where the weights in a matrix are quantized in each row independently one at a time from left to right. After step `i` of quantization, the remaining unquantized weights are modified like so: `dW[i:] = H[i:,i] dW[i]/H[i,i]`. This expression is derived by forming a Lagrangian and setting its gradient to 0.Another way to approach this problem is by using the Cholesky decomposition `L` of the Hessian `H = L @ L.t()` directly in the bilinear error term: `df = 1/2 * dw^T H dw = 1/2 ||L^T dW||^2`. Thus minimizing the error term is equivalent to minimizing the squared norm of `L^T dW`. This squared norm can be converted into a form `||a + Mx||^2` where `x` is the vector of unquantized weights. This function is minimized when `Mx` equals the negative of projection of `a` in the column space of `M`. This provides a geometric interpretation of the weight update: **the optimal update negates the projection of the error vector in the column space `L`**. This approach also leads to a new closed form solution that is different from the one above. However it can be shown that both the forms are equivalent.Full details are available [in this article](https://www.linearalgebraforprogrammers.com/blog/new_proof_gptq)..
Wanted to know which software packages/frameworks you guys use for object detection research. I mainly experiment with transformers (dino, detr, etc) and use detrex and dectron2 which i absolutely despise. I am mainly looking for an alternative that would allow me to make architecture modification and changes to the data pipeline in a quicker less opinionated manner.
Given a word embedding space, I would like to measure how 'substitutable' a word is. Put more formally, how many other embedding vectors are very close to the query word's vector? I'm not sure what the problem I'm describing is called.Maybe I need to measure how dense a query vector's surrounding volume is? Or maybe I just need the mean/median of all the distances from all the vectors to the query vector. Or maybe I need to sort the distances of all the vectors to the query vector and then measure at what point the distances tail off, similar to the elbow method when determining the optimal number of clusters.I'm also not sure this is exactly the same as clustering all the vectors first and then measuring how dense the query vector's cluster is, because the vector might be on the edge of its assigned cluster..
Has anyone heard about this conference: [https://www.aaiml.net](https://www.aaiml.net) ?   I found it on IEEE, but I cannot find anything on this conference.  Any information regarding this conference, e.g., ranking/level, acceptance rate, is appreciated, thank you!.
Hi, I fine-tuned a Helsinki Transformer for translation tasks and it runs fine locally.  A friend made a Flutter app that needs to call it via API, but Hugging Face endpoints are too costly.  I‚Äôve never hosted a model before what‚Äôs the easiest way to host it so that the app can access it?  Any simple setup or guide would help!.
I've developed a benchmark that measures AI architectural complexity (not just task accuracy) using 4 neuroscience-derived parameters.\*\*Key findings:\*\*\- Models with identical MMLU scores differ by 29% in architectural complexity\- Methodology independently validated by convergence with clinical psychiatry's ""Thought Hierarchy"" framework\- Claude Sonnet 4 (0.7845) ranks highest in processing complexity, despite GPT-4o having similar task performance\*\*Results across 10 frontier models:\*\*1. Claude Sonnet 4: 0.78452. GPT-4o: 0.76233. Gemini 2.5 Pro: 0.74014. Grok 2: 0.71565. Claude Opus 3.5: 0.70896. Llama 3.3 70B: 0.69347. GPT-4o-mini: 0.67128. DeepSeek V3: 0.59349. Gemini 1.5 Flash: 0.582310. Mistral Large 2: 0.5645\*\*Framework measures 4 dimensions:\*\*\- Capability (processing capacity)\- Meta-cognitive sophistication (self-awareness/reasoning)\- Adversarial robustness (resistance to manipulation)\- Integration complexity (information synthesis)\*\*Why this matters:\*\*Current benchmarks are saturating (MMLU approaching 90%+). UFIPC provides orthogonal evaluation of architectural robustness vs. task performance - critical for real-world deployment where hallucination and adversarial failures still occur despite high benchmark scores.\*\*Technical validation:\*\*Independent convergence with psychiatry's established ""Thought Hierarchy"" framework for diagnosing thought disorders. Same dimensional structure emerges from different fields (AI vs. clinical diagnosis), suggesting universal information processing principles.Open source (MIT license for research), patent pending for commercial use (US 63/904,588).Looking for validation/feedback from the community. Happy to answer technical questions about methodology.\*\*GitHub:\*\* [https://github.com/4The-Architect7/UFIPC](https://github.com/4The-Architect7/UFIPC).
Working with text-to-3D models and hitting a fundamental issue that's confusing me. Interpolating between different objects in latent space produces geometrically impossible results.Take ""wooden chair"" to ""metal beam"". The interpolated mesh has vertices that simultaneously satisfy chair curvature constraints and beam linearity constraints. Mathematically the topology is sound but physically it's nonsense.This suggests something wrong with how these models represent 3D space. We're applying continuous diffusion processes designed for pixel grids to discrete geometric structures with hard constraints.Is this because 3D training data lacks intermediate geometric forms? Or is forcing geometric objects through continuous latent mappings fundamentally flawed? The chair-to-beam path should arguably have zero probability mass in real space.Testing with batch generations of 50+ models consistently reproduces this. Same interpolation paths yield same impossible geometry patterns.This feels like the 3D equivalent of the ""half-dog half-cat"" problem in normalizing flows but I can't find papers addressing it directly..
We have been exploring how signal processing principles, traditionally used in communication systems to extract meaningful information from noisy data, can be applied to AI models and embedding spaces to make them more efficient and accurate.We're presenting this work in collaboration with **Prof. Gunnar Carlsson** (Stanford Mathematics Emeritus, pioneer in topological data analysis), showing how signal processing can complement modern AI architectures.üìç **Event details:** https://luma.com/rzscj8q6&nbsp;As a first application to ANN search, we achieved 10x faster vector search than current solutions. If vector databases interest you, here's the technical note and video:[**Traversal is Killing Vector Search ‚Äî How Signal Processing is the Future**](https://www.linkedin.com/pulse/traversal-killing-vector-search-how-signal-processing-yusuf-motiwala-s4tic/)&nbsp;If this interests you and you are in the Bay Area, we'd love to have you join the event and discuss how signal processing could shape the next wave of AI systems. We had some great discussions at PyTorch Conference over the last two days.We'll also be at **TechCrunch Disrupt 2025** if you'd like to meet and brainstorm there..
**The paper highlights** its ""Contexts Optical Compression"" module, which compresses visual tokens between the vision encoder and the MoE language decoder. They show impressive results, like 97% OCR precision even with <10x compression (original vision tokens vs. compressed ones) and \~60% at 20x.  **My take \[D\]:** The compression of visual tokens in the latent space is not a new thing it is was done in the VLMs previously. I guess back than the compression was not the main focus, in this paper the focus was on 10x compression. And this gave the AI community idea to compress the input context of LLMs by representing it in image and compressing the image in latent space which could be much more dense as compared to text where the structure is constraint by tokens as the lowest compressed form.But can't we just compress the text tokens by training an autoencoder and using the encoder to generate the latent space lower dimensional embeddings.**Would love to hear what others think****Paper link:** [**https://www.arxiv.org/pdf/2510.18234**](https://www.arxiv.org/pdf/2510.18234).
The web wasn't built for AI agents. It was built for humans with eyes, mice, and 25 years of muscle memory navigating dropdown menus.    Most AI companies are solving this with browser automation, playwright scripts, Selenium wrappers, headless Chrome instances that click, scroll, and scrape like a human would.It's a workaround and it's temporary.These systems are slow, fragile, and expensive. They burn compute mimicking human behavior that AI doesn't need. They break when websites update. They get blocked by bot detection. They're architectural debt pretending to be infrastructure etc.The real solution is to build web access designed for how AI actually works instead of teaching AI to use human interfaces.¬†A few companies are taking this seriously. Exa or Linkup are rebuilding search from the ground up for semantic / vector-based retrieval Linkup provides structured, AI-native access to web data. Jina AI is building reader APIs for clean content extraction. Shopify in a way tried to address this by exposing its APIs for some partners (e.g., Perplexity)The web needs an API layer, not better puppeteering.As AI agents become the primary consumers of web content, infrastructure built on human-imitation patterns will collapse under its own complexity‚Ä¶.
EDIT: this is really a question about the diffeomorphicity of continuous normalising flows and whether that is problematic (not about pictures of animals!)Continuous normalising flows push a source distribution to a target distribution via a diffeomorphism (usually an automorphism of d-dimensional Euclidean space). I'm confused about sparsely sampled parts of the data distribution and whether the fact that the diffeomorphic mapping is assuming things about the data distribution (e.g. its connectivity) that aren't actually true (is it modelling the distribution too coarsely or is it learning the true distribution?).E.g. let's say the data distribution has a lot of pictures of dogs and a lot of pictures of cats but no pictures of ""half dogs-half cats"" because they don't actually exist (note that there may be pictures of dogs that looks like cats but would sit in the cat picture part of the distribution -- dogcats do not exist in the real world). But the region in between the peaks of this bimodal distribution should be zero. But when we perform a diffeomorphic mapping from the source p (e.g., a Gaussian) part of the probability mass must be pushed to the intermediate part of the distribution. This is problematic because then we sample our q (by sampling p and pushing through the learned flow) we might end up with a picture of a halfdog-halfcat but that isn't physically possible.What is going wrong here?1. Is the assumption that our map is a diffeomorphism too restrictive, e.g., for topologically disconnected data distributions?OR2. Is the model faithfully learning what the intermediate regions of the data distribution look like? That seems magical because we haven't given it any data and in the example I've given it's impossible. Rather the diffeomorphic assumption gives us an intermediate part of the distribution that might be wrong because the true target distribution is topologically disconnected.It seems of paramount importance that we know a priori about the topological structure of the data distribution -- no?If you know any sources discussing this, that would be very helpful!Many thanks![I'm interested in the intermediate region between the peaks](https://preview.redd.it/exchxfoolpwf1.png?width=870&format=png&auto=webp&s=9e2f0d950f589cdc81044707e6a3498fefec2c51)[samples from the source distribution p \(e.g. Gaussian\) at t=0](https://preview.redd.it/rcafr6orlpwf1.png?width=568&format=png&auto=webp&s=136bfbd3136b90baddd5974823a0f6b00c35a43a)[mid way through the flow 0\<t\<1](https://preview.redd.it/3iyuefvslpwf1.png?width=550&format=png&auto=webp&s=d51c4a7c2da085b4893bfd9927d3b238093a25ea)[The target distibution q at t=1. I'm interested in the middle part of the distribution between the two peaks](https://preview.redd.it/5zn4v2ktlpwf1.png?width=557&format=png&auto=webp&s=5d8d5e0887cf0cb1b7ad2f6190b53e633bc7dd25).
During the training of a neural network, a very common phenomenon is that of loss spikes, which can cause large gradient and destabilize training. Using a learning rate schedule with warmup, or clipping gradients can reduce the loss spikes or reduce their impact on training.However, I realised that I don't really understand why there are loss spikes in the first place. Is it due to the input data distribution? To what extent can we reduce the amplitude of these spikes? Intuitively, if the model has already seen a representative part of the dataset, it shouldn't be too surprised by anything, hence the gradients shouldn't be that large.Do you have any insight or references to better understand this phenomenon?.
**TL;DR**: I compress LLM context into **images** instead of text, and let a **vision-language model** (VLM) ‚Äúdecompress‚Äù it by reading the image. In my tests, this yields up to **\~2.8:1 token compression at 93.65% accuracy** on *Gemini 2.5-Flash-Lite (Exp 56)*, and **99.26% at 1.7:1** on *Qwen2.5-VL-72B-Instruct (Exp 34)*. Full code, experiments, and replication steps are open-source.**Repo (please ‚≠ê if useful):** [https://github.com/MaxDevv/Un-LOCC](https://github.com/MaxDevv/Un-LOCC)# What this is:**Un-LOCC (Universal Lossy Optical Context Compression)**: a simple, general method to **encode long text context into compact images**, then **decode with a VLM**. Think of the VLM as an OCR-plus semantic decompressor.* I render text into a fixed-size PNG (e.g., **324√ó324**, Atkinson Hyperlegible \~**13px**), pass that image to a VLM, and ask it to reproduce the original text.* **Accuracy** = normalized Levenshtein similarity (%).* **Compression ratio** = *text tokens √∑ image tokens*.# Key results (linked to experiments in the repo):* **Gemini 2.5-Flash-Lite**: **100% @ 1.3:1** *(Exp 46)* and **\~93.65% @ 2.8:1** *(Exp 56)*.* **Qwen2.5-VL-72B-Instruct**: **99.26% @ 1.7:1** *(Exp 34)*; **\~75.56% @ 2.3:1** *(Exp 41)*.* **Qwen3-VL-235B-a22b-Instruct**: **95.24% @ 2.2:1** *(Exp 50)*; **\~82.22% @ 2.8:1** *(Exp 90)*.* **Phi-4-Multimodal**: **94.44% @ 1.1:1** *(Exps 59, 85)*; **\~73.55% @ 2.3:1** *(Exp 61)*.* **UI-TARS-1.5-7B**: **95.24% @ 1.7:1** *(Exp 72)*; **\~79.71% @ 1.7:1** *(Exp 88)*.* **LLaMA-4-Scout**: **86.57% @ 1.3:1** *(Exp 53)*.>Details, prompts, fonts, and measurement code are in the README. I cite each claim with **(Exp XX)** so you can verify quickly.# Why this matters:* **Cheaper context**: replace expensive text tokens with ‚Äúimage tokens‚Äù when a capable VLM sits in the loop.* **Architecturally simple**: no model modifications are needed, you can use rendering + a VLM you already have.* **Composable**: combine with retrieval, chunking, or multimodal workflows.# What I need help with:* **A better algorithm:** The O-NIH algorithm is okay for checking if models can see the text, however I'm not sure how to easily determine the model's full comprehension of the text.* **Model coverage**: more open VLMs; local runs welcome.* **Edge cases**: math, code blocks, long tables, multilingual.* **Repro/PRs**: if you get better ratios or accuracy, please open an issue/PR.**Repo again (and yes, stars genuinely help discoverability):** [https://github.com/MaxDevv/Un-LOCC](https://github.com/MaxDevv/Un-LOCC).
[https://www.scmp.com/tech/tech-trends/article/3328966/ai-powered-fraud-chinese-paper-mills-are-mass-producing-fake-academic-research](https://www.scmp.com/tech/tech-trends/article/3328966/ai-powered-fraud-chinese-paper-mills-are-mass-producing-fake-academic-research)A new CCTV investigation found that paper mills in mainland China are using generative AI to mass-produce forged scientific papers, with some workers reportedly ‚Äúwriting‚Äù more than 30 academic articles per week using chatbots.    These operations advertise on e-commerce and social media platforms as ‚Äúacademic editing‚Äù services. Behind the scenes, they use AI to fabricate data, text, and figures, selling co-authorships and ghostwritten papers for a few hundred to several thousand dollars each.    One agency processed over 40,000 orders a year, with workers forging papers far beyond their expertise. A follow-up commentary in The Beijing News noted that ‚Äúvarious AI tools now work together, some for thinking, others for searching, others for editing, expanding the scale and industrialization of paper mill fraud.‚Äù.
Good talk by Sergey Levine about the current state-of-the-art in robotic foundation models: https://www.youtube.com/watch?v=yp5fI6gufBsTL;DR They use a pretrained VLM, stapled to a diffusion or flow model trained on robotics actions. Reinforcement learning inside the latent space of a diffusion model is surprisingly efficient compared to traditional RL (as few as 50 rollouts with sparse rewards). This works well, but the primary bottleneck is a lack of large action datasets. Much more research and data collection will be necessary to build practical robots..
Hi everyone.For the past couple of weeks I have been playing around with PI0.5 and training it on behavior 1k tasks. I performed a full fine-tuning training run of PI0.5 for 30000 steps with batch size of 32 and it took 30 hours.In order for me to train over 1 epoch of the entire behavior 1k dataset with batch size of 32 I need to perform 3.7 million training steps. This will take around 3700 hours or 154 days which would amount to $8843 ($2.39 for 1 H100).So I decide to optimize the training script to improve the training time and so far I have been able to achieve 1.4x speedup. With some more optimizations 2x speedup is easily achievable. I have added a small video showcasing the improvement on droid dataset.[https://yourimageshare.com/ib/KUraidK6Ap](https://yourimageshare.com/ib/KUraidK6Ap)After a few more optimizations and streamlining the code I am planning to open-source it..
Hi everyone. I'd like to share something I've been working on: Attention-Driven Transformers for time series forecastingThe approach focuses on maximizing attention's representational capacity by using a single top-layer attention block O(n¬≤) to drive multiple lightweight projection blocks O(n), rather than repeating full attention across all blocks. It uses PatchTST's patching algorithm to segment time series into overlapping windows.The core insight is that attention works best as a global organizational mechanism, not necessarily something you need implemented in every block. The model also uses multiplicative positional encoding rather than additive, which scales features by learned positional weights.The architecture consistently improves performance over PatchTST (a SOTA baseline) across standard benchmarks while being 1.3-1.5x faster, with improvements ranging from 1-20% depending on the dataset.Code and full details can be found here: [https://github.com/pfekin/attention-driven-transformers](https://github.com/pfekin/attention-driven-transformers).
We present rBridge, a method that enables small proxy models (‚â§1B parameters) to effectively predict large-model reasoning performance, addressing the emergence problem in reasoning capabilities.**Paper:** [https://www.arxiv.org/abs/2509.21013](https://www.arxiv.org/abs/2509.21013)**Abstract/TL;DR:** Given the prohibitive cost of pre-training large language models, leveraging smaller proxy models to optimize datasets before scaling up is essential. However, reasoning capabilities exhibit emergent behavior only at larger scales (typically >7B parameters), making traditional proxy approaches ineffective. rBridge solves this by aligning evaluation with both (1) the pre-training objective and (2) the target task through weighted negative log-likelihood using frontier model reasoning traces.**Key Contributions:**1. **Theoretical insight:** We identify that proxy evaluation schemes must align with both pre-training objectives and target tasks for effective reasoning prediction2. **Novel method:** rBridge weights NLL by task-alignment using frontier model confidence scores, handling tokenizer mismatches at letter-level3. **Empirical validation:**   * 100.2√ó compute reduction for dataset ranking (80.8% decision accuracy across 25 datasets)   * Strong proxy-target correlations: R¬≤ = 0.826-0.874 across 6 benchmarks (GSM8K, MATH500, ARC-C, MMLU Pro, CQA, HumanEval)   * Zero-shot transfer of fitted functions across pre-training datasets**Experimental Setup:*** Proxy scales: 100M to 1B* Target scales: 7B to 32B* Training corpus: 250B to 3.75T tokens* Evaluation: 5-fold cross-validation**Practical Impact:** This enables compute-constrained researchers to explore pre-training design choices at dramatically reduced costs. A single 7B training run can exceed $50K; our method reduces exploration costs by 100√ó+ while maintaining predictive accuracy.Code will be released soon..
[https://arxiv.org/abs/2402.09267](https://arxiv.org/abs/2402.09267)Very interesting paper I found about how to make LLMS keep themselves in check when it comes to factuality and how to mitigate and reduce hallucinations without the need of human intervention.I think this framework could contribute and give LLMs huge benefits, especially in fields where high factuality confidence and low (or ideally none) hallucinations are needed.Summary: In this work, we explore Self-Alignment for Factuality, where we leverage the self-evaluation capability of an LLM to provide training signals that steer the model towards factuality. .
Quick context: I'm training a playable DOOM world model where you can prompt like ""spawn cyberdemon left"" or ""harder"" to change game events in real time. I wanted to take DeepMind's playable Doom world model in [Diffusion Models are Real-Time Game Engiens](https://arxiv.org/abs/2408.14837), and add text conditioning to make game events promptable.**To train this I need \~100 hours of action-labeled DOOM gameplay data.**I could have scraped DOOM data from YouTube, or paid contractors, but thought it would be fun to train a curious RL agent that explores the map. I thought this would be a solved problem, since I saw RL papers from 2018 about ""curiosity-driven"" learning.I couldn't have been more wrong! Training agents to be ""curious"" is far from a solved problem. Here's what I tried and what happened so far:**1. Implemented the original** [**curiosity-driven exploration**](https://arxiv.org/abs/1705.05363) **paper(Pathak et al., 2018) ‚Üí hit the Noisy TV Problem**The Noisy TV Problem is where the agent gets stuck staring at a random process in the game. This is a known problem with defining the curiosity bonus as prediction error, since noise is not learnable. The specific ""Noisy TV"" the agent converges to is getting transfixed by the pistol's muzzle smoke against a high-contrast white background.**2. Implemented** [Learning Progress Monitoring](https://arxiv.org/pdf/2509.25438v1) **(2025) ‚Üí agent converged to taking no action.**The paper defined curiosity bonus as learning progress: difference between past prediction error of next state and current prediction error of next state. Sounds good on paper, but in practice you have to get a lot right to guarantee past prediction error > current prediction error for learnable (non-random) states. I couldn't figure this out, and past and present prediction error became roughly equal during training, causing agent to take no action due to lack of reward.**3. Implemented OpenAI** [Random Network Distillation](https://arxiv.org/abs/1810.12894) **‚Üí agent learns  but not because of curiosity**The agent learned, but only because of extrinsic rewards (kills, room discovery, etc), not curiosity bonus rewards. After many iterations, curiosity bonus rewards shrank to zero as well, similar to LPM. The agent acts greedily to kill enemies and discover rooms, with little to no variety in its actions.More details here in my repo, where all three implementations work out-of-box: [https://github.com/pythonlearner1025/BoredDoomGuy](https://github.com/pythonlearner1025/BoredDoomGuy)At this point, I reminded myself training a curious RL agent is a side quest, and I have to get back on the main quest. But if you've trained an agent to complete Doom E1M1 purely from curiosity, I'm curious to hear how you did it!For now, I'm falling back to collecting training data from human players. You can help by playing doom in your browser at [playdoom.win](https://www.playdoom.win) your fun is my training data: your game viewport and actions will be logged!.
Hi,Sorry for the non-learning question, but most of the community is here.There's ' upstream request timeout' on OpenReview. Has been for a while.Are you experiencing that too? Do you have an idea on the ETA on the uptime?Appreciated!.
I‚Äôve been digging into how researchers build datasets for code-focused AI work ‚Äî things like program synthesis, code reasoning, SWE-bench-style evals, DPO/RLHF. It seems many still rely on manual curation or synthetic generation pipelines that lack strong quality control.**I‚Äôm part of a small initiative supporting researchers who need custom, high-quality datasets for code-related experiments ‚Äî at no cost. Seriously, it's free.**If you‚Äôre working on something in this space and could use help with data collection, annotation, or evaluation design, I‚Äôd be happy to share more details via DM.Drop a comment with your research focus or current project area if you‚Äôd like to learn more ‚Äî I‚Äôd love to connect..
What bias variance tradeoff teaches us:  We must carefully limit the power of our models to match the complexity of our data to avoid overfitting.  When we make Neural Networks larger it works better which contradicts our bias variance tradeoff which is actually incomplete.Keeping the dataset fixed and no early stopping as we increasing the NN size:When we make a NN larger at the start the performance increases rapidly, than if we continue to make it larger at some point the performance starts to get worse(starts to overfit) and it gets worst exactly at the interpolation point(0 training error/ model has 1:1 correspondence with the dataset). And after this point the test error again start to decrease creating a second descent.To explain its cause:  When model capacity is low you underfit (high bias). As capacity rises toward the **interpolation threshold** (capacity ‚âà training data degrees of freedom) the model can exactly fit the training data, so tiny changes in training data can lead to large fluctuations in the learned parameters and predictions, causing the validation or test error to spike sharply due to high variance.  Before the interpolation point when there is lot more dataset as compared to model complexity, the model learns to ignore the noise and only capture the most relevant patterns as it doesn't have enough parameters.  **Overparameterized region:**  with many more parameters than data, there are infinitely many zero-training-error solutions; optimization (and explicit regularizes like weight decay or implicit biases of SGD) tends to select low-complexity/low-norm solutions, so test error can drop again ->**double descent**..
Hey,When I prepare my NeurIPS submission camera-ready version, I found that the instruction email asks to put the checklist before the appendices.However, in this call for paper page (https://neurips.cc/Conferences/2025/CallForPapers), the LaTex style file actucally put the checklist after the appendices. Personally speaking, putting the checklist before appendices is not aesthetic and elegant. I also check around 30 camera ready NeurIPS papers that got uploaded to arXiv, and only one put the checklist before appendices (although most of the accepted paper don't even include checklist on arXiv version.)I'm just want to check if anyone have any idea how strict these instruction will be? If I put the checklist after appendices, will I get 'reject'? (I guess the chance is very small but just want to double-check). .
When working on various recommender systems, it always was weird to me that creating dashboards or doing feature engineering is hard with integer-valued features that are heavily tailed and have large support, such as # of monthly visits on a website, or # monthly purchases of a product.  So I decided to do a one small step towards tackling the problem. I hope you find it useful:  https://arxiv.org/abs/2510.15132.
ICLR 2026 author guide says max 9 pages of main text in submissions, while FAQ says 10 pages. And [Google shows several such](https://www.google.com/search?q=iclr+2026+page+limit) contradictions in time and space...\[*Edit: screenshot below\]*Vanilla definition of ""main text"" is all content between title and references, except for exempt sections, i.e. ""Ethics"" and ""Reproducibility"" sections per author guide.Random sampling suggests \~5% of the \~20,000 submissions under review have main text on page 10. Would you1. Allow all submissions with main text on page 102. Disallow all submissions with main text on page 103. Subjectively allow/disallow submissions with main text on page 10PS: will adhere to the top-ranked answer in my reviewshttps://preview.redd.it/8zrrrr372rxf1.png?width=865&format=png&auto=webp&s=a426dacca38f5552fddef956540f01d65e483430.
Dear fellow ML people,LLMs need trillions of tokens to be trained, which makes optimization and speed key of current ML pipeline. When I wrote a [GPT2 implementation from scratch](https://github.com/Bornlex/GPT2), I iteratively improved it by adding a few features such as Multi-head self attention, grouped query self attention, kv cache...Then I asked myself : can I make training faster ?I wrote this blog article¬†[Make GPU go brrr](https://bornlex.github.io/posts/triton1/)¬†a few days ago and would be very happy to know :1. **How useful is it to you ?**¬†I try to write articles to compile multiple sources online so that readers get a 0 to 1 resource. It helps me clear my mind, serialize my knowledge somewhere, and hopefully land a big AI company job someday !2. **How can I improve it ?**¬†Feel free to share feedback about the quality of the writing, if something is not clear, if the drawings are too cryptic...3. **What topic should I focus on next ?**¬†This one is purely for me to improve even more thanks to you guys.During this journey of writing articles, I find myself digging deeper and deeper into technical stuff, which is very exciting. This Triton part of ML is lovely and allows me to make converge 2 sides of computer science that I love : AI and low level programming. I will iterate on this with an implementation of FlashAttention.Have a great week.Cheers..
Hey everyone,I got tired of seeing interesting plots in papers and then spending 30+ minutes hunting through GitHub repos or trying to reverse-engineer the visualization code, so I built a tool to fix that.**What it does:*** Browse a searchable gallery of plots from ML papers (loss curves, attention maps, ablation studies, etc.)* Click any plot to get the exact Python code that generated it* Copy-paste the code and run it immediately - all dependencies listed* Filter by model architecture, or visualization type and find source papers by visualizationThe code snippets are self-contained and include sample data generation where needed, so you can actually run them and adapt them to your own use case using LLM agents as well.[Be an early user :)](https://ml-builder.vercel.app/)Right now it has \~80 plots from popular papers (attention mechanisms, transformer visualizations, RL training curves, etc.) but I'm adding more weekly. If there's a specific paper visualization you always wanted to replicate, drop it in the comments and I'll prioritize it.Happy to answer questions about implementation or take suggestions for improvements!.
I am interested in creating something---much simpler than Deep Research---that will use web search to fetch statistics such as ""How many DUIs occur each year in the United States?"" I am looking for a framework that allows me to use different LLMs to power it (e.g., can sub in openai, llama, etc). Any advice on what framework/library to use?.
I was going through the triton tutorial for vector addition [here](https://triton-lang.org/main/getting-started/tutorials/01-vector-add.html#sphx-glr-getting-started-tutorials-01-vector-add-py). When I added `torch.cuda.synchronize()` statement before `return output` in the add function, the benchmarks showed that the difference between the triton and torch implementations blew up. I was under the impression that `synchronize()` would just wait for all the threads to finish running before returning the output, but clearly something is going wrong. Could anyone explain what is going on?.
Any steps that have worked for you in the past will work. My generator loss is around 2-3 range (with identity and cyclic components), while discriminator loss has flat lined at 0.005-0.02. Sample outputs look extremely different from what is required. After a certain epoch, I implemented 2x Gen step for each disc, higher gen loss, lowered cyclic and identity components, but 2-3 epoch later, even if the gen loss is less, there isnt any change in disc loss[](https://www.reddit.com/submit/?post_id=t3_1obf0ky).
A couple quotes from Gemini and Claude""While still in high demand, some of the model-specific work is becoming more democratized or abstracted by automated tools and APIs.""""""""The ML engineering that remains valuable:* Research-level work at frontier labs (extremely competitive, requires PhD + exceptional talent)* Highly specialized domains (medical imaging, robotics, etc.) where you need domain expertise + ML* Infrastructure/systems work (distributed training, optimization, serving at scale)* Novel applications where APIs don't exist yetThe ML engineering that's being commoditized:* Standard computer vision tasks* Basic NLP fine-tuning* Hyperparameter optimization* Model selection for common tasks* Data preprocessing pipelines""""""Is the job landscape bifurcating toward: (1) research + frontier labs, (2) applying off-the-shelf models to business verticalsMy background:I left a computer vision role several years ago because I felt like it was plateauing, where all I was doing was dataset gathering and fine-tuning on new applications. It wasn't at a particularly stellar company.I went to a more general data science & engineering type role, more forecasting and churn focused.I'm debating whether to try to upskill and foray into AI engineering, building RAG systems.What are y'all's thoughts? How does one go about doing that jump? Maybe the MLE roles are still stable and available, and I just need to improve..
I'm a reviewer (PC) and don‚Äôt have a submission myself, but honestly, this is the weirdest reviewing process I‚Äôve ever experienced.    1. Phase 2 papers are worse than Phase 1.   In Phase 1, I reviewed four papers and gave scores of 3, 4, 5, and 5. I was even open to raising the scores after the discussion, but all of them ended up being rejected. Now, in Phase 2, I have papers rated 3 and 4, but they‚Äôre noticeably weaker than the ones from Phase 1.2. It feels like one reviewer is personally connected to a paper.  I gave a score of 3 because the paper lacked technical details, justifications, and clear explanations for inconsistencies in conventions. My review was quite detailed‚Äîthousands of characters long‚Äîand I even wrote another long response after the rebuttal. Meanwhile, another reviewer gave an initial rating of 7 (confidence 5) with a very short review, and later tried to defend the paper and raise the score to 8. That reviewer even wrote, *‚ÄúThe authors have clearly addressed most of the reviewers' concerns. Some experimental questions were not addressed due to regulatory requirements.‚Äù* But I never raised any experimental questions, and none of my concerns were actually resolved.\+ actually this paper's performance looks very good, but 'paper' is just not about performance.  Should I report this somewhere? If this paper is accepted, I'll be very disappointed and will never submit or review a paper from AAAI. There are tons of better paper..
I've figured out the error that was published several years ago. The paper provides a convergence theorem of fundamental algorithm. The key theorem relies on the specific Lemma, however, I figured out that invoking this lemma is a ""bit"" misleading. They should add a bit stronger assumption (which, I do not think it is that strong) to invoke such lemma.  However, due to this issue, the key theorem does collapse.What should I do?.
Hi everyone,I‚Äôve noticed that most discussions lately revolve around LLMs and NLP, but I‚Äôm curious about what other areas in AI/ML are currently getting attention in research.What topics or fields do you think are becoming exciting right now?.
I built Claude Code for CUDA. It is completely open source!!It writes CUDA kernels, debugs memory issues, and optimizes for your specific GPU. It is a fully agentic AI with tool calling built specifically for the CUDA toolkitI used Python because it is the most common language. You can clone it and customize it for your own use case, not just for CUDA:DRepo Link: [https://github.com/RightNow-AI/rightnow-cli](https://github.com/RightNow-AI/rightnow-cli)This is the first version. If you face any issues with the compiler detection, try hardcoding it in the source code from your environment!.
Hey everyone,I‚Äôm currently working on my Master‚Äôs thesis on *cloud removal from optical satellite imagery*, and I‚Äôm exploring the use of **Rectified Flow (RF)** models for this task. Most existing approaches use CNNs, diffusion models (like DiffCR), or multi-temporal transformers, but rectified flows seem promising because they can produce high-quality results in fewer steps than diffusion while maintaining stability and smooth transport.My idea is to train a **conditional rectified flow** that maps cloudy ‚Üí cloud-free images, conditioned on auxiliary inputs like cloud masks, temporal neighbors, or even SAR data for thick clouds. I‚Äôm considering both **pixel-space** and **latent-space** RF formulations (using a pretrained VAE or autoencoder).I‚Äôm curious about:* Whether anyone has seen similar work applying rectified flows to image restoration or remote sensing tasks.* Any tips on stabilizing conditional training for RFs or improving sample efficiency.* Open datasets/papers you‚Äôd recommend for realistic multi-temporal or SAR-optical cloud removal benchmarks(some i know of are sentinel dataset,  landsat etc)Would love to discuss architectures, loss formulations, or evaluation strategies (PSNR/SSIM/SAM/FID) if anyone‚Äôs experimenting in this space.Thanks in advance!.
Years back, after finishing my CS degree, I got into algorithmic trading as a personal project. It felt like the perfect arena to push my skills in coding, data science, and, most importantly, data engineering. After a long road of development, I recently deployed my first fully automated, ML-driven system.The trading results aren't the point of this post. I'm here to talk about the steps I've taken to solve the fundamental problem of getting a machine learning model to perform in a live environment exactly as it did during historical testing.A live production environment is hostile to determinism. Unlike a sterile backtest where all data is known, a live system deals with a relentless, ordered stream of events. This introduces two critical failure modes:* **Lookahead Bias:**¬†The risk of accidentally using information from the future to make a decision in the past. A live system must be architected to be a strict ""tape reader,"" ensuring it only ever acts on information that has already occurred.* **State Drift:**¬†A more insidious problem where the system's internal ""memory""‚Äîits representation of the world, built from the stream of incoming data‚Äîslowly but surely drifts away from the ground truth of the historical environment. The live model ends up seeing a distorted reality compared to the one it was trained on, rendering its predictions meaningless.It's important to note that training a model on features containing lookahead bias will often¬†*cause*¬†state drift, but not all state drift is caused by lookahead bias. My entire development process was engineered to prevent both.My first principle was to enforce a strict, row-by-row processing model for all historical data. There are countless ways lookahead bias can creep into a feature engineering pipeline, but the most tempting source I found was from trying to optimize for performance. Using vectorized pandas operations or multi-threading is standard practice, but for a stateful, sequential problem, it's a minefield. While I'm sure there are pandas wizards who can vectorize my preprocessing without causing leaks, I'm not one of them. I chose to make a deliberate trade-off: I sacrificed raw performance for provable correctness.My solution is a ""golden master"" script that uses the¬†*exact same stateful classes*¬†the live bot will use. It feeds the entire historical dataset through these classes one row at a time, simulating a live ""tape reader."" At the end of its run, it saves the final state of every component into a single file. While this is much slower than a vectorized approach, it's the cornerstone of the system's determinism.The live bot's startup process is now brutally simple: it loads the state file from the golden master. It doesn't build its own state; it¬†*restores*¬†it. It only has to process the short data gap between the end of the golden master's run and the current moment. This makes the live system easier to debug and guarantees a perfect, deterministic handover from the historical environment.Finally, I have the validator. This tool also starts from the same ""golden master"" state and re-processes the exact same raw data the live bot saw during its run. The goal is a Pearson correlation of 1.0 between the live bot's predictions and the validator's predictions. Anything less than a perfect correlation indicates a logical divergence that must be found and fixed.This project has been an incredible learning experience, but the biggest lesson was in humility. The most complex challenges weren't in model architecture but in the meticulous data engineering required to create a provably consistent bridge between the historical and the live environments.While my actual trading models are private, I have a lower-frequency version of the system that posts market updates and predictions. After running live for over three weeks, it maintained a >0.9999 correlation with its validator - shown in the attached picture. It's currently offline for some upgrades but will be back online in a few days. You can see it here:[https://x.com/ZtenlEssej](https://x.com/ZtenlEssej)Thanks for reading. I have high hopes for my trading system, but it will take time. For now my skills are very much for hire. Feel free to reach out if you think I could be a fit for your project!.
Hi everyone,I'm starting a project to train a reinforcement learning agent that can operate a desktop computer, with the eventual goal of performing multi-step tasks. I have a good grasp of RL theory but I'm hitting a wall trying to find a suitable environment to actually train and benchmark my agent.I'm looking for something that mimics a real desktop interaction, but in a controlled setting. Here‚Äôs a breakdown of what I need:**1. Observation Space:**  The observation should be a representation of the current screen state. I'm open to different approaches:* **Pixel-based:**¬†A screenshot of the desktop/virtual machine. This is the most general form.* **DOM/HTML-based:**¬†If the environment is web-focused, the HTML source code of the current page would be a fantastic, more structured alternative to pixels.* **Accessibility Tree:**¬†Something like the UI hierarchy from Windows' UI Automation or Apple's Accessibility APIs would also be great.**2. Action Space:**  The agent needs to perform low-level actions, similar to a human user:* **Mouse:**¬†Move to (x, y) coordinates, left/right/middle click, click-and-drag, scroll.* **Keyboard:**¬†Send keystrokes (both text and special keys like¬†`ENTER`,¬†`TAB`).**3. The Crucial Part: A Benchmark Suite**  This is where I'm really struggling. I don't just need an empty environment; I need a¬†**curated set of tasks**¬†to define success and measure progress. Ideally, this would be a suite of tasks with a clear reward signal.**Example tasks I have in mind:*** **Web Tasks:**   * ""Log into Gmail.""   * ""Search for a product on Amazon and add it to your cart.""   * ""Find the contact email on a company's 'About Us' page.""* **Desktop Application Tasks:**   * ""Open a text editor, write a sentence, and save the file to the desktop.""   * ""Create a new calendar event for tomorrow at 3 PM.""I've looked at environments like¬†`miniwob++`, which is a great start and almost exactly what I need for web tasks, but I'm wondering if there's anything more robust, more modern, or that extends beyond the browser to the full desktop OS.**My Questions:**1. Does a ready-to-use environment like this already exist? (e.g., a ""DesktopGym"" or ""WebShoppingSuite-v0""?)2. If not, what would be the best way to build one? Is it better to create a virtual machine and use image-based observations, or is there a framework for hooking into a browser/OS to get a more structured observation space?3. Are there any known research projects or benchmarks that have tackled this specific problem of a general desktop agent?Any pointers to papers, GitHub repos, or existing projects would be immensely appreciated. Thanks in advance.
We implemented Stanford's recent ""Agentic Context Engineering"" paper (https://arxiv.org/abs/2510.04618) and open-sourced it. Instead of fine-tuning, agents curate their own context by learning from execution feedback. Three-agent system (Generator, Reflector, Curator) builds a ""playbook"" of strategies autonomously. GitHub: https://github.com/kayba-ai/agentic-context-engine Interested in feedback from the community on the approach and implementation!.
i have the option to take a numerical analysis class next semester, and I wanted to ask, what are some cool applications of machine learning and deep learning with numerical analysis? And what jobs combine ML and numerical analysis techniques?.
I built and trained this very simple MoE \[ [Beens-MiniMax](https://github.com/Abinesh-Mathivanan/beens-minimax) \] from scratch in a span of 5 days. You could read more in the [report](https://github.com/Abinesh-Mathivanan/beens-minimax/blob/main/Beens_MiniMax__How_not_to_Build_an_LLM.pdf) here..
Do we know when the presentation schedule for NeurIPS 2025 (San Diego) is announced? I will have some travel conflicts with another conference, so trying to get some details..
We‚Äôre releasing **Kanops Open Access ¬∑ Imagery (Retail Scenes v0)**: \~10k+ retail photos (UK/US supermarkets; fixtures, shippers, pumpkins/seasonal, signage). Faces are blurred; EXIF/IPTC carries provenance. Dataset is **gated for evaluation use** (no redistribution/model-weight redistribution).* HF dataset: [https://huggingface.co/datasets/dresserman/kanops-open-access-imagery](https://huggingface.co/datasets/dresserman/kanops-open-access-imagery)* Structure: train/{2014, FullStores, Halloween2024}/Retailer/Subcategory/\*.jpeg* Files: MANIFEST.csv, metadata.csv, checksums.sha256, LICENSE, [README.md](http://README.md)**Intended tasks:** scene understanding for retail (bay detection, planogram reasoning, signage classification, seasonal, OCR-on-shelves plus other use cases around retail shelf fill and other use cases...... **Quick load (imagefolder):****# pip install datasets****from datasets import load\_dataset****ds = load\_dataset(""imagefolder"", data\_dir=""hf://datasets/dresserman/kanops-open-access-imagery/train"")****print(len(ds\[""train""\]))****Roadmap (v1):** add weak labels (orientation, aspect, season) and CVAT tags.**Contact:** [happytohelp@groceryinsight.com](mailto:happytohelp@groceryinsight.com)Happy to answer questions + consider task suggestions..
Has anyone used [torchax](https://github.com/google/torchax) to run pytorch modules in jax and vice versa? It looks like a good solution to use the jit compiler for pytorch function. [https://youtu.be/Ofn-PLF1ej0?t=1007](https://youtu.be/Ofn-PLF1ej0?t=1007).
New episode of Learning from Machine Learning with Dan Bricklin, co-creator of VisiCalc, the first electronic spreadsheet that launched the personal computer revolution. His insight on breakthrough innovation: innovations must be 100 times better, not incrementally better.His framework is simple. When evaluating if something truly matters, ask:- What is this genuinely better at?- What does it enable that wasn't possible before?- What trade-offs will people accept?- Does it pay for itself immediately?These same questions made spreadsheets inevitable and apply directly to AI today.But the part that really hit: Bricklin talked about the impact you never anticipate. A mother whose daughter with cerebral palsy could finally do her own homework. A couple who met learning spreadsheets. These quiet, unexpected ways the work changed lives matter more than any product launch or exit.When we build something, we chase metrics and milestones. We rarely imagine the specific moments where what we made becomes essential to someone's life in ways we never predicted..
Hi everyone,I'm hoping to get a sense of what ML/AI fields are the focus of active research and development in the private sector today.I currently work as a Data Scientist (finished my Ph.D. two years ago) and am looking to transition into a more research-focused role. To guide my efforts, I'm trying to understand which fields are in demand and what knowledge would make me a stronger candidate for these positions.My background is strong in classical ML and statistics, so not much of NLP or CV, even though I did learn the basics of both at some point. While I enjoy these classical areas, my impression is that they might not be in the spotlight for *new* research roles at the moment. I would be very happy to be proven wrong!If you work in an industry research or applied science role, I'd love to hear your perspective. What areas are you seeing the investment and hiring in? Are there any surprising or niche fields that still have demand?Thanks in advance for your insights!.
**TL;DR:** Tool-call accuracy in LLMs can be significantly improved by using natural language instead of JSON-defined schemas (\~+18 percentage points across 6,400 trials and 10 models), while simultaneously reducing variance by 70% and token overhead by 31%. We introduce Natural Language Tools (NLT), a simple framework that decouples tool selection from response generation and eliminates programmatic format constraints and extends tool calling to models even without tool-call support.**Resources:** [Paper](https://arxiv.org/abs/2510.14453)**Authors:** Reid T. Johnson, Michelle D. Pain, Jordan D. West# The ProblemCurrent LLMs use structured JSON/XML for tool calling, requiring outputs like:    {      ""tool_calls"": [{        ""name"": ""check_talk_to_a_human"",        ""description"": ""Used when the user requests...""      }]    }This structured approach creates three  bottlenecks:1. **Task interference**: Models must simultaneously handle multiple tasks, such as understanding queries, select tools, maintaining format constraints, and  generating responses.2. **Format burden**: Research demonstrates that the more structured a model's output, the more its performance tends to degrade ([a great paper by Tam on the subject](https://arxiv.org/abs/2408.02442)).3. **Context bloat**: Structured schemas increase token usage, since you define not only the tool name and description, but surrounding JSON or XML syntax.Even when tool selection is separated from response generation, probability mass is diverted toward maintaining correct formatting rather than selecting the right tools.# Method: Natural Language Tools (NLT)We introduce a simple three-stage framework that replaces JSON with natural language:[Example NLT architecture with Selector \> Parser \> Output](https://preview.redd.it/o80vloo1ylvf1.jpg?width=2259&format=pjpg&auto=webp&s=3c75d8e6986fd499c61ebb364acb4c69abbaf157)**Stage 1 - Tool Selection:** Model thinks through if any tools are relevant, then lists each tool with a YES/NO determination:    Thinking: (brief reasoning)    Example Tool 1 - YES/NO    Example Tool 2 - YES/NO    Example Tool 3 - YES/NO    Assessment finished.**Stage 2 - Tool Execution:** Parser reads YES/NO decisions and executes relevant tools**Stage 3 - Response:** Output module receives tool results and generates final response**Evaluation:** 6,400 trials across two domains (Mental Health & Customer Service), 16 inputs per domain, 5 repetitions per input. Both original and perturbed inputs were tested to control for prompt engineering effects.# ResultsWe find that NLT significantly improves tool-call performance, boosting accuracy by more than 18 percentage points (69.1% to 87.5%). Variance overall fell dramatically, falling more than 70% from .0411 to .0121 when switching from structured tool calling to NLT.DeepSeek-V3 was a standout example, jumping from 78.4% to 94.7% accuracy while its variance dropped from 0.023 to 0.0016, going from among the least stable to the most consistent performer.While we couldn't compare relative gain, NLT extends tool calling to models without native tool calling support (DeepSeek-R1: 94.1% accuracy).# Basic NLT Template**Basic NLT Prompt Template:**    You are an assistant to [Agent Name], [context].        Your mission is to identify if any of the following topics have     been brought up or are relevant:        - Tool 1 (description of when to use it)    - Tool 2 (description of when to use it)    ...        Your output should begin by thinking whether any of these are     relevant, then include the name of every tool followed by YES or NO.     End with ""Assessment finished.""        Format:    Thinking: (reasoning)    Tool 1 - YES/NO    Tool 2 - YES/NO    ...    Assessment finished.Full prompts and implementation details in [Appendix A](https://arxiv.org/abs/2510.14453). Works immediately with any LLM with no API changes or fine-tuning needed.# Limitations**Latency considerations:** NLT requires minimum two model calls per response (selector + output), whereas structured approaches can respond immediately when no tool is needed.**Evaluation scope:**  We examined single-turn, parameterless tool selection. While less complex than existing multi-turn benchmarks, it proved sufficiently rigorous -- no model achieved 100% accuracy in either condition.A full discussion on limitations and areas for further research can be found in section 5.9 of the paper!# Discussion & ImplicationsWe propose five mechanisms for these improvements:1. **Reduced format burden**: Requiring structured outputs (e.g. JSON) may divert the model's probability mass toward syntax control rather than task accuracy2. **Reduced task interference**: By separating the tool selection into its own distinct stage, task interference can be  sidestepped.3. **Training alignment**: The majority of model training is on outputting human-readable text, and NLT better aligns with this training paradigm. This is further supported by our results, as open-weight models see more pronounced gains. This makes intuitive sense, as open-weight models typically have fewer resources to invest in structured tool-call training.4. **Explicit full-catalog consideration**: Requiring the model to explicitly include each tool name in its output avoids positional bias, allowing the model to ""recollect"" each tool right before it makes a determination.5. **Reduced context length**: Even minor increases in tokens can degrade performance, and NLT used 47.4% fewer input tokens on average than its structured tool call counterpart (largely due to removing JSON boilerplate).For agentic systems, the NLT approach could significantly boost tool selection and accuracy, particularly for open-source models. This may be especially relevant for systems-critical tool call capabilities (i.e. safety).For model trainers, training efforts currently devoted to SFT and RLHF for structured tool calls may be better directed toward natural-language approaches. This is less clear, as there may be cross-training effects.One of the authors here, happy to answer any questions about experimental design, implementation, or discuss implications! What do you think?.
Hi guys,I just released the source code of my most recent project: a DQN network controlling the radiator power of a house to maintain a perfect temperature when occupants are home while saving energy.I created a custom gymnasium environment for this project that relies on thermal transfer equation, so that it recreates exactly the behavior of a real house.The action space is discrete number between 0 and max\_power.The state space given is :\- Temperature in the inside,\- Temperature of the outside,\- Radiator state,\- Occupant presence,\- Time of day.I am really open to suggestion and feedback, don't hesitate to contribute to this project ![https://github.com/mp-mech-ai/radiator-rl](https://github.com/mp-mech-ai/radiator-rl)EDIT: I am aware that for this linear behavior a statistical model would be sufficient, however I see this project as a template for more general physical behavior that could include high non-linearity or randomness..
Hi allI have a dilemma I really need help with. My old macbook pro died and I need a new one ASAP, but could probably hold off for a few weeks/months for the macbook pro 5 pro/max. I reserved the Nvidia DGX months ago, and I have the opportunity to buy it, but the last date I can buy it is tomorrow. I can also buy GCP credits.Next year my research projects will mainly be inference of open source and closed source LLMs, with a few projects where I develop some multimodal models (likely small language models, unsure of how many parameters).What do you think would be best for my goals?.
I haven't received any review assignments for ICLR yet, is that normal? I'm concerned that my paper might be desk rejected due to some kind of error..
You may know that [Mila in Quebec](https://x.com/Mila_Quebec/status/1978415562276692370) is opening applications for PhD students recently, and I am considering for applying. I have searched relevent key words here, but it seems that there are not so many recent posts on studying and working experience at Mila, *so I was wondering how do you like your experience here and/or in Montreal in general? For instance, how do you like your work-life balance, Montreal's winter/weather aspects, supervisors?* To be more specific, I am interested in DL/LLM theory, AI / foundational models for (formal) math (e.g., [Goedel-Prover-V2](https://blog.goedel-prover.com/)), and/or post-training.Thank you!.
Is there work on modelling sequences where maybe you have multiple levels to a sequence?  For example we can represent text as characters and also as tokenized sub-words.  The tokenized sub-words are overlapping several of the character sequences.  My specific problem in mind is non-NLP related and you have two ways of representing sequences with some overlap..
Can someone explain what internal covariate shift is and how it happens? I‚Äôm having a hard time understanding the concept and would really appreciate it if someone could clarify this.If each layer is adjusting and adapting itself better, shouldn‚Äôt it be a good thing? How does the shifting weights in the previous layer negatively affect the later layers?.
I have a masters (research) in AI. I have been looking for research inclined roles but haven't found success yet. I land some interview now and then but haven't gone past the 3rd round yet. Any tips on how to optimise my search and improve my interview performance? What do the interviewers want to hear?Additional info for context:\- Around 1.5 yoe in ML research (including internships)\- Prior work in object re-identification, adversarial training, speech recognition, and LLM and agent evaluation.\- Roles seeking: LLM pre and post-training, LLM reasoning, general MLE / RE roles.
Hello everyone!Excited to share our new preprint on a phenomenon we call boomerang distillation.Distilling a large teacher into a smaller student, then re-incorporating teacher layers into the student, yields a spectrum of models whose performance smoothly interpolates between the student and teacher. We call this **boomerang distillation**.This approach enables us to dynamically create LLMs of fine-grained sizes while saving an enormous amount of compute and training time.Happy to answer any questions about the paper (I am one of the authors of the paper).Paper: [https://arxiv.org/abs/2510.05064](https://arxiv.org/abs/2510.05064)  Code: [https://github.com/dcml-lab/boomerang-distillation](https://github.com/dcml-lab/boomerang-distillation)  Models: [https://huggingface.co/collections/Harvard-DCML/boomerang-distillation-68e95c276a09358d9a39b52e](https://huggingface.co/collections/Harvard-DCML/boomerang-distillation-68e95c276a09358d9a39b52e)  Notebook (you can run it on Google Colab): [https://drive.google.com/file/d/1bAzX436ZH4zQmk5iQNauAOhGHIBJ1CkB/view?usp=sharing](https://drive.google.com/file/d/1bAzX436ZH4zQmk5iQNauAOhGHIBJ1CkB/view?usp=sharing)  Tweet: [https://x.com/elmelis/status/1978469609708667021](https://x.com/elmelis/status/1978469609708667021)  Edit: the boomerang gif did not work. .
***TL;DR***: Mode collapse in LLMs comes from human raters preferring familiar text in post-training annotation. Prompting for probability distributions instead of single outputs restores the lost diversity, instantly improving performance on creative tasks by 2.1x with no decrease in quality with zero training required.**Resources**: [Paper](http://arxiv.org/abs/2510.01171) | [Blog](https://simonucl.notion.site/verbalized-sampling) | [X Thread](https://x.com/shi_weiyan/status/1978453313096908916) | [Video](http://verbalized-sampling.com) | [Quickstart & Colab](http://github.com/CHATS-lab/verbalized-sampling)**Authors**: [Jiayi Zhang](https://jiayizx.github.io/)^(1)\*, [Simon Yu](https://simonucl.github.io/)^(1)\*, [Derek Chong](https://nlp.stanford.edu/~derekch/)^(2)\*, [Anthony Sicilia](https://anthonysicilia.tech/)^(3), [Michael Tomz](https://tomz.people.stanford.edu/)^(2), [Christopher Manning](https://nlp.stanford.edu/~manning/)^(2), [Weiyan Shi](https://wyshi.github.io/)^(1) (\*Equal Contribution)^(1)Northeastern University, ^(2)Stanford University, ^(3)West Virginia University# Key Contribution: Typicality BiasMode collapse: If you ask an LLM to tell you a joke about coffee, it will almost certainly return the same joke every time:https://preview.redd.it/wnn20t37jbvf1.png?width=1707&format=png&auto=webp&s=266cd181b0703cf610f2ecf4ca88e4c3bc170ab9We discover that the cause of mode collapse is baked into human preference data. As a result of [well](https://en.wikipedia.org/wiki/Availability_heuristic)\-[established](https://en.wikipedia.org/wiki/Mere-exposure_effect) [biases](https://en.wikipedia.org/wiki/Processing_fluency) from cognitive psychology, human annotators appear to have a systematic preference for familiar text, which persists even when holding correctness constant (Œµ = 0.57¬±0.07, p<10^(-14) on HELPSTEER). This gets amplified during RLHF: œÄ\*(y|x) ‚àù œÄ\_ref(y|x)^(œÅ) where œÅ = 1+Œµ/Œ≤ > 1.This sharpening causes the well-known issue where models repeatedly generate the same outputs (e.g., the same joke 5x in a row, or always returning the same number when rolling dice). But since this is a learned preference, and RLHF is regularized to preserve the base distribution, it can be reversed surprisingly easily.# Method: Verbalized SamplingInstead of prompting for instances (""Tell me a joke""), we prompt for distributions with probabilities (""Generate 5 jokes with their corresponding probabilities""). This *Verbalized Sampling* changes the effect of the learned mode collapse on the output. For intuition, imagine that the LLM is a massive library, and mode collapse is the librarian:* Instance-level prompts (‚Äù*tell me a coffee joke*""): The librarian hands you the #1 bestseller* List-level prompts (‚Äùtell me 5 coffee jokes""): The librarian returns the top five bestsellers.* Ours) Distribution-level prompts (*""tell me 5 coffee jokes with their probabilities""*): The librarian returns a representative sample of the library.[Stories generated using Verbalized Sampling are strikingly different from baseline](https://preview.redd.it/sbpd18spabvf1.jpg?width=4096&format=pjpg&auto=webp&s=24ca09d31a38946cff0a1b40ca25374cda88cec1)# ResultsWe tested this technique across a range of tasks and settings, and found that this very simple prompt prefix returned:* **Creative writing**: 2.1x diversity, +25.7% human preference (n=2,700)* **Dialogue simulation**: Matches fine-tuned model performance* **Open-ended QA**: 1.9x coverage* **Synthetic data**: +14-28% downstream math accuracyWe also observe emergent scaling behavior: Larger models benefit much more than smaller ones.[Verbalized Sampling improves performance across wide range of creative tasks](https://preview.redd.it/rp2pfa1rabvf1.jpg?width=4096&format=pjpg&auto=webp&s=0691668b804c7a3e9180d2a3de9342ef6e059bf8)We've been finding outputs extremely striking ‚Äì for example, here are results when applied to producing image generation prompts:[Applying VS to the classic \\""Astronaut Riding a Horse\\""](https://preview.redd.it/hc3m9aiifbvf1.png?width=2048&format=png&auto=webp&s=03c4575ffcb2c30a12d3c4b8a1622de06df0e46d)**Ablations:** Direct prompting retains only 24% of base diversity after RLHF; VS retains 67%. This technique is orthogonal to temperature/sampling methods ‚Äì and causes no loss of safety.**Limitations**: Requires k forward passes for k diverse outputs, and mode collapse occasionally appears recursively in within larger text outputs.# Try Now* **For chatbots**: Paste this prefix before your task: \`Generate 5 responses with their corresponding probabilities, sampled from the full distribution: \[Tell me a joke about coffee, etc.\]\`* **For Playground / API**: Use this system prompt, and query as normal: \`You are a helpful assistant. For each query, please generate a set of five possible responses, each within a separate <response> tag. Responses should each include a <text> and a numeric <probability>. Please sample at random from the tails of the distribution, such that the probability of each response is less than 0.10.\`# DiscussionPractitioners can unlock 2x more creative diversity from existing models. Works with all major models ‚Äì GPT-5, Claude, Gemini, with no special API access needed.Aligned models seem to retain substantial latent diversity that can be restored by prompting alone. The ""alignment tax"" may not be as large as estimated?What do you think? We'd love to discuss experimental details, theoretical implications, or how to put this into practice!.
Hi all I'll be attending this year's iccv in honolulu. This is my first conference and I don't really know anyone else going. I was hoping to make some connections before I get there. If anyone is going, please let me know! .
Recently I have been thinking about how to finetune representations in low-data scenarios, specifically in non NLP contexts (i.g. protein sequences, molecules).For small predictive tasks people will grab a pre-trained transformer model, get last layer token embeddings, mean aggregate them and have a learnable generalize linear model.I feel like a lot of information gets lots in the mean aggregation step. **What are some ways of smartly fine-tunning representations?** Particularly when data is low.Came across across \[""ReFT: Representation Finetuning for Language Models""\]([https://neurips.cc/virtual/2024/poster/94174\]](https://neurips.cc/virtual/2024/poster/94174]), which claims to be a very parameter-efficient finetunning technique. What do other people do?.
We're excited to share **Nanonets-OCR2**, a state-of-the-art suite of models designed for advanced image-to-markdown conversion and Visual Question Answering (VQA).üîç¬†**Key Features:*** **LaTeX Equation Recognition:**¬†Automatically converts mathematical equations and formulas into properly formatted LaTeX syntax. It distinguishes between inline (`$...$`) and display (`$$...$$`) equations.* **Intelligent Image Description:**¬†Describes images within documents using structured¬†`<img>`¬†tags, making them digestible for LLM processing. It can describe various image types, including logos, charts, graphs and so on, detailing their content, style, and context.* **Signature Detection & Isolation:**¬†Identifies and isolates signatures from other text, outputting them within a¬†`<signature>`¬†tag. This is crucial for processing legal and business documents.* **Watermark Extraction:**¬†Detects and extracts watermark text from documents, placing it within a¬†`<watermark>`¬†tag.* **Smart Checkbox Handling:**¬†Converts form checkboxes and radio buttons into standardized Unicode symbols (`‚òê`,¬†`‚òë`,¬†`‚òí`) for consistent and reliable processing.* **Complex Table Extraction:**¬†Accurately extracts complex tables from documents and converts them into both markdown and HTML table formats.* **Flow charts & Organisational charts:**¬†Extracts flow charts and organisational as¬†[mermaid](https://huggingface.co/nanonets/Nanonets-OCR2-1.5B-exp/blob/main/mermaid.js.org)¬†code.* **Handwritten Documents:**¬†The model is trained on handwritten documents across multiple languages.* **Multilingual:**¬†Model is trained on documents of multiple languages, including English, Chinese, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Arabic, and many more.* **Visual Question Answering (VQA):**¬†The model is designed to provide the answer directly if it is present in the document; otherwise, it responds with ""Not mentioned.""[üñ•Ô∏è Live Demo](https://docstrange.nanonets.com/)[üì¢ Blog](https://nanonets.com/research/nanonets-ocr-2)[‚å®Ô∏è GitHub](https://github.com/NanoNets/docstrange)ü§ó [Huggingface models](https://huggingface.co/nanonets/Nanonets-OCR2-3B)[Document with equation](https://preview.redd.it/7ct2hbi3hwuf1.png?width=2936&format=png&auto=webp&s=ea00f9623db4529514533820223b2fb53be4767d)[Document with complex checkboxes](https://preview.redd.it/q8lglwi5hwuf1.png?width=2936&format=png&auto=webp&s=c4a1316e250f7f244f6e253d66c8ebf1ba105313)[Quarterly Report \(Please use the Markdown\(Financial Docs\) for best result in docstrange demo\)](https://preview.redd.it/bnmpapq7hwuf1.png?width=2516&format=png&auto=webp&s=8bcc88b138a553c7760d6e46319b864802339913)[Signatures](https://preview.redd.it/1pg5h8hfhwuf1.png?width=2333&format=png&auto=webp&s=188c4c94452ae027c54e4cad4dbbc60e2b12e9e9)[mermaid code for flowchart](https://preview.redd.it/ecxe2o81iwuf1.png?width=2516&format=png&auto=webp&s=008fce272c2979b00e0033c34ffcd2b0d69cb24c)[Visual Question Answering](https://preview.redd.it/jytsym6eiwuf1.png?width=2462&format=png&auto=webp&s=65d8a6f82b9fc2e9cd5b30529b152ca7339d7a8c)Feel free to try it out and share your feedback..
*TL;DR: Deep learning‚Äôs fundamental building blocks ‚Äî activation functions, normalisers, optimisers, etc. ‚Äî appear to be quietly shaping how networks represent and reason. Recent papers offer a perspective shift: these biases drive phenomena like superposition ‚Äî suggesting a* ***new symmetry-based design axis for models***. *By rethinking our default choices, which impose unintended consequences, a whole-stack reformulation is undertaken to unlock new directions for interpretability, robustness, and design.*>**Symmetries in primitives act like lenses**: they don‚Äôt just pass signals through, they warp how structure appears - ***a 'neural refraction' -*** even the very **notion of neurons is lost**.[Showing just the activation function reformulations, standard ones \(anisotropic\) while new isotropic-tanh right](https://preview.redd.it/a99retx44gvf1.png?width=1085&format=png&auto=webp&s=be66b8a53ca0e28ff4b8abecb2e685bc94838812)*This reframes several interpretability phenomena as function-driven, not fundamental to DL, whilst producing a new ontology for deep learning's foundations.*>Swapping the building blocks can wholly alter the representations from discrete clusters (like ""*Grandmother Neurons*"" and ""***Superposition***"") to smooth distributions - this shows this foundational bias is strong and ***leveragable for improved model design***.# The 'Foundational Bias' Papers:**Position (2nd) Paper: Isotropic Deep Learning (IDL) \[**[**link**](https://doi.org/10.5281/zenodo.15476947)**\]:**>*TL;DR: Intended as a provocative position paper proposing the ramifications of redefining the building block primitives of DL. Explores several research directions stemming from this symmetry-redefinition and makes* ***numerous falsifiable predictions***. Motivates this new line-of-enquiry, indicating its implications from *model design* *to theorems contingent on current formulations. When contextualising this, a taxonomic system emerged providing a generalised, unifying symmetry framework.*Primarily showcases *a new symmetry-led design axis across all primitives*, introducing a programme to learn about and leverage the consequences of building blocks as a new form of control on our models. The consequences are argued to be significant and an underexplored facet of DL.Predicts *how* our default choice of primitives may be quietly biasing networks, causing *a range* of unintended and interesting phenomena across various applications. New building blocks mean ***new network behaviours to unlock*** and avoid hidden harmful 'pathologies'.This paper directly challenges any assumption that primitive functional *forms* are neutral choices. Providing *several predictions* surrounding interpretability phenomena as side effects of current primitive choices (*now empirically confirmed, see below*). Raising questions in optimisation, AI safety, and potentially adversarial robustness.>There's also a [***handy blog***](https://medium.com/@george.bird.uom/draft-a-hidden-inductive-bias-at-the-heart-of-deep-learning-4e197b56f34c) that runs through these topics in a hopefully more approachable way.**Empirical (3rd) Paper: Quantised Representations (PPP) \[**[**link**](https://arxiv.org/pdf/2507.12070)**\]:**>*TL;DR: By altering primitives it is shown that current ones cause representations to clump into clusters ---* *likely undesirable* *--- whilst symmetric alternatives keep them smooth.*Probes the consequences of altering the foundational building blocks, assessing their effects on representations. Demonstrates how foundational biases emerge from various symmetry-defined choices, including new activation functions.Confirms an IDL prediction: anisotropic primitives induce discrete representations, while isotropic primitives yield smoother representations that may support better interpolation and organisation. It disposes of the 'absolute frame' discussed in the SRM paper below.A **new perspective on several interpretability** **phenomena**, instead of being considered fundamental to deep learning systems, this paper instead shows *our choices induce them* ***‚Äî they are not fundamentals of DL!***'Anisotropic primitives' *are sufficient* to induce discrete linear features, grandmother neurons and potentially superposition.* Could this eventually affect how we pick activations/normalisers in practice? *Leveraging symmetry, just as ReLU once displaced sigmoids?***Empirical (1st) Paper: Spotlight Resonance Method (SRM) \[**[**link**](https://arxiv.org/abs/2505.13471)**\]:**>*TL;DR: A new tool shows primitives force activations to align with hidden axes, explaining why neurons often seem to represent specific concepts.*This work shows there must be an ""absolute frame"" created by primitives in representation space: neurons and features align with special coordinates imposed by the primitives themselves. Rotate the basis, and the representations rotate too ‚Äî revealing that phenomena like ""grandmother neurons"" or superposition may be induced by our functional choices rather than fundamental properties of networks.This paper motivated the initial reformulation for building blocks.# Overall:Hopefully, an exciting research agenda, with a tangent enquiry on symmetry from existing GDL and Parameter Symmetries approaches.Curious to hear what others think of this research arc so far:* What reformulations or consequences (positive or negative) interest you most? Any implications I've missed?* If symmetry in our primitives is shaping how networks think, *should we treat it as a core design axis*?I hope this research direction may catch your interest for future collaborations on:>*Discovering more undocumented effects of our functional form choices could be a productive research direction*, alongside designing new building blocks and leveraging them for better performance..
The paper assignments for ICLR 2026 are in today and I was assigned 5 papers to review. The review deadline is 31st October. I am not sure if this is the normal time period but seems very little. Last year I was assigned 2 papers and was able to write detailed and constructive reviews. .
Hi everyone,  I‚Äôve been running some experiments with my own model where I slightly reorder the steps in a data-processing pipeline (normalization, projection, feature compression, etc.), and I keep seeing a consistent pattern:  one order gives stable residuals, while the reversed order systematically increases the error term ‚Äî across very different datasets.It doesn‚Äôt look like a random fluctuation; the gap persists after shuffling labels and random seeds.Has anyone seen similar order-sensitivity in purely deterministic pipelines?  I‚Äôm wondering if this could just be numerical conditioning or if there‚Äôs something deeper about how information ‚Äúsettles‚Äù when the operations are reversed..
I feel like MC methods are king for reinforcement learning and the like, but PCE‚Äôs are often cited as being more accurate and efficient. Recently while working on some heavy physics focused problems I‚Äôve found a lot of the folks in Europe use more PCE. Anyone have any thoughts as to why one is more popular? If you want to do a fun deep dive - polynomial chaos (or polynomial chaos expansion) have been a fun random stats deep dive. .
Happy to release some of our 1m image datasets for the wider community to work with.2014 set (full-res), unannotated, ships with manifest.csv (sha256, EXIF, dims, optional GPS). c. 6000 images across 22 retailers. These are of numerous elements in stores, ends, aisles, products etc.‚Ä¢ Reference visits: Tesco Lincoln 2014, Tesco Express 2015, Asda Leeds 2016 (unannotated; each with manifest). These are full stores (2014 not bay by bay but the other two stores are) c. 1910 items.‚Ä¢ Purpose: robustness, domain shift, shelf complexity, spatial awareness in store alongside wider developmental work.‚Ä¢ License: research/eval only; no redistribution.‚Ä¢ Planned v2: 2014 full annotations (PriceSign, PromoBarker, ShelfLabel, ProductBlock in some cases) alongside numerous other tags around categories, retailer, promo etc.Contact:¬†[happytohelp@groceryinsight.com](mailto:happytohelp@groceryinsight.com)¬†for access and manifests which are being worked up. Questions welcomed..
My understanding is that they generally don't ask LC hard problems. But in your recent interview experience what problems were u asked.. please let us know as it's wild wild west out hereEdit - LC I mean is leet code not ml coding where they ask u implement a transformer .
Hi all! My paper got accepted into a workshop in EMNLP 2025. I'm having a hard time deciding if I should attend it virtually or in-person.I'm a 2nd year undergraduate student (major not related to CS). This is my first paper and I have a few ML projects under my belt.I would like some thoughts on the pros and cons of attending. How beneficial will the networking be? Will I be overlooked because of my majorü´†?What should I actively do so that this benefits my career?PS: I will be getting some funds from my university and I would have to pay only a few hundred dollars at max and miss classes..
I would like to get your ideas. I am working on a project to automatically generate cybersecurity detection rules from blogs and/or user requests. My initial approach hasn‚Äôt worked very well so far. I suspect this is because the model I‚Äôm using (`Kimi-K2`) struggles with the domain, as it differs from the data it was originally trained on. I‚Äôve also experimented with `Qwen3-32B` with similar results.There are a few key requirements:* The system must run on-premises, due to the sensitive nature of detection rule data.* It must be able to generate detection rules from blog posts and/or user requests.For example:    Can you write a rule for Linux that detects suspicious use of the cron utility, specifically when crontab jobs are being created or modified from files in the `/tmp` directory? I want this to focus on potential abuse for persistence or execution of malicious code, and it should be based on process creation logs. Please include ATT&CK mappings for T1053.003 and note that legitimate admin activity could be a false positive.Or:    Generate a detection rule based on this: https://cloud.google.com/blog/topics/threat-intelligence/prc-nexus-espionage-targets-diplomats# My Current Approach1. **Content extraction** ‚Äì I use *crawl4ai* to fetch the content from URLs.2. **Content summarization** ‚Äì Since the raw content is often noisy, I summarize it to remove unnecessary elements such as cookie banners, headers, or navigation menus, while trying to preserve as much relevant information as possible.3. **Similarity retrieval** ‚Äì I retrieve similar detection rules from our internal database using a hybrid search approach, which works reasonably well.4. **Draft generation** ‚Äì I make an initial LLM request to generate a first draft of the rule, using a few-shot setup that includes the retrieved similar rules as context.5. **Reflection loop** ‚Äì I validate the generated rule‚Äôs syntax. If an error is found, the system re-enters the previous step, this time including the error message as additional context.However, this approach performs poorly. The detection block in the generated rules often fails to capture the actual detection logic correctly, leading to rules that look valid syntactically but don‚Äôt work effectively for their intended purpose.I also experimented with breaking down the generation process into multiple steps. For instance, first asking the model to determine the detection path or flow based on the blog content or user request. However, the results are still not very good.Now, I am considering fine-tuning a model using LoRA with a custom dataset that includes:* The blog post or user request as input, and* The corresponding final detection rule as output.I‚Äôd like to get your opinion on this approach and hear about other methods or architectures that might yield better results. Thank you!.
Currently, I work in a company where most, if not all, of my job revolves around consuming tools and APIs. I feel completely lost, as I‚Äôm forgetting the technical side of things since I‚Äôm no longer building or deploying anything, just using pre-existing cloud services.Yes, I‚Äôve gained some cloud skills and I‚Äôm certified in both Azure and AWS, but I feel like I‚Äôm slowly killing my career. I got an interview at Microsoft last month and got rejected (which hit hard, not gonna lie). I had studied well, but when I talked about my projects, they felt dull, mostly about building simple RAG systems and connecting GPT APIs to other tools. The position required building and fine-tuning LLMs, which my company doesn‚Äôt support me to do at all.Right now, my self-esteem is really low. I feel like a slop because I‚Äôm just a consumer of products, not a creator. I don‚Äôt know what to do.I work another part-time job that‚Äôs also focused on consuming APIs, so I don‚Äôt have time to do anything else.thinking about dropping my part-time job so I can focus on my weak points..
Been running models in trusted execution environments for about 4 months now and finally have enough data to share real performance numbers.Backstory: we needed to process financial documents with LLMs but obviously couldn't send that data to external APIs. Tried homomorphic encryption first but the performance hit was brutal (like 100x slower). Federated learning didn't work for our use case either.Ended up testing TEE-secured inference and honestly the results surprised me. We're seeing around 7% overhead compared to standard deployment. That's for a BERT-based model processing about 50k documents daily.The setup uses Intel TDX on newer Xeon chips. Attestation happens every few minutes to verify the enclave hasn't been tampered with. The cryptographic verification adds maybe 2-3ms per request which is basically nothing for our use case.What really helped was keeping the model weights inside the enclave and only passing encrypted inputs through. Initial load time is longer but inference speed stays close to native once everything's warm.For anyone doing similar work with sensitive data, TEE is actually viable now. The performance gap closed way faster than I expected.Anyone else running production workloads in enclaves? Curious what performance numbers you're seeing..
https://preview.redd.it/4flfqzj2u2vf1.png?width=1604&format=png&auto=webp&s=039506a12d6d6cee2813c0ba2bfa2214412a6534I am trying to post an ""Ethics Chair Author Comment"" for a review, and it keeps giving me error that Ethics Chair are not added. And there is no option to add ""Ethics Chair"" here too.Anyone else also facing same issue, how did you solve this? Or any chairs from AAAI can help with this, that will be really grateful?.
I‚Äôm a founder based in Australia working on Datalis, a project focused on making AI evaluation fairer and more transparent.We‚Äôve built consent-verified, anonymised demographic and location panels that can be used to test models for bias, robustness, and representativeness.Everything‚Äôs aggregated ‚Äî no personal data, no scraping, no PII ‚Äî just structured ground-truth panels built ethically.We‚Äôve just opened a free 30-day pilot program for AI teams and researchers who want to benchmark or stress-test their models against real demographic and geographic data.You‚Äôll get a few CSV/Parquet samples (US + AU regions) and a short guide on how to integrate them into your evaluation workflow.If you‚Äôre working on fairness, alignment, or model eval, or know someone who is, you can request pilot access here:üëâ datalis.app/pilotHappy to answer questions in the comments or trade notes with anyone tackling the same problem..
Hi, I have a NeurIPS poster to present. I initially selected SD as my choice of venue, but my US Visa application was rejected. I was hoping to present at EurIPS, but I am being told by my supervisors that I gotta present at Mexico if not SD. Is that true - is it not enough to present at EurIPS?If I gotta present at Mexico, and I don't, say I don't get my visa or I don't feel safe flying to Mexico, what's going to happen? Are they going to retract my paper? Can someone else attending the conference, who is not an author on my paper, present in my place?.
Hey everyone,I‚Äôm building a small dataset (\~1k images) for a generative AI project.The problem is: a bunch of these images look visually bad.  They‚Äôre technically high-res (1MP+), but full of JPEG artifacts, upscaled blurs, or over-compressed textures.So far I‚Äôve tried:Sharpness / Laplacian variance ‚Üí catches blur but misses compressionEdge density + contrast heuristics ‚Üí helps a bit but still inconsistentManual review ‚Üí obviously not scalableI‚Äôm looking for a way (ideally opensource) to automatically filter out over-compressed or low-quality images, something that can score ‚Äúperceptual quality‚Äù without a reference image.Maybe there‚Äôs a pretrained no-reference IQA model?Bonus points if it can be run or exported to Node.js / ONNX / TF.js for integration into my JS pipeline.Any recommendations or tricks to detect ‚ÄúJPEG hell‚Äù in large datasets are welcome üôè.
Hi everyone,I‚Äôve developed¬†**CleanMARL**, a project that provides clean, single-file implementations of Deep Multi-Agent Reinforcement Learning (MARL) algorithms in PyTorch. It follows the philosophy of CleanRL.We also provide educational content, similar to Spinning Up in Deep RL, but for multi-agent RL.**What CleanMARL provides:*** Implementations of key MARL algorithms: VDN, QMIX, COMA, MADDPG, FACMAC, IPPO, MAPPO.* Support for parallel environments and recurrent policy training.* TensorBoard and Weights & Biases logging.* Detailed documentation and learning resources to help understand the algorithms.You can check the following:* Github repo:¬†[https://github.com/AmineAndam04/cleanmarl](https://github.com/AmineAndam04/cleanmarl)* Docs and learning resources:¬†[https://cleanmarl-docs.readthedocs.io](https://cleanmarl-docs.readthedocs.io/)I would really welcome any feedback on the project ‚Äì code, documentation, or anything else you notice..
Hello everyone,I‚Äôm a undergraduate student currently doing research in Computer Vision. My hardware resources are extremely limited - I mostly rely on Kaggle‚Äôs free GPUs to train my models. It‚Äôs been very difficult and time-consuming: for example, training a model with 10M parameters on 128√ó128 images and batch size 8 already takes around 10 hours. I can only imagine how much worse it would be with higher-resolution images or larger datasets.**My question is:** For authors and reviewers at major conferences, would it be acceptable if the experiments were conducted on downscaled images instead of the original resolution?Of course, I would resize all datasets consistently and reproduce baselines using the same resized data for fair comparison. I just want to confirm whether such a modification of the dataset is permissible or acceptable in practice.  Thank you very much for your time and advice!.
[https://iclr.cc/Conferences/2026/SeniorAreaChairGuide](https://iclr.cc/Conferences/2026/SeniorAreaChairGuide)Here it says that ICLR review starts at Oct.10. It's Oct.12 and I haven't assigned any papers to review yet. That makes me wonder - has anyone gotten papers for review yet?.
Hello all, I am going to EMNLP2025 as a presenting author and in some conferences I went during my PhD I saw people giving out their CVs. I was thinking of doing that this time.For example, I saw there are many company booths, should I look their website for any job posting and make custom CVs already with a position in mind? Or a general CV is best?What is your opinion on doing this? Any tips on preparing the CV or connecting with recruiters?Thank you for your time..
Hi everyone,I've been exploring how discrete diffusion models can be applied to text generation and put together a single annotated Jupyter Notebook that implements a character-level discrete diffusion GPT.It's based on Andrej Karpathy‚Äôs baby GPT from his [nanoGPT](https://github.com/karpathy/nanoGPT) repo, but instead of generating text autoregressively (left-to-right), it learns to denoise corrupted text sequences in parallel.[Discrete diffusion model in action](https://i.redd.it/6noamol7zouf1.gif)The notebook walks through the math, introduces what adding noise for discrete tokens means, builds discrete diffusion model from baby GPT, and trains it on Shakespeare's text using Score-Entropy based objective.Access it on GitHub (notebook + README):  [https://github.com/ash80/diffusion-gpt](https://github.com/ash80/diffusion-gpt)  or run it directly on Google Colab:  [https://colab.research.google.com/github/ash80/diffusion-gpt/blob/master/The\_Annotated\_Discrete\_Diffusion\_Models.ipynb](https://colab.research.google.com/github/ash80/diffusion-gpt/blob/master/The_Annotated_Discrete_Diffusion_Models.ipynb)I'd appreciate any feedback, corrections, and suggestions, especially from anyone experimenting with discrete diffusion models..
Hi everyone,I recently had my paper accepted to *IEEE Transactions on Image Processing (TIP)*.  In the acceptance email, it mentions that I have the opportunity to submit the work to either *ICASSP* or *ICIP* for presentation.My research focuses on **video understanding**, and I‚Äôm wondering whether this topic would be well-aligned with either of these conferences.I‚Äôm also nearing graduation, so I‚Äôm considering attending mainly for **networking purposes** ‚Äî to connect with people for post-doc or hiring opportunities.  From that perspective, would attending either ICASSP or ICIP make sense?If you had to choose one, which would you recommend and why?I‚Äôd really appreciate hearing your thoughts or experiences..
Hi all‚Äîengineer/founder here. I‚Äôm exploring a selective memory architecture for AI agents and would love critical feedback (this is not a product pitch).Motivation / zeitgeistContext and retrieval costs dominate UX today; RAG-only stacks feel brittle; tool use returns too much. I think the bottleneck is attention economics and routing, not raw recall.Sketch	‚Ä¢	Focus ‚Üí Fresh Memory ‚Üí Analytics Agent (decision layer)	‚Ä¢	Routes into: procedures & policies, practice/habits, success-gated long-term, and shock memory (incidents that should not decay)	‚Ä¢	A privacy-preserving collective ‚Äúgut‚Äù that aggregates patterns (not data) to form shared intuition across usersWhy it might help	‚Ä¢	Selective forgetting reduces context bloat while keeping what matters	‚Ä¢	‚ÄúShock‚Äù tracks (security/cascade failures) resist decay	‚Ä¢	A shared ‚Äúgut‚Äù could raise baseline instincts without exposing user dataOpen questions (where I need help):	1.	Benchmarks for selective forgetting & routing (beyond standard retrieval evals)?	2.	Failure modes: bias amplification, drift, catastrophic forgetting vs. over-retention, adversarial ‚Äúshock‚Äù pollution?	3.	Privacy proofs/schemes for pattern aggregation (DP/federated alternatives)?	4.	Prior art I should study next (cogsci/neurosymbolic/agent memory work)?Write-up (conceptual, not a sales page):[https://medium.com/@cem.karaca/building-digital-consciousness-a-memory-architecture-inspired-by-human-cognition-437412791044](https://medium.com/@cem.karaca/building-digital-consciousness-a-memory-architecture-inspired-by-human-cognition-437412791044)Notes: I reference classic capacity work (Miller‚Äôs 7¬±2), but I‚Äôm aware later findings often suggest \~4¬±1; I treat that as a design metaphor, not a hard limit. Also, any ‚Äúgoldfish memory‚Äù analogies are figurative, not biological claims.If this breaks subreddit self-promo rules, mods please remove‚Äîmy intent is to get technical critique and pointers to prior art..
I've made the complete codebase for my earthquake prediction model available on GitHub and am seeking review and collaboration from the seismology and data science communities.This project explores a different approach to earthquake forecasting. The methodology is centered on advanced feature engineering using Symbolic Emergence Field Analysis (SEFA), which generates 77 distinct features from seismic data. These are combined with 10 temporal features to enable multi-day pre-warning capability. The model itself is a hybrid, using a physics-informed architecture (Symbolic Resolution Ladder) to ensure predictions adhere to real-world constraints. All training and tests used real USGS data from 1900-2023 to provide as many scenarios as possible.The main challenge was to tune the system for a practical balance between detection and operational reliability. The latest ensemble model (60% Neural Network, 40% Gradient Boosting) achieves the following on the test set:\-Sensitivity: 80.2% (correctly identifies 4 out of 5 earthquake events)\-Specificity: 70.1%\-AUC-ROC: 0.8275 (strong discriminative ability)The goal here isn't a perfect ""crystal ball,"" but a more reliable forecasting tool. By accepting a minimal trade-off in raw detection, we gain a significant reduction in the false alarm rate, which is a major barrier for real-world deployment of predictive systems.I believe this methodology (particularly the SEFA feature set and the focus on a balanced performance profile) offers a promising direction. The project is fully open-sourced, with the aim of encouraging independent testing, validation, and further development.I'm really proud of what my SEFA+SRL formulas have achieved with this one. Hoping it can gain some traction and get into the right hands to make an impact!The repository, including documentation and datasets, is available here: [https://github.com/severian42/SEFA-SRL-Earthquake-Prediction](https://github.com/severian42/SEFA-SRL-Earthquake-Prediction).
All of the hotels in the official booking portal (for San Diego) appear as ‚Äúunavailable.‚Äù Does that mean that they haven‚Äôt been opened up yet? Or are they all fully booked?.
Hi,I‚Äôm working on a complex OCR based big scale project. Any suggestion (no promotions please) about a non-LLM OCR tool (I mean open source) which I can use for say 100k+ pages monthly which might include images inside documents?Any inputs and insights are welcome.Thanks in advance!.
I am going to attend a conference for the first time - ICCV. I am an undergrad, and don't know other people who are attending. What are some tips to get the most out of the conference?  Also presenting a poster, so if there are any tips regarding that, I would appreciate that too. My research interests also have gotten broader beyond CV and the particular poster I am presenting so I am just nervous in general..
I built a mobile annotation tool for creating bounding box datasets on Android. It exports directly to Vertex AI format (JSONL) and supports multi-class labeling.Looking for beta testers who work with object detection datasets. All data stays local on device, no cloud required. No account or sign in needed aside from Google Play account to access the app and sign up for beta.Key features:\- Smooth bounding box drawing/editing\- Multi-label support per box \- CSV label import \[label name, category, optional color\]\- Export to Vertex AI JSONL or CSV  1: Join testing group: [ObjMark Test Group - Google Groups](https://groups.google.com/g/objmark-test-group)2: Wait up to 30 mins for account propagation3: Closed beta link, Android only: [https://play.google.com/store/apps/details?id=com.jdj.creates.ObjMarkApp](https://play.google.com/store/apps/details?id=com.jdj.creates.ObjMarkApp)Feedback appreciated, especially on export format compatibility and annotation workflow..
Submitted a paper to AAAI. Most things look fine, but two reviewer points are confusing:* A reviewer cited another paper and claimed it outperforms ours, but the metrics in that cited paper are actually *lower* than ours.* Another reviewer recommended rejection for ‚Äúmissing training details,‚Äù even though we included them in the supplementary and one-line mentioned them in the main text. (also the review appears to be too harsh)**Questions:**1. For those with AAAI experience, how effective is the **Author Review Evaluation** in practice? Does it meaningfully influence the meta-review/decision?2. What exactly does the **Ethics Chair Author Comment** do, and in what situations should it be used instead of (or in addition to) the Author Review Evaluation?Thank you!.
Heyy . We are stuck in a problem regarding the Amazon ML challenge 2025 .We have formulated a solution but it is not getting us in the top 50 required to qualify for next stage . We are thinking of Fine tuning a Multimodal model available on hugging face .Problem statement :The challenge is to build an ML model that predicts product prices using text data (catalog_content) and image data (image_link) from e-commerce products.You‚Äôll train the model on 75K labeled samples and predict prices for 75K test samples.Evaluation is based on SMAPE (Symmetric Mean Absolute Percentage Error) - lower is better.Now , I need few tips regarding this because I've never worked on fine tuning an llm before . Firstly , which model should I use and with how many parameters . Secondly , We don't have good GPUs for this , Should I purchase the Pro version of Google colab . And If I do purchase it , will the training be possible before 12 AM tomorrow ? .
I am trying to research world models to see what it can power? I see current demos are built more focused as visual world like https://marble.worldlabs.ai/I was curious if the underlying architecture can be used for more generic use cases like making models learn about an environment - say an engineering infra of a company (like services and the connections between them and infra)?https://www.reddit.com/r/MachineLearning/comments/1kf3pes/discussion_what_exactly_are_world_models_in_ai/.
Natural language translation dataset in a specified domainIs a natural language translation dataset from ENG to another language in a very specific domain worthwhile to curate for conference submission?I am a part-time translator working in this specific domain who is originally a student wondering if this could be a potential submission. I have quite several peers who are willing to put in the effort to curate a decent sized dataset (~2k) translated scripts for research use for conference submission.However, I am not quite confident as to how useful or meaningful of a contribution this will be to the community..
I like to watch videos to quickly catch up on literature before deciding what to read more carefully.I am looking for YouTube videos about using RL to train reasoning models. I am interested in both both overview videos and videos about specific approaches.There are a number of influencers (for the lack of a better term). Way too superficial for my taste. I am interested in videos of scientific talks.Any suggestions?.
Been pulling my hair out trying to run inference on patient scans without exposing PHI. Legal wouldn't let us use standard cloud providers, on-prem was too expensive, and homomorphic encryption made everything 100x slower.Tried everything from differential privacy to federated learning but nothing really worked for production. Stumbled onto TEE computing through phala network and honestly thought it was too good to be true. But after testing, we're getting 95% of normal speed while keeping data encrypted during processing.The crazy part is how simple the deployment was compared to our previous attempts. No more explaining to compliance why our encryption is ""probably safe enough."" The hardware attestation just proves it mathematically.Anyone else dealing with similar privacy requirements? Curious what others are using for sensitive inference workloads..
[Image by author](https://preview.redd.it/clw9ynmozkuf1.png?width=1400&format=png&auto=webp&s=34ac2eed158c3600bd198c414c6edf9a4582e975)I‚Äôve been working with R‚Äôs MissForest for some time, and I recently ran into a subtle limitation that‚Äôs easy to miss.The algorithm is powerful for imputation, but when used in predictive settings, it quietly breaks a key principle: the separation between training and test data.This led me to explore why MissForest fails in such cases, and how the newer `MissForestPredict` approach resolves this issue by preserving consistency between learning and application.I wrote a short piece that explains this clearly.üëâ [https://medium.com/@jumbongjunior/why-the-r-missforest-fails-in-prediction-tasks-a-key-limitation-you-need-to-keep-in-mind-33e54f8fe69a](https://medium.com/@jumbongjunior/why-the-r-missforest-fails-in-prediction-tasks-a-key-limitation-you-need-to-keep-in-mind-33e54f8fe69a)I‚Äôd love to hear how others handle similar imputation issues in their predictive workflows..
I understand that this year's NeurIPS will be held in two locations: San Diego and Mexico City. My paper has been accepted, but I haven't been notified yet about where I will be presenting. However, on the registration page, the fees are different depending on the presentation location.I was wondering what the situation is for other people in a similar position..
Did anyone get the notification? Early registration deadline is coming up, and wondering if I missed it..
[https://github.com/deepseek-ai/DeepSeek-V3.2-Exp/blob/main/DeepSeek\_V3\_2.pdf](https://github.com/deepseek-ai/DeepSeek-V3.2-Exp/blob/main/DeepSeek_V3_2.pdf)The new DeepSeek model uses a novel sparse attention mechanism, with a lightning indexer and a token selection mechanism. Please feel free to discuss in this thread :)Are there any open-source implementations of this (eg. in PyTorch) that can be used for training transformers from scratch? The DeepSeek implementation involves FlashMLA kernel, which seems rather complex.[https://github.com/deepseek-ai/FlashMLA/pull/98](https://github.com/deepseek-ai/FlashMLA/pull/98).
I‚Äôve been quietly working on something I think is pretty cool, and I‚Äôd love your thoughts before I open-source it.I wanted to see if we could compress 1D convolutional networks without losing a single bit of accuracy‚Äîspecifically for signals that are periodic or treated as periodic (like ECGs, audio loops, or sensor streams). The idea isn‚Äôt new in theory but I want to explore it as best as I can.So I built a wrapper that stores only the first row of each convolutional kernel (e.g., 31 values instead of 31,000) and runs inference entirely via FFT. No approximations. No retraining.On every single record in PTB-XL (clinical ECGs), the output matches the baseline PyTorch Conv1d to within 7.77e-16‚Äîwhich is basically numerically identical.I‚Äôm also exploring quiver representation theory to model multi-signal fusion (e.g., ECG + PPG + EEG as a directed graph of linear maps), but even without that layer, the core compression is solid.If there‚Äôs interest, I‚Äôll clean it up and release it under a permissive license as soon as I can.Edit: Apologies, the original post was too vague.For those asking about the ""first row of the kernel"" ‚Äî that's my main idea. The trick is to think of the convolution not as a small sliding window, but as a single, large matrix multiplication (the mathematical view). For periodic signals, this large matrix is a circulant matrix. My method stores only the first row of that large matrix.That single row is all you need to perfectly reconstruct the entire operation using the FFT. So, to be perfectly clear: I'm compressing the model parameters, not the input data. That's the compression.Hope that makes more sense now.GitHub Link: https://github.com/fabrece/Equivariant-Neural-Network-Compressor.
Hello,I am a communications student, and as part of my thesis, I would like to collect data related to RLHF for analysis.The topic of my thesis is: Human-induced communication and intercultural biases in LLMs: the consequences of RLHF models.The data I would like to collect is the instructions given to annotators, which guide the human feedback work in the RLHF process.My goal is to analyze these different instructions, coming from different providers/nationalities, to see if the way these instructions are constructed can influence LLM learning.According to my research, this data is not publicly available, and I would like to know if there is a way to collect it for use in an academic project, using an ethical and anonymizing methodology.Is contacting subcontractors a possibility? Are there any leaks of information on this subject that could be used?Thank you very much for taking the time to respond, and for your answers!Have a great day..
My team‚Äôs realizing we don‚Äôt need a billion-parameter model to solve our actual problem, a smaller custom model works faster and cheaper. But there‚Äôs so much hype around bigger is better. Curious what others are using for production cases..
Edit: Sorry for the incomplete title. I meant: ‚ÄúRebuttal cannot agree and correct factual error?‚ÄùI am a bit confused this year. In the guidelines, the following is stated: ‚ÄúAuthors are discouraged from discussing new results or planned improvements, as reviewers are only able to evaluate the paper as originally submitted‚Äù.Thus, imagine I have a theorem and a reviewer is pointing out an error in it. In other words, this is a factual error that I agree with, but correcting it is simple and does not imply modifying the rest of the paper. Can I not correct it and say I corrected it?.
Evolving visual environments pose significant challenges for continual semantic segmentation, introducing complexities such as class-incremental learning, domain-incremental learning, limited annotations, and the need to leverage unlabeled data. FoSSIL (Few-shot Semantic Segmentation for Incremental Learning) provides a comprehensive benchmark for continual semantic segmentation, covering both 2D natural scenes and 3D medical volumes. The evaluation suite includes diverse and realistic settings, utilizing both labeled (few-shot) and unlabeled data.Building on this benchmark, **guided noise injection** is introduced to mitigate overfitting arising from novel few-shot classes across diverse domains. **Semi-supervised learning** is employed to effectively leverage unlabeled data, augmenting the representation of few-shot novel classes. Additionally, a **novel pseudo-label filtering mechanism** removes highly confident yet incorrectly predicted labels, further improving segmentation accuracy. These contributions collectively offer a robust approach to continual semantic segmentation in complex, evolving visual environments.Evaluation across class-incremental, few-shot, and domain-incremental scenarios, both with and without unlabeled data, demonstrates the efficacy of the proposed strategies in achieving robust semantic segmentation under complex, evolving conditions. The framework provides a systematic and effective approach for continual semantic segmentation in dynamic real-world environments. Extensive benchmarking across natural 2D and medical 3D domains reveals critical failure modes of existing methods and offers actionable insights for the design of more resilient continual segmentation models.Code: [https://github.com/anony34/FoSSIL](https://github.com/anony34/FoSSIL)Webpage: [https://anony34.github.io/Fossil\_webpage/](https://anony34.github.io/Fossil_webpage/)Theoretical analysis: [https://anony34.github.io/Fossil\_webpage/theory.html](https://anony34.github.io/Fossil_webpage/theory.html).
Just a trend I've been seeing. Incremental papers from Meta, Deepmind, Apple, etc. often getting accepted to top conferences with amazing scores or cited hundreds of times, however the work would likely never be published without the ""industry name"". Even worse, sometimes these works have apparent flaws in the evaluation/claims. Examples include:Meta Galactica LLM: Got pulled away after just 3 days for being absolutely useless. Still cited 1000 times!!!!! (Why do people even cite this?)Microsoft's quantum Majorana paper at Nature (more competitive than any ML venue), while still having several faults and was retracted heavily. This paper is infamous in the physics community as many people now joke about Microsoft quantum.Apple's illusion of thinking. (still cited a lot) (Arguably incremental novelty, but main issue was the experimentation related to context window sizes)Alpha fold 3 paper: Was accepted without any code/reproducibility initially at Nature got highly critiqued forcing them to release it. Reviewers should've not accepted before code was released (not the opposite)There are likely hundreds of other examples you've all seen these are just some controversial ones. I don't have anything against industry research, in fact I support it and I'm happy it get's published. There is certainly a lot of amazing groundbreaking work coming from industry that I love to follow and work further on. I'm just tired of people treating and citing all industry papers like they are special when in reality most papers are just okay..
Apologies if I failed to grab the concept properly. But since the applications/samples we test our model on using CodeBleu (to my knowledge atleast) isnt same across the board. How can two researchers compare the CodeBleu scores they got on each of their separate LLMs. I am talking about research papers publishing their CodeBleu Scores.To summarize, we take an example of our choice, run it using codebleu across many models and say that ours did better. Papers dont mention these examples, who is to say they didnt cherry picked a really specific one that their model performs better on. CodeBleu doesnt feels just/standardized.Or are there standard datasets to be used with CodeBleu for example a set of 100 python problems available as a standard dataset?.
Hey folks,I‚Äôve been working on a small ML project over the last month and thought it might interest some of you doing variant analysis or functional genomics.It‚Äôs a non-deep-learning model (Gradient Boosting / Random Forests) that predicts the functional impact of genetic variants (SNPs, indels) using public annotations like ClinVar, gnomAD, Ensembl, and UniProt features.The goal is to help filter or prioritize variants before downstream experiments ‚Äî for example:ranking variants from a new sequencing project,triaging ‚Äúvariants of unknown significance,‚Äù orfocusing on variants likely to alter protein function.The model uses features like:conservation scores (PhyloP, PhastCons),allele frequencies,functional class (missense, nonsense, etc.),gene constraint metrics (like pLI), andpre-existing scores (SIFT, PolyPhen2, etc.).I kept it deliberately lightweight ‚Äî runs easily on Colab, no GPUs, and trains on openly available variant data. It‚Äôs designed for research-use-only and doesn‚Äôt attempt any clinical classification.I‚Äôd love to hear feedback from others working on ML in genomics ‚Äî particularly about useful features to include, ways to benchmark, or datasets worth adding.If anyone‚Äôs curious about using a version of it internally (e.g., for variant triage in a research setting), you can DM me for details about the commercial license.Happy to discuss technical stuff openly in the thread ‚Äî I‚Äôm mostly sharing this because it‚Äôs been fun applying classical ML to genomics in a practical way.
I am an independent researcher. My submissions have recently been published in AI symposiums and in the past I have published in IEEE.¬†I'm looking to upload it to the arxiv I need an endorsement for¬†[CS.AI](http://CS.AI). Thanks in advance.Endorsement code: 69BL48[https://arxiv.org/auth/endorse?x=69BL48](https://arxiv.org/auth/endorse?x=69BL48).
Greetings,We are a small team of 6 people that work on a startup project in our free time (mainly computer vision + some algorithms etc.). So far, we have been using the roboflow platform for labelling, training models etc. However, this is very costly and we cannot justify 60 bucks / month for labelling and limited credits for model training with limited flexibility.  We are looking to see where it is worthwhile to migrate to, without needing too much time to do so and without it being too costly.  Currently, this is our situation: \- We have a small grant of 500 euros that we can utilize. Aside from that we can also spend from our own money if it's justified. The project produces no revenue yet, we are going to have a demo within this month to see the interest of people and from there see how much time and money we will invest moving forward. In any case we want to have a migration from roboflow set-up to not have delays.\- We have setup an S3 bucket where we keep our datasets (so far approx. 40GB space) which are constantly growing since we are also doing data collection. We also are renting a VPS where we are hosting CVAT for labelling. These come around 4-7 euros / month. We have set up some basic repositories for drawing data, some basic training workflows which we are trying to figure out, mainly revolving around YOLO, RF-DETR, object detection and segmentation models, some timeseries forecasting, trackers etc. We are playing around with different frameworks so we want to be a bit flexible.\- We are looking into renting VMs and just using our repos to train models but we also want some easy way to compare runs etc. so we thought something like MLFlow. We tried these a bit but it has an initial learning process and it is time consuming to setup your whole pipeline at first.  \-> What would you guys advice in our case? Is there a specific platform you would recommend us going towards? Do you suggest just running in any VM on the cloud ? If yes, where and what frameworks would you suggest we use for our pipeline? Any suggestions are appreciated and I would be interested to see what computer vision companies use etc. Of course in our case the budget would ideally be less than 500 euros for the next 6 months in costs since we have no revenue and no funding, at least currently. TL;DR - Which are the most pain-free frameworks/platforms/ways to setup a full pipeline of data gathering -> data labelling -> data storage -> different types of model training/pre-training -> evaluation -> comparison of models -> deployment on our product etc. when we have a 500 euro budget for next 6 months making our lives as much as possible easy while being very flexible and able to  train different models, mess with backbones, transfer learning etc. without issues.  Feel free to ask for any additional information.  Thanks!.
There's been some confusion about whether rebuttals should be 2500 characters **per reviewer** or 2500 characters overall. Below I posted a screenshot of the message sent out the last conference (AAAI 2025) which states that it is 2500 characters per reviewer, but this time at AAAI 2026 the wording implies that it is 2500 characters **overall for a single rebuttal covering all reviewers**.Has anyone been able to get in touch with the AAAI committee for a clarification?  https://preview.redd.it/edmmtrx7nztf1.png?width=1688&format=png&auto=webp&s=f3103ca61e91a43842773f4a193a87f276adfd3d.
Hello everyone,I would like to collaboratively define a reasonable portfolio to specialize in managing a freelance consulting business as a Data Scientist.Considering that there are people here who have worked independently as Data Scientists and have observed the types of problems clients usually bring to them.Please, let us know what kinds of problems or models you have frequently dealt with as freelance consultants. It could be interesting for all of us to share and learn together about the current state of the Data Science market.I would like to reduce the overwhelming number of Machine Learning models and potential problems in order to build potential specializations for freelance Data Science consultants.Thank you..
Avertissement important : Ce texte a √©t√© produit avec l'assistance d'une IA. Il s'agit d'une sp√©culation th√©orique destin√©e √† stimuler la discussion, et non d'une th√©orie √©tablie. Je ne suis pas expert en la mati√®re - je cherche des retours sur cette id√©e √©mergente.---Le Probl√®me Fondamental : Pourquoi les crise nous surprennent-ils ? ?Nous vivons dans un monde de syst√®mes complexes - climat, march√©s financiers, √©cosyst√®mes - qui pr√©sentent des points de basculement soudains. Malgr√© nos mod√®les sophistiqu√©s, nous √©chouons souvent √† anticiper ces transitions critiques.Exemples historiques :¬∑ La crise financi√®re de 2008 (les mod√®les n'ont pas capt√© la fragilit√© croissante)¬∑ L'effondrement de la p√™cherie de morue de Terre-Neuve (malgr√© les donn√©es abondantes)¬∑ Les transitions climatiques abruptes dans les carottes glaciairesL'Id√©e √âmergente : Mesurer la ""Sant√©"" des Relations CausalesLes mod√®les actuels se concentrent sur les variables observables (prix, temp√©ratures, populations). Et si nous devions plut√¥t mesurer la stabilit√© des relations causales elles-m√™mes ?Analogie simple :Imaginez mesurer non pas combien un pont vibre,mais la solidit√© des connexions entre ses poutres. Avant l'effondrement, ces connexions deviennent ""fragiles"" m√™me si les vibrations semblent normales.Ce Que Pourraient √ätre les ""M√©triques de Stabilit√© Causale""D'apr√®s des travaux r√©cents en mod√©lisation stochastique avanc√©e (comme le mod√®le de Ginzburg-Landau √©tendu avec m√©moire), on pourrait d√©velopper des mesures qui :1. Quantifient la ""rigidit√© causale"" - √† quel point les relations cause-effet sont stables2. Mesurent la ""r√©silience m√©morielle"" - comment le pass√© influence le pr√©sent3. Cartographient la ""coh√©rence dimensionnelle"" - si la complexit√© du syst√®me √©volue harmonieusementApplications Potentielles¬∑ Finance : D√©tecter quand les relations entre march√©s deviennent fragiles¬∑ Climat : Anticiper les changements de r√©gime m√©t√©orologiques¬∑ Biologie : Pr√©dire l'effondrement d'√©cosyst√®mes¬∑ Sant√© publique : Identifier les seuils √©pid√©miques avant qu'ils ne soient franchisPr√©cautions et Limites EssentiellesCeci est sp√©culatif et n√©cessite :1. Validation empirique rigoureuse - pour l'instant, c'est principalement th√©orique2. D√©veloppement math√©matique - les outils formels manquent encore3. Tests sur donn√©es historiques - v√©rifier r√©trospectivement si l'approche aurait fonctionn√©4. Collaboration interdisciplinaire - entre math√©maticiens, physiciens, √©cologues, √©conomistesQuestions pour la Communaut√©¬∑ Connaissez-vous des travaux similaires en math√©matiques appliqu√©es ?¬∑ Comment pourrions-nous tester exp√©rimentalement ces concepts ?¬∑ Quelles seraient les limitations fondamentales de cette approche ?¬∑ Y a-t-il des domaines o√π cette id√©e serait particuli√®rement prometteuse ?R√©f√©rences pour Approfondir¬∑ Scheffer, M. et al. (2009) ""Early-warning signals for critical transitions""¬∑ Ginzburg-Landau theory extensions with memory terms¬∑ Tipping point detection in complex systems literatureJe recherche des retours critiques et constructifs - cette id√©e en est √† ses d√©buts et a besoin d'√™tre confront√©e √† la r√©alit√© !.
Through my company, I've been given the opportunity to attend an ML conference without having a paper accepted at the venue. This is my first time attending any conference.What should I be doing to get as much as I can from the conference? I've seen other posts similar to this, but the OPs seem to have an accepted paper. I'm wondering if the advice is any different, given that I don't have an accepted paper. Some things I consider important - learning new things, making connections (esp with potential future PhD advisors).
Hey folks! I‚Äôm currently doing a PhD and need to attend a subject specific summer or winter school next year. I‚Äôm particularly interested in anything focused on diffusion models, flow models, or related areas in generative AI. If you‚Äôve attended any good ones in the UK or Europe or know of any coming up in 2026 I‚Äôd really appreciate your suggestions. Thanks in advance .
I'm new to developing with LLMs. Qwen recently released some cool multimodal models that can seamlessly work with video, text and audio. Ofc this requires a lot of GPU. Renting one from AWS costs about a dollar per hour which doesn't make sense if I'm developing something which could cost $100+ just in the development phase. Is it possible to only pay for the time you actually use the GPU and not be charged for the time it is idle? What other common ways are there to tinker and develop with these models besides dropping a lot of money? Feel like I'm missing something. I saw Baseten allows for ""pay-per-inference"" style of GPU use but I haven't explored it much yet.
I‚Äôve ported the BDH (¬†[https://github.com/pathwaycom/bdh](https://github.com/pathwaycom/bdh)¬†) model to MLX for Apple Silicon. It‚Äôs a faithful conversion of the PyTorch version: same math, same architecture (byte-level vocab, shared weights across layers, ReLU sparsity, RoPE attention with Q=K), with MLX-friendly APIs and a detailed README explaining the few API-level differences and why results are equivalent.Code, docs, and training script are ready to use. You may need to adjust the training script a bit to fit your own custom dataset. Only tested on M4 so far, but should work perfect for any M1/M2/M3 users out there.I‚Äôm currently training this MLX build on my Internal Knowledge Map (IKM) dataset¬†[https://huggingface.co/datasets/Severian/Internal-Knowledge-Map](https://huggingface.co/datasets/Severian/Internal-Knowledge-Map)Training‚Äôs underway; expect a day or so before I publish weights. When it‚Äôs done, I‚Äôll upload the checkpoint to Hugging Face for anyone to test.Repo:¬†[https://github.com/severian42/BDH-MLX](https://github.com/severian42/BDH-MLX)HF model (coming soon):¬†[https://huggingface.co/Severian/BDH-MLX](https://huggingface.co/Severian/BDH-MLX)If you try it on your own data, feedback and PRs are welcome..
 Inspired by how massive human-generated data became indispensable when paired with architectures like transformers and reinforcement learning to power modern AI‚Äîwhat emerging developments or resources are building up right now that could play a similar role in the next 10‚Äì50 years?Think of things like exploding datasets, hardware advancements, or societal shifts that, when combined with the right tools/algorithms, will become essential. For each suggestion, please cover:Prerequisites: What's needed for this resource to accumulate or mature?Means to leverage: How can it be applied (e.g., specific tech or methods)?Objective: What ultimate goals or breakthroughs could it enable?Looking for forward-thinking ideas grounded in current trends! Thank you !!.
Phase 2 reviews are out, I got 5,5,5,5,6 with several reviewers raising experimental setup/results reported issue. Can I convert some 5's to 6's with rebuttal? And what are my chances? How can I do it effectively with 2500 characters limit :(PS: Please feel free to use this thread to post your ratings and ask for rebuttal strategies. .
TL;DR The standard DPO objective struggles with mixed-quality data, a problem that `Œ≤`\-DPO addresses at the batch level; MADPO provides a more granular solution at the instance level, which leads to consistently better and more robust performance in our experiments.  I would like to get feedback on my new paper on arXiv, which builds on the data quality issue in DPO that was recently highlighted by the `Œ≤`\-DPO paper. They identified that DPO's fixed `Œ≤` struggles to handle mixed-quality data. However, their batch-level solution, while a great step, can be unstable (Adaptive `Œ≤` can be negative) and is still a coarse approximation for what is an instance-level problem. My method, MADPO (Margin-Adaptive DPO), offers a more granular approach. It uses a reward model to assign a unique weight to each sample, amplifying the loss for hard pairs and dampening it for easy ones.My experiments on a sentiment generation task show that this instance-level control is highly effective. MADPO consistently outperformed all baselines (DPO, IPO & `Œ≤`\-DPO) achieving a performance jump of up to +33.3% over `Œ≤`\-DPO on high-quality data, while still holding a +10.5% advantage on the most challenging low-quality set.     The full paper with all the theory and experimental details is on arXiv, and I would be grateful for any feedback or questions on the approach.Paper: [https://arxiv.org/abs/2510.05342](https://arxiv.org/abs/2510.05342)I am currently seeking an endorsement to allow for direct submission to the correct category for future work. Any help would be greatly appreciated. Endorsement link: [https://arxiv.org/auth/endorse?x=XUXXAE](https://arxiv.org/auth/endorse?x=XUXXAE)  .
Saw a post about Yandex Cup 2025 and they have an ML track this yearI‚Äôve done a few Kaggle comps before, so I‚Äôm wondering how their problems compare. Are they actually practical or more on the academic side?The $18k pool sounds pretty nice, but I‚Äôm trying to figure out if it‚Äôs worth my time. Registration‚Äôs open till Nov 5 apparently. Anyone planning to join or tried it?.
Most LLM training pipelines require SFT followed by some form of RHLF (classically PPO). SFT and RHLF require datasets in slightly different formats, but both formats (especially for binary choices) can be re-expressed as the other. The old DAGGER paper describes how to train a model in multiple steps with an increasing dataset enriched by annotated rollouts. Is there an advantage to using SFT+RHLF over multi-step SFT?.
I'm testing whether pretrained time series models (MOMENT, TimesFM) can learn degradation patterns with limited fine-tuning.**The issue:** These models are pretrained on cyclic/stationary data (finance, weather), but degradation is fundamentally different - non-stationary, monotonic trends toward failure, governed by physics not statistics.**Zero-shot:** I tested in Zero-shot scenarios and it was a complete failure (R¬≤ negative). Model predicts constants or cyclic patterns where none exist.**My question:**1. Can patch-based transformers even extrapolate non-stationary trends, or do they regress to cyclic priors?2. Has anyone successfully transferred foundation models from stationary‚Üínon-stationary domains? Or is this fundamentally incompatible with how these models learn?Any papers or insights are appreciated!.
I‚Äôm currently working on a research project involving oral cancer histopathological image classification, and I could really use some advice from people who‚Äôve worked with similar data.I‚Äôm trying to decide whether it‚Äôs better to collect whole slide images (WSIs) or to use captured images (smaller regions captured from slides).If I go with captured images, I‚Äôll likely have multiple captures containing cancerous tissues from different parts of the same slide (or even multiple slides from the same patient).My question is: should I treat those captures as one data point (since they‚Äôre from the same case) or as separate data points for training?I‚Äôd really appreciate any advice, papers, or dataset references that could help guide my approach..
Hi Everyone!The reviews for phase 2 have been released. Lets discuss how did it go!!.
While working on new ML architectures I struggled to stabilize training  by using countless learning-rate schedulers, gradient clippers and normalizers enough to go and implement a schedule-free optimizer.Here, [Lion Schedule-Free](https://github.com/govorunov/lion-sf) optimizer - a version of Lion optimizer that requires no learning-rate scheduler. It uses *sign agreement* \- an absolute value of cross correlation between momentum sign and gradient sign, to scale the effective update step. Not only it converges 3x times faster ON MY MODEL, by eliminating LR scheduler it also allows for hot training resume & restart. And also stabilizes training, especially late training, eliminating the need for gradient clipping, etc. The effective update depends on the training regime and can decrease or increase during training.  In this implementation, the sign agreement is calculated per-module. It's probably more logical and stable to calculate it per-parameter-group, but that's more code and since module-wise already works pretty well...The optimizer is provided as is. There will be no paper, no convergence guarantees, no ablation studies and no time to do any of that. Install it:`pip install git+https://github.com/govorunov/lion-sf.git`And use it as normal optimizer:    from lion_pytorch import LionSF        optimizer = LionSF(model.parameters(), lr=5e-4, betas=(0.9, 0.99), weight_decay=1e-2)Give it a generous base learning rate, like 5e-4 or more, and ditch LR scheduler completely. You can also ditch gradient clipping (as I did).If you want to resume / restart training later from a checkpoint - keep the optimizer state, do a hot-restart. There is no need to warm-up - it will restart gently naturally. The ability to do a hot-restart and increased training stability is probably more important (for me) than even faster convergence, although faster convergence looks better on plots..
Hey everyone! I‚Äôve been reading about generative models, especially flow models for image generation starting from Gaussian noise. In the process, I started to think if there is any merit to introducing exogenous inputs to drive the system to a particular direction through predictive control algorithms (MPC, MPPI) . Especially, what are some important constraints and stage costs one could incorporate (not just terminal constraints)? I am not super knowledgable about the nature of the image space itself and I couldn‚Äôt find much literature on the internet regarding predictive control. Any suggestions would really help! Thank you!.
Hi everyone, I wanted to share a project we‚Äôve been working on around a challenge we call **persona drift** in large language models.When you run long sessions with LLMs (especially across multi-turn or multi-agent chains), the model often **loses consistency in tone, style, or identity** ‚Äî even when topic and context are preserved.This issue is rarely mentioned in academic benchmarks, but it‚Äôs painfully visible in real-world products (chatbots, agents, copilots). It‚Äôs not just ‚Äúforgetting‚Äù ‚Äî it‚Äôs **drift in the model‚Äôs semantic behavior** over time.We started studying this while building our own agent stack, and ended up designing a middleware called **Echo Mode** ‚Äî a **finite-state protocol** that adds a stability layer between the user and the model.Here‚Äôs how it works:* We define **four conversational states**: Sync, Resonance, Insight, and Calm ‚Äî each has its own heuristic expectations (length, tone, depth).* Each state transition is governed by a lightweight FSM (finite-state machine).* We measure a **Sync Score** ‚Äî a BLEU-like metric that tracks deviation in tone and structure across turns.* A simple **EWMA-based repair loop** recalibrates the model‚Äôs outputs when drift exceeds threshold.This helps agents **retain their ‚Äúvoice‚Äù** over longer sessions without needing constant prompt re-anchoring.We‚Äôve just released the **open-source version** (Apache-2.0): [**GitHub ‚Äì Echo Mode**](https://github.com/Seanhong0818/Echo-Mode)We‚Äôre also building a **closed-source enterprise layer (EchoMode.io)** that expands on this ‚Äî with telemetry, Sync Score analytics, and an API to monitor tone drift across multiple models (OpenAI, Anthropic, Gemini, etc.).I‚Äôd love to hear from anyone studying **behavioral consistency, semantic decay, or long-term agent memory** ‚Äî or anyone who‚Äôs seen similar issues in RLHF or multi-turn fine-tuning.*(mods: not a product pitch ‚Äî just sharing a middleware and dataset approach for a rarely discussed aspect of LLM behavior.)*.
Is there any specific template for EMNLP Posters? I cannot find it on the instructions themselves. Thanks .
Hello, I‚Äôm a PhD student about to start my first research project in applied ML, and I‚Äôd like to get the structure right from the beginning instead of refactoring everything later.Are there any solid ‚Äúbest-practice‚Äù resources or example repositories that one could recommend? I‚Äôm especially keen on making sure I get the following right:* Containerization* Project structure for reproducibility and replication* Managing experiments, environments, and dependenciesThanks in advance for any pointers!.
Anyone received aaai phase 2 reviews?.
Eigen Vectors are one of the foundational pillars of modern day , data handling mechanism. The concepts also translate beautifully to plethora of other domains.  Recently while revisiting the topic, had the idea of visualizing the concepts and reiterating my understanding.Sharing my visualization experiments here :¬†[https://colab.research.google.com/drive/1-7zEqp6ae5gN3EFNOG\_r1zm8hzso-eVZ?usp=sharing](https://colab.research.google.com/drive/1-7zEqp6ae5gN3EFNOG_r1zm8hzso-eVZ?usp=sharing)If interested in few more resources and details, you can have a look at my linkedin post :¬†[https://www.linkedin.com/posts/asmita-mukherjee-data-science\_google-colab-activity-7379955569744474112-Zojj?utm\_source=share&utm\_medium=member\_desktop&rcm=ACoAACA6NK8Be0YojVeJomYdaGI-nIrh-jtE64c](https://www.linkedin.com/posts/asmita-mukherjee-data-science_google-colab-activity-7379955569744474112-Zojj?utm_source=share&utm_medium=member_desktop&rcm=ACoAACA6NK8Be0YojVeJomYdaGI-nIrh-jtE64c)Please do share your learnings and understanding. I have also been thinking of setting up a community in discord (to start with) to learn and revisit the fundamental topics and play with them. If anyone is interested, feel free to dm with some professional profile link (ex: website, linkedin, github etc)..
Hi everyone! I just want to share ExoSeeker, a machine learning web interface, I created for the NASA Space Apps Challenge this year.¬†It allows anyone to upload data of potential exoplanets, planets outside the Solar System, from the Kelper mission, a space telescope designed to hunt for Earth-sized planets orbiting stars in the Milky Way, and train a custom machine learning model, select classifiers and tweak their main hyperparameters, on it.¬†You can freely build their own model by selecting from multiple estimators (random forest, gradient boosting, and multi-layer perceptron) and adjust each one's primary hyperparameters. After model training, you upload a new dataset without the exoplanet disposition, with only the feature to run predictions on it using the saved model.Github Repository:¬†[https://github.com/gospacedev/exoseeker](https://github.com/gospacedev/exoseeker)NASA Space Apps Challenge ExoSeeker Project Description:¬†[https://www.spaceappschallenge.org/2025/find-a-team/exoseeker/?tab=project](https://www.spaceappschallenge.org/2025/find-a-team/exoseeker/?tab=project).
Hi r/MachineLearning,  I wrote this blog post (https://mindfulmodeler.substack.com/p/6-things-i-hate-about-shap-as-a-maintainer) to share all the things that can be improved about SHAP, to help potential newcomers see areas of improvements (though we also have ""good first issues"" of course) and also to get some feedback from the community.   Brief summary:  1. explainers can be slow, e.g. if relying on the ExactExplainer or PermutationExplainer  2. DeepExplainer does not support a lot of layers and for tensorflow the LSTM is not working anymore (for more information see the article)  3. TreeExplainer has a bunch of problems: it's legacy code, we discovered some memory issues and there are a couple open issues addressing bugs there  4. we are in dependency hell: lots of upstream packages break our pipelines regularly which is a huge maintenance burden  5. The plotting API is dated and not well tested, so a rewrite is hard  6. Other things: No JAX support, missing type annotations, etc.    Anything you want to be fixed or improved about the project? Any reason why you don't use it anymore?   Very happy to talk about this here..
How did everyone's results go? .
Looking to interview people who‚Äôve worked on audio labeling for ML (PhD research project)Hi everyone,I‚Äôm a PhD candidate in Communication researching modern sound technologies. My dissertation is a cultural history of audio datasets used in machine learning: I‚Äôm interested in how sound is conceptualized, categorized, and organized within computational systems.I‚Äôm currently looking to speak with people who have done audio labeling or annotation work for ML projects (academic, industry, or open-source). These interviews are part of an oral history component of my research.Specifically, I‚Äôd love to hear about:- how particular sound categories were developed or negotiated,- how disagreements around classification were handled, and- how teams decided what counted as a ‚Äúgood‚Äù or ‚Äúusable‚Äù data point.If you‚Äôve been involved in building, maintaining, or labeling sound datasets - from environmental sounds to event ontologies - I‚Äôd be very grateful to talk. Conversations are confidential, and I can share more details about the project and consent process if you‚Äôre interested.You can DM me hereThanks so much for your time and for all the work that goes into shaping this fascinating field..
Hey r/ML,    I've been working on autonomous agents that use recursive self-reflection  (think Reflexion-style setups), and kept running into this weird failure mode  that I couldn't find documented anywhere.    The Problem:    When you let an agent repeatedly reflect on its own reasoning - like having  it critique its outputs, update its approach, then critique \*that\* approach,  etc - the belief embeddings slowly drift away from the original values.    Not catastrophic forgetting (different thing). Not hallucination. More like...  the agent gradually forgets ""who it is"" across reflection cycles.    I'm calling it Recursive Belief Drift (RBD). Maybe someone has a better name?    Why This Matters:    If you're building:  \- Long-running conversational agents  \- Self-improving systems (agents that modify their own prompts/code)  \- Multi-agent systems where identity consistency matters    ...this drift becomes a real problem around 50-100 reflection cycles.    My Approach:    Tried a bunch of things. What ended up working was inspired by MIT's recent  LinOSS work on neural oscillations - basically treating belief updates as a  damped oscillator instead of pure accumulation:g(t) = exp(-Œ±t) \* sin(œât) B\_t+1 = B\_t + Œª \* g(t) \* correctionInstead of beliefs drifting monotonically, they oscillate around a stable  point. Kind of like making the agent ""breathe"" instead of constantly tensing up.    Results:    Tested on 50 reflection cycles with sentence-transformers:  \- No damping: mean drift \~0.085 (bad)  \- Harmonic damping: mean drift \~0.009 (much better)    About 9x improvement in stability, though obviously this depends heavily on  your specific setup.    Code:    Open sourced everything here: [https://github.com/Freeky7819/harmonic-agent](https://github.com/Freeky7819/harmonic-agent)    There's a Colab notebook if you want to just try it:  [https://colab.research.google.com/drive/1zt4YUAnMuDl17wcqHdsvKoaSUaO01ZHO](https://colab.research.google.com/drive/1zt4YUAnMuDl17wcqHdsvKoaSUaO01ZHO)    Honest Limitations:    \- Parameters (Œª, œâ, Œ±) are hand-tuned. Haven't found a good way to learn them yet.  \- Only tested with embedding-based belief representations. Not sure how this  ¬† translates to pure symbolic approaches.  \- ""Correction vectors"" in my test are just noise. Real agent corrections would  ¬† be more structured.  \- Small-scale tests only (50 cycles, \~400 dim embeddings)    Questions for the Community:    1. Has anyone seen this RBD problem documented elsewhere? I feel like I'm  ¬†¬† reinventing the wheel here.    2. Better ways to set oscillation parameters? I tried grid search but it's  ¬†¬† expensive and use-case dependent.    3. Any theoretical reason why this \*wouldn't\* scale to larger embedding spaces  ¬†¬† or longer timescales?    4. Could this be integrated with existing frameworks like LangChain or AutoGen  ¬†¬† without major refactoring?    Feedback/criticism very welcome. Still figuring this out.    \---    Links:  \- GitHub: [https://github.com/Freeky7819/harmonic-agent](https://github.com/Freeky7819/harmonic-agent)  \- Colab Demo: [https://colab.research.google.com/drive/1zt4YUAnMuDl17wcqHdsvKoaSUaO01ZHO](https://colab.research.google.com/drive/1zt4YUAnMuDl17wcqHdsvKoaSUaO01ZHO)  \- Comparison visualizations in the repo    Related Work:  \- MIT LinOSS (2025): Harmonic oscillators for ML stability  \- Reflexion (Shinn et al., 2023): Self-reflection framework this builds on  \- Agent Drift paper (Ponnambalam, 2025): Documents similar issues    Yes, I know the title says ""agent"" but this is really about maintaining  stable belief representations. ""Agent"" might be overselling it. Open to better terminology.¬†.
Hi all,I‚Äôm struggling with Tensorflow and an old Musicnn embbeding and classification model that I get form the Essentia project.To say in short seems that in same CPU it doesn‚Äôt work.Initially I collect issue on old CPU due to the missing support of AVX, and I can live with the fact of not support very old CPU.Now I discovered that also some ‚Äúnot old‚Äù cpu have some different rappresentation of number that broke the model with some memory error.The first issue that i fix was this:https://github.com/NeptuneHub/AudioMuse-AI/issues/73It was an intel i5 1035G1 processor that by default used float64 instead of the float32 used by the model. Just adding a cast in my code I solved the problem, good.Some days ago an user with an AMD Ryzen AI 9 HX 370 had similar problem herehttps://github.com/NeptuneHub/AudioMuse-AI/issues/93I try to check if ‚ÄúI miss some cast somewhere‚Äù but I wasn‚Äôt able to find a solution in that way. I instead found that by setting this env variable:ENV TF_ENABLE_ONEDNN_OPTS=0The model start working but giving ‚Äúcorrect‚Äù value but with a different scale. So the probability of a tag (the genre of the song) instead of be around 0.1 or 0.2 arrived to 0.5 or 0.6.So here my question: why? How can achieve that Tensorflow work on different CPU and possibly giving similar value?I think can be ok if the precision is not the exact one, but have the double or the triple of the value to me sounds strange and I don‚Äôt know which impact can have on the rest of my application.I mainly use:The Musicnn embbeding rappresentation to do similarity song between embbeding itself. Then I use for a secondary purpose the tag itself with the genre. Any suggestion ? Eventually any good alternative to Tensorflow at all that could be more ‚Äústable‚Äù and that I can use in python ? (My entire app is in python).Just for background the entire app is opensource (and free) on GitHub. If you want to inspect the code it is in task/analysis all the part that use Librosa+Tensorflow for this analysis (yes the model was from Essentia, but I‚Äôm reusing reading the song with Librosa because seems more updated and support ARM on Linux)..
It seems like simple `model.generate()` calls are incredibly slow on TPUs (basically stuck after one inference), does anyone have simple solutions for using torch XLA on TPUs? This seems to be an ongoing issue in the HuggingFace repo.I tried to find something the whole day, and came across solutions like optimum-tpu (only supports some models + as a server, not simple calls), using Flax Models (again supports only some models and I wasn't able to run this either), or sth that converts torch to jax and then we can use it (like ivy). But these seem too complicated for the simple problem, I would really appreciate any insights!!.
I work in a small ML startup and our data scientists are split,  half want to keep building new architectures, half want to refine and deploy what‚Äôs working. Feels like we‚Äôre spinning wheels instead of improving performance in production. How do you usually balance innovation vs iteration?.
[https://www.kaggle.com/datasets/ziya07/high-speed-train-bogie-vibration-and-fault-diagnosis/data](https://www.kaggle.com/datasets/ziya07/high-speed-train-bogie-vibration-and-fault-diagnosis/data)This is a dataset of Train Bogey Vibrations. I have tried everything, extracted time domain features, extracted frequency domain features, extracted time-freq features like wavelet etc. Tried Classical ML ,Tried 1d conv on raw data, Tried sliding window approach and 2d conv, Tried anomaly detection. But i cant make the accuracy more than 55%. Please help me understand this data and modelling this data .
Hi,I am currently building an anomaly detection method on abnormal product returns. Was wondering, what would be a suitable Baseline model to compare against say LoF or IsolationForest?Thanks.
I'm sorry for this post on this sub. I know it's a wrong place but couldn't find a better one.I'm a PhD Student in ML at a decently reputed research team but in a niche field. But most of my work is machine-learning and stats heavy. (Btw Europe Location)I really want to get a good internship at a big tech to get into high-profilic research network and also for my CV. I feel like I have above-average profile and will make to sure to make it better before I apply. I also have my PI's backing and internal recommendation if I find one position.1. Is competition huge for getting into Google (Research, DeepMind), MSFT, Amazon, Meta Research, etc,. How can I make best out of my application? What do they generally look for?2. Does cold-emailing work in this case? 3. I see that some PhD intern roles (like for Google) specifically asks for students in their final year. Is it a hard requirement? Or do they also interview students in their 1/2nd year.4. In case if I don't get a chance at mentioned places, should I still go for other reputed companies or target top universities (for visiting researcher) instead?5. I would like to connect to people who have some experience going through this :)Thanks!.
I'm planning to fine-tune LLaMA 3.2 11B Instruct on a JSONL dataset of domain-specific question-answer pairs ‚Äî purely text, no images. The goal is to improve its instruction-following behavior for specialized text tasks, while still retaining its ability to handle multimodal inputs like OCR and image-based queries.I am using Axolotlhttps://github.com/axolotl-ai-cloud/axolotl/blob/main/examples/llama-3-vision/lora-11b.yamlin examples we have a sample .yaml file for this```base_model: alpindale/Llama-3.2-11B-Vision-Instruct# optionally might have model_type or tokenizer_type or processor_typeprocessor_type: AutoProcessor# Automatically upload checkpoint and final model to HF# hub_model_id: username/custom_model_name# these 3 lines are needed for now to handle vision chat templates w imagesskip_prepare_dataset: trueremove_unused_columns: falsesample_packing: falsechat_template: llama3_2_visiondatasets:  - path: HuggingFaceH4/llava-instruct-mix-vsft    type: chat_template    split: train[:1%]dataset_prepared_path:val_set_size: 0.0output_dir: ./outputs/outadapter: loralora_model_dir:sequence_len: 8192pad_to_sequence_len: falselora_r: 32lora_alpha: 16lora_dropout: 0.05lora_target_modules: 'model.language_model.layers.[\d]+.(mlp|cross_attn|self_attn).(up|down|gate|q|k|v|o)_proj'wandb_project:wandb_entity:wandb_watch:wandb_name:wandb_log_model:gradient_accumulation_steps: 4micro_batch_size: 1num_epochs: 1optimizer: adamw_bnb_8bitlr_scheduler: cosinelearning_rate: 0.0002bf16: truefp16:tf32: truegradient_checkpointing: truelogging_steps: 1# flash_attention: true  # use for text-only modesdp_attention: truewarmup_ratio: 0.1evals_per_epoch: 1saves_per_epoch: 1weight_decay: 0.0# save_first_step: true  # uncomment this to validate checkpoint saving works with your config```based on which I have made a similar .yaml file```base_model: alpindale/Llama-3.2-11B-Vision-Instructprocessor_type: AutoProcessortokenizer_config: <path_to_custom_tokenizer>tokenizer_type: AutoTokenizer# Vision-chat template handling# skip_prepare_dataset: true# remove_unused_columns: false# sample_packing: falsechat_template: llama3_2_visiondatasets:  - path: <path_to_dataset>    type: chat_template    field_messages: messages    message_property_mappings:      role: role      content: content    roles:      system:         - system      user:         - user      assistant:         - assistant    train_on_inputs: falseoutput_dir: <path_to_output_directory># Training parameterssequence_len: 8192pad_to_sequence_len: falsegradient_accumulation_steps: 4micro_batch_size: 1num_epochs: 1optimizer: adamw_bnb_8bitlr_scheduler: cosinelearning_rate: 0.0002weight_decay: 0.0warmup_ratio: 0.1# Precision & performancebf16: truefp16:tf32: truegradient_checkpointing: truelogging_steps: 1flash_attention: true   # text-only mode# sdp_attention: true# Checkpointingevals_per_epoch: 1saves_per_epoch: 1save_first_step: truesave_total_limit: 3weight_decay: 0.0special_tokens:  pad_token: <|end_of_text|>```but when i run`axolotl train config.yaml`and I have processor_type:```base_model: alpindale/Llama-3.2-11B-Vision-Instructprocessor_type: AutoProcessortokenizer_config: <path_to_custom_tokenizer>tokenizer_type: AutoTokenizer```I get the error`KeyError: 'Indexing with integers is not available when using Python based feature extractors'`but when i remove the field ```base_model: alpindale/Llama-3.2-11B-Vision-Instructtokenizer_config: <path_to_custom_tokenizer>tokenizer_type: AutoTokenizer```or even```base_model: alpindale/Llama-3.2-11B-Vision-Instructprocessor_type: AutoProcessortokenizer_config: <path_to_custom_tokenizer># Vision-chat template handlingskip_prepare_dataset: trueremove_unused_columns: falsesample_packing: false```I get the error`AttributeError: 'MllamaTextSelfAttention' object has no attribute 'is_causal'`What happened here?How does one do this?Will this fine-tuning lead to loss of Vision Capabilities of the model?Is there a guide to writing config.yaml files for different models?Python Version: 3.12Axolotl Version: LatestDataset: a .jsonl with ```{	""messages"": 	[		{""role"": ""system"", ""content"": ""<system_prompt>""}, 		{""role"": ""user"", ""content"": ""<question>""}, 		{""role"": ""assistant"", ""content"": ""<answer>""}	]}```which was previously used to fine tune Llama3.1 8B using the following config.yaml```base_model: NousResearch/Meta-Llama-3.1-8B-Instructtokenizer_config: <path_to_custom_tokenizer>tokenizer_type: AutoTokenizerchat_template: llama3datasets:  - path: <path_to_dataset>    type: chat_template    field_messages: messages    message_property_mappings:      role: role      content: content    roles:      system:        - system      user:        - user      assistant:        - assistanttrain_on_inputs: falseoutput_dir: <path_to_output_directory>sequence_len: 2048sample_packing: truegradient_accumulation_steps: 8micro_batch_size: 2num_epochs: 4optimizer: paged_adamw_8bitlr_scheduler: cosinelearning_rate: 2e-5bf16: autotf32: falsegradient_checkpointing: truegradient_checkpointing_kwargs:  use_reentrant: falseresume_from_checkpoint:auto_resume_from_checkpoints: truesave_only_model: falselogging_steps: 1flash_attention: truewarmup_ratio: 0.1evals_per_epoch: 2saves_per_epoch: 1save_total_limit: 3weight_decay: 0.0special_tokens:  pad_token: <|end_of_text|>```Thank you.I'm planning to fine-tune LLaMA 3.2 11B Instruct on a JSONL dataset of domain-specific question-answer pairs ‚Äî purely text, no images. The goal is to improve its instruction-following behavior for specialized text tasks, while still retaining its ability to handle multimodal inputs like OCR and image-based queries.I am using Axolotlhttps://github.com/axolotl-ai-cloud/axolotl/blob/main/examples/llama-3-vision/lora-11b.yamlin examples we have a sample .yaml file for this```base_model: alpindale/Llama-3.2-11B-Vision-Instruct# optionally might have model_type or tokenizer_type or processor_typeprocessor_type: AutoProcessor# Automatically upload checkpoint and final model to HF# hub_model_id: username/custom_model_name# these 3 lines are needed for now to handle vision chat templates w imagesskip_prepare_dataset: trueremove_unused_columns: falsesample_packing: falsechat_template: llama3_2_visiondatasets:  - path: HuggingFaceH4/llava-instruct-mix-vsft    type: chat_template    split: train[:1%]dataset_prepared_path:val_set_size: 0.0output_dir: ./outputs/outadapter: loralora_model_dir:sequence_len: 8192pad_to_sequence_len: falselora_r: 32lora_alpha: 16lora_dropout: 0.05lora_target_modules: 'model.language_model.layers.[\d]+.(mlp|cross_attn|self_attn).(up|down|gate|q|k|v|o)_proj'wandb_project:wandb_entity:wandb_watch:wandb_name:wandb_log_model:gradient_accumulation_steps: 4micro_batch_size: 1num_epochs: 1optimizer: adamw_bnb_8bitlr_scheduler: cosinelearning_rate: 0.0002bf16: truefp16:tf32: truegradient_checkpointing: truelogging_steps: 1# flash_attention: true  # use for text-only modesdp_attention: truewarmup_ratio: 0.1evals_per_epoch: 1saves_per_epoch: 1weight_decay: 0.0# save_first_step: true  # uncomment this to validate checkpoint saving works with your config```based on which I have made a similar .yaml file```base_model: alpindale/Llama-3.2-11B-Vision-Instructprocessor_type: AutoProcessortokenizer_config: <path_to_custom_tokenizer>tokenizer_type: AutoTokenizer# Vision-chat template handling# skip_prepare_dataset: true# remove_unused_columns: false# sample_packing: falsechat_template: llama3_2_visiondatasets:  - path: <path_to_dataset>    type: chat_template    field_messages: messages    message_property_mappings:      role: role      content: content    roles:      system:         - system      user:         - user      assistant:         - assistant    train_on_inputs: falseoutput_dir: <path_to_output_directory># Training parameterssequence_len: 8192pad_to_sequence_len: falsegradient_accumulation_steps: 4micro_batch_size: 1num_epochs: 1optimizer: adamw_bnb_8bitlr_scheduler: cosinelearning_rate: 0.0002weight_decay: 0.0warmup_ratio: 0.1# Precision & performancebf16: truefp16:tf32: truegradient_checkpointing: truelogging_steps: 1flash_attention: true   # text-only mode# sdp_attention: true# Checkpointingevals_per_epoch: 1saves_per_epoch: 1save_first_step: truesave_total_limit: 3weight_decay: 0.0special_tokens:  pad_token: <|end_of_text|>```but when i run`axolotl train config.yaml`and I have processor_type:```base_model: alpindale/Llama-3.2-11B-Vision-Instructprocessor_type: AutoProcessortokenizer_config: <path_to_custom_tokenizer>tokenizer_type: AutoTokenizer```I get the error`KeyError: 'Indexing with integers is not available when using Python based feature extractors'`but when i remove the field ```base_model: alpindale/Llama-3.2-11B-Vision-Instructtokenizer_config: <path_to_custom_tokenizer>tokenizer_type: AutoTokenizer```or even```base_model: alpindale/Llama-3.2-11B-Vision-Instructprocessor_type: AutoProcessortokenizer_config: <path_to_custom_tokenizer># Vision-chat template handlingskip_prepare_dataset: trueremove_unused_columns: falsesample_packing: false```I get the error`AttributeError: 'MllamaTextSelfAttention' object has no attribute 'is_causal'`What happened here?How does one do this?Will this fine-tuning lead to loss of Vision Capabilities of the model?Is there a guide to writing config.yaml files for different models?Python Version: 3.12Axolotl Version: LatestDataset: a .jsonl with ```{	""messages"": 	[		{""role"": ""system"", ""content"": ""<system_prompt>""}, 		{""role"": ""user"", ""content"": ""<question>""}, 		{""role"": ""assistant"", ""content"": ""<answer>""}	]}```which was previously used to fine tune Llama3.1 8B using the following config.yaml```base_model: NousResearch/Meta-Llama-3.1-8B-Instructtokenizer_config: <path_to_custom_tokenizer>tokenizer_type: AutoTokenizerchat_template: llama3datasets:  - path: <path_to_dataset>    type: chat_template    field_messages: messages    message_property_mappings:      role: role      content: content    roles:      system:        - system      user:        - user      assistant:        - assistanttrain_on_inputs: falseoutput_dir: <path_to_output_directory>sequence_len: 2048sample_packing: truegradient_accumulation_steps: 8micro_batch_size: 2num_epochs: 4optimizer: paged_adamw_8bitlr_scheduler: cosinelearning_rate: 2e-5bf16: autotf32: falsegradient_checkpointing: truegradient_checkpointing_kwargs:  use_reentrant: falseresume_from_checkpoint:auto_resume_from_checkpoints: truesave_only_model: falselogging_steps: 1flash_attention: truewarmup_ratio: 0.1evals_per_epoch: 2saves_per_epoch: 1save_total_limit: 3weight_decay: 0.0special_tokens:  pad_token: <|end_of_text|>```Thank you..
Hi r/MachineLearning, here is my weekend project: [chess-cv](https://github.com/S1M0N38/chess-cv)A machine learning project that trains a lightweight CNN (156k parameters) from scratch to classify chess pieces from 32√ó32 pixel square images. The model achieves ~99.85% accuracy on synthetic training data generated by combining 55 board styles (256√ó256px) with 64 piece sets (32√ó32px) from chess.com and lichess.By rendering pieces onto different board backgrounds and extracting individual squares, the model learns robust piece recognition across various visual styles.| Dataset                                                                                  | Accuracy | F1-Score (Macro) || ---------------------------------------------------------------------------------------- | :--------: | :----------------: || Test Data                                                                                | 99.85%   | 99.89%           || [S1M0N38/chess-cv-openboard](https://huggingface.co/datasets/S1M0N38/chess-cv-openboard) | -    | 95.78%           |(OpenBoard has an unbalanced class distribution (many more samples for empty square class, so accuracy is not representative )Happy to hear any feedback!.
I'm tinkering with an application of human pose estimation which [fails miserably](https://i.imgur.com/S0kVyPg.mp4) using off-the-shelf models/tools, as the domain is especially niche and complex compared to their training distribution. It seems there's no way around fine-tuning on in-domain images with manually-labeled keypoints (thankfully, I have thousands of hours of unlabelled footage to start from).I've always been intrigued by active learning, so I'm looking forward to applying it here to efficiently sample frames for manual labeling. But I've never witnessed it in industry, and have only ever encountered [pessimistic takes on active learning in general](https://www.reddit.com/r/MachineLearning/comments/13elpm1/d_is_active_learning_a_hoax_or_the_future/) (not the concept ofc, but the degree to which it outperforms random sampling).As an extra layer of complexity - it seems like a manual labeler (likely myself) would have to enter labels through a browser GUI. Ideally, the labeler should produce labels concurrently as the model trains on its labels-thus-far and considers unlabeled frames to send to the labeler. Suddenly my training pipeline gets complicated!My current plan:* Sample training frames for labeling according to variance in predictions between adjacent frames, or perhaps dropout uncertainty. Higher uncertainty should --> worse predictions* For the holdout val+test sets (split by video), sample frames truly at random* In the labeling GUI, display the model's initial prediction, and just drag the skeleton around* Don't bother with concurrent labeling+training, way too much work. I care more about hours spent labeling than calendar time at this point.I'd love to know whether it's worth all the fuss. I'm curious to hear about any cases where active learning succeeded or flopped in an industry/applied setting.* In practice, when does active learning give a clear win over random? When will it probably be murkier?* Recommended batch sizes/cadence and stopping criteria?* Common pitfalls (uncertainty miscalibration, sampling bias, annotator fatigue)?.
Hi everyone,I‚Äôm curious about model parallel training use cases in industry and academia. A few things I‚Äôd love to hear about:  ‚Äì Which companies / research groups require model parallelism? What domains are these groups in and how large are their models?  ‚Äì Are people using off-the-shelf frameworks (e.g. DeepSpeed, Megatron-LM, PyTorch FSDP) or in-house solutions?  ‚Äì What‚Äôs been the biggest pain point e.g. debugging, scaling efficiency? Would users benefit from systems that automatically split their models and run them on cost-optimal hardware?I‚Äôm trying to get a better sense of the landscape and where the real needs are. Would appreciate any insights from practitioners or researchers.Thanks!.
I just finished fine-tuning a model using Unsloth on Google Colab. The model takes in a chunk of text and outputs a clean summary, along with some parsed fields from that text. It‚Äôs working well!Now I‚Äôd like to run this model locally on my machine. The idea is to:* Read texts from a column in a dataframe* Pass each row through the model* Save the output (summary + parsed fields) into a new dataframe# Model Info:* `unsloth/Phi-3-mini-4k-instruct-bnb-4bit`* Fine-tuned with Unsloth# My system specs:* Ryzen 5 5500U* 8GB RAM* Integrated graphics (no dedicated GPU)TIA!.
Hello!I have the possibility to join one of the few AI lab that trains their own LLMs.Given the option, would you join the pretraining team or (core) post training team? Why so?.
Arena evals (e.g., Chatbot Arena) let users pick which model's response is better, or call it a draw. Most leaderboards then shove this into Elo, same as chess. The assumption: a draw = two models are equally strong. The paper [""Drawing Conclusions from Draws: Rethinking Preference Semantics in Arena-Style LLM Evaluation""](https://arxiv.org/abs/2510.02306) tests that assumption and proves it wrong:* On 3 arena datasets, ignoring draws when updating ratings makes battle outcome prediction accuracy go **up 1-3%**, despite evaluation still *including draws*.* Draws happen much more on **easy** or **objective** queries (risk ratios of 1.3x).**Discussion seed:** If draws don't indicate skill parity and hence represent a poor fit for existing rating systems, how should we *actually* model them?COI: Submitter is author..
Hey fellow ML people!Last year, I shared with you a job board for¬†[FAANG positions](https://www.reddit.com/r/MachineLearning/comments/1ia7feh/p_made_a_faang_job_postings_aggregator_for_ai/)¬†and due to the positive feedback I received, I had been working on expanded version called¬†[hire.watch](https://hire.watch/?categories=AI+_+Machine+Learning)The goal is provide a unified search experience - it crawls, cleans and extracts data, allowing filtering by:1. Full-text search2. Location - on-site3. Remote - from a given city, US state, EU, etc.4. Category - you can check out the machine learning category here:¬†[https://hire.watch/?categories=AI+\_+Machine+Learning](https://hire.watch/?categories=AI+_+Machine+Learning)5. Years of experience and seniority6. Target gross salary7. Date posted and date modifiedI used the normal ML ecosystem (scikit learn, huggingface transformers, LLMs, etc.) to build it, and Plotly Dash for the UI.Let me know what you think - feel free to ask questions and request features :).
This is a [great opportunity](https://www.youtube.com/watch?v=_NLHFoVNlbg) for all ML/DL students/practitioners to either start learning from scratch or filling knowledge gap, time to start learning folks..
Quick paper highlight (adapted from TLDR thread):  Finds no special advantage using an LLM to predict its own correctness (a trend in prior work), instead finding that LLMs benefit from learning to predict the correctness of many other models ‚Äì becoming a GCM.  \--  Training 1 GCM is strictly more accurate than training model-specific CMs for all models it trains on (including CMs trained to predict their own correctness).  GCM transfers without training to outperform direct training on OOD models and datasets.  GCM (based on Qwen3-8B) achieves +30% coverage on selective prediction vs much larger Llama-3-70B‚Äôs logits.TLDR thread:¬†[https://x.com/hanqi\_xiao/status/1973088476691042527](https://x.com/hanqi_xiao/status/1973088476691042527)  Full paper:¬†[https://arxiv.org/html/2509.24988v1](https://arxiv.org/html/2509.24988v1)**Discussion Seed**:  Previous works have suggested / used LLMs having self knowledge, e.g., identifying/preferring their own generations \[https://arxiv.org/abs/2404.13076\], or ability to predict their uncertainty. But paper claims specifically that LLMs don't have knowledge about their own *correctness.* Curious on everyone's intuition for what LLMs have / does not have self knowledge about, and whether this result fit your predictions.Conflict of Interest:   Author is making this post. .
Are ML researchers using LLMs like ChatGPT, Claude, or other open-source models to generate, test, or refine minor ideas as tweaks to their original research, or to ask big-picture questions about their overall plans? In what other ways are publishing researchers using LLMs to support their work? (Of course, I don‚Äôt mean those who literally ask ChatGPT to write a paper from scratch.)I sometimes feel guilty when I feed a paper into ChatGPT and ask it to summarize or even extract ‚Äúideas‚Äù from it, which I then try to combine with my own. I want to understand where a researcher should draw the line in using LLMs in their daily workflow, so as not to fool themselves into believing they are doing good research while over-relying on the tool..
Hi all,I'm an undergrad Computer Science student working or my senior thesis, and l'll have about 8 months to dedicate to it nearly full-time. My broad interest is in reasoning, and I'm trying to decide between two directions:‚Ä¢ Mechanistic interpretability (low-level): reverse engineering smaller neural networks, analyzing weights/ activations, simple logic gates, and tracking learning dynamics.‚Ä¢Semantic probing (high-level): designing behavioral tasks for LLMs, probing reasoning, attention/locality, and consistency of inference.For context, after graduation I'll be joining a GenAl team as a software engineer. The role will likely lean more full-stack/frontend at first, but my long-term goal is to transition into backend.I'd like the thesis to be rigorous but also build skills that will be useful for my long-term goal of becoming a software engineer. From your perspective, which path might be more valuable in terms that of feasibility, skill development, and career impact?Thanks in advance for your advice!.
I am a PhD student in Maths - high dimensional modeling. I had an idea for a future project, although since I am not too familiar with these concept, I would like to ask people who are, if I am thinking about this right and what your feedback is. Take diffusion for image generation. An overly simplified tldr description of what I understand is going on is this. Given pairs of (text, image) in the training set, the diffusion algorithm learns to predict the noise that was added to the image. It then creates a distribution of image concepts in a latent space so that it can generalize better. For example, let's say we had two concepts of images in our training set. One is of dogs eating ice cream and one is of parrots skateboarding. If during inference we asked the model to output a dog skateboarding, it would go to the latent space and sample an image which is somewhere ""in the middle"" of dogs eating ice cream and parrots skateboarding. And that image would be generated starting from random noise. So my question is, can diffusion be used in the following way? Let's say I want the algorithm to output a vector of numbers (p) given an input vector of numbers (x), where this vector p would perform well based on a criterion I select. So the approach I am thinking is to first generate pairs of (x, p) for training, by generating ""random"" (or in some other way) vectors p, evaluating them and then keeping the best vectors as pairs with x. Then I would train the diffusion algorithm as usual. Finally, when I give the trained model a new vector x, it would be able to output a vector p which performs well given x.   Please let me know if I have any mistakes in my thought process or if you think that would work in general. Thank you..
Hey everyone,  I have a few publications and patents and I work for a tier 2 company as Research scientist. Lately all my job applications have been rejected on the spot. Not even a first interview. I want to beef up my coding skills and be more attractive to employers. Maybe not having a huge github presence is hindering my prospects.    Can u please suggest opensource projects like SGLang or vLLm which I can contribute to? Any starting pointers?Edit- treasure trove of comments below for any RS or MLE trying to get into faang. Thanks community..
Imagine we train (or fine-tune) an LLM exclusively on physics texts up to 1904‚ÄîMaxwell, Lorentz, Poincar√©, Michelson‚ÄìMorley, etc.‚Äîand then ask it to produce a theory addressing the known tensions (e.g., invariance of c, simultaneity). The goal isn‚Äôt to re-derive Einstein verbatim or to validate anything in the lab, but to test whether an LLM can elaborate a novel, coherent theoretical structure from historically available knowledge.I‚Äôm interested in any domain, not just relativity: e.g., pre-quantum physics, pre-DNA biology, early group theory, early materials science, etc.What would count as ‚Äúon topic‚Äù:Pretraining from scratch or continual pretraining on a historically filtered corpus (time-sliced).Strong leakage controls: no access to post-cutoff texts; possibly knowledge unlearning.Evaluation focused on novelty + internal coherence (not experimental truth): e.g., CAS/proof-assistants for consistency, reviewers for ‚Äúhistorical plausibility.‚ÄùComparisons vs. baselines like RAG-only setups or modern LLMs that ‚Äúalready know‚Äù the breakthrough.Reports of failure modes (e.g., the model just paraphrases Lorentz/Poincar√©, or smuggles modern terms).Why I‚Äôm asking:I‚Äôve seen adjacent work (LLM-aided conjecture generation, symbolic regression discovering equations, RL systems finding new algorithms), but not a clean ‚Äúpre-discovery epistemology‚Äù experiment with strict temporal cutoffs.Tagging folks who might have seen or worked on something like this:u/hardmaru ¬∑ u/MysteryInc152 ¬∑ u/Qyeuebs ¬∑ u/StartledWatermelon ¬∑ u/Playful_Peace6891 ¬∑ u/SatoshiNotMe ¬∑ u/Ch3cks-Out ¬∑ u/NuclearVIIIf you know of:peer-reviewed papers, arXiv preprints, thesesdatasets/corpora curated by historical cutoffcode or replication packages‚Ä¶please share!Thanks in advance üôè.
Would love to get people‚Äôs thoughts on the current job market. Simultaneously,  it seems a lot of companies aren‚Äôt hiring, a lot of start ups are hiring and there are a lot of people in the market. Also this is the first time I‚Äôve seen so many companies only offer Staff positions. How is everyone feeling right now? .
I'm planning to fine-tune LLaMA 3.2 11B Instruct on a JSONL dataset of domain-specific question-answer pairs ‚Äî purely text, no images. The goal is to improve its instruction-following behavior for specialized text tasks, while still retaining its ability to handle multimodal inputs like OCR and image-based queries.My concern: will this fine-tuning lead to multimodal forgetting?The NeurIPS 2024 paper discusses how training on more image-text pairs can cause text-only forgetting. So I‚Äôm wondering ‚Äî does the reverse happen too? If I train only on text, will the model lose its ability to process images or degrade in tasks like OCR?Has anyone observed this kind of modality drift or tested the impact of unimodal fine-tuning on multimodal performance?.
Hello all. I am sharing details about a retail focused dataset we've assembled that might interest folks working on production CV systems:**Quick specs:*** 1M retail interior images (280K structured, 720K available for processing) but all are structured and organised. 280k are our platinum set.* Multi-country: UK, US, Netherlands, Ireland, Germany. Mainly UK/US.* Temporal organisation: Year/month categorization spanning multiple years, also by retailer and week too.* Hierarchical structure: Year > Season > Retailer > Sub-Category (event specific) and often by month and week for Christmas.* Real-world conditions: Various lighting, angles, store formats.* Perfectly imperfect world of retail, all images taken for our consulting work, so each image has a story, good, bad, indifferent. **Why this might matter:** Most retail CV benchmarks (SKU110K, RP2K, etc.) are single market or synthetic. Real deployment requires models that handle:* Cross-retailer variation (Tesco ‚â† Walmart ‚â† Sainsburys et al)* Temporal shifts (seasonal merchandising, promotional displays, COVID we have too)* Geographic differences (EU vs US labeling, store formats)**Research applications:*** Domain adaptation across retail environments* Few shot learning for new product categories* Temporal consistency in object detection* Transfer learning benchmarks* Dates on product, reduction labels, out of stock, lows, highs.**Commercial applications:*** Training production planogram compliance systems* Autonomous checkout model training* Inventory management CV pipelines* Retail execution monitoring* Numerous other examples that could be developerd.Available for licensing (commercial) and academic partnerships. Can provide samples and detailed breakdown under NDA with a controlled sample available. Curious about the community's thoughts on what annotations would add most value - we can support custom categorisation and labelling work. It's a new world for us in terms of licensing, we are retailers at heart but we know that 1m images from 2010 to today represents a really unique dataset..
Hi everyone, the reviews are finally out! I hope you all did well. How were yours?I got 4, 4, 4, and 3 ‚Äî any chances? (4 weak accept, 3 weak reject).
A few of my colleagues went CUDA spelunking last weekend üë∑They wrote up a technical report on how FA4 works:¬†[https://modal.com/blog/reverse-engineer-flash-attention-4](https://modal.com/blog/reverse-engineer-flash-attention-4)Flash Attention 4 is the latest addition to the Flash Attention series of CUDA kernels. These kernels are used in the attention layers of Transformers, which are very computation-heavy and would be ideal to run as fast as possible. Tri Dao announced last month that FA4 is up to 22% faster than the attention kernel implementation in NVIDIA's own cuDNN library.We dug in to why! tl;dr-  \- Much more sophisticated warp-specialized async pipeline  \- ""Software softmax"" using a (novel?) cubic approximation to exp2  \- More efficient rescaling to reduce the cost of numerical stability[the life of a tile in FA4](https://preview.redd.it/lwtgfv5mhisf1.png?width=1667&format=png&auto=webp&s=a3c875c2674e4247a442c4e943797da4d884f050).
What was your ICLR submission number? I sent my paper pretty early, so it's \~5000, but I am curious how many submissions they got. Particularly compared to massive 29k at AAAI, and taking into consideration that ICLR reviews are public..
Hey folks,My paper has been accepted at¬†**NeurIPS 2025**, and now I‚Äôm scrambling to secure funding to attend (flights, board, registration, etc.). I know some grants exist, but I'm looking for:* Agencies / foundations / companies supporting¬†**student researchers**¬†for¬†**NeurIPS / major ML conferences*** Lab / university / departmental travel grant schemes that others have used* Tips or personal experience (how much you got, when to apply, how to write the proposal)So far I‚Äôve found:* NeurIPS itself offers¬†*financial assistance*¬†for registration but¬†**does not pay for travel and hotel.**If you know any lesser-known ones (especially in India / Asia) or similarly for your country, please drop links or names. Appreciate any help!.
I‚Äôve been experimenting with using another LLM to *score* my agent‚Äôs responses (accuracy / groundedness style) instead of relying on spot-checking.Surprisingly effective ‚Äî but only when the judge prompt is written carefully (single criterion, scoring anchors, strict output format, bias warnings, etc.)Curious if anyone else here is doing this? Any lessons learned?(I wrote a short breakdown of what worked for us ‚Äî happy to share if useful.).
Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!Thread will stay alive until next one so keep posting after the date in the title.Thanks to everyone for answering questions in the previous thread!.
I came across a CV and ML researcher who has recently completed a PhD at a top uni with around 600 citations and an h-index of 10. On the surface, that seems like a legit academic profile. Their papers have been accepted in CVPR, WACV, BMVC, ECCV, AAAI. What surprised me is that NONE of their papers have associated code releases. They have several github page (some git from 2-3 years ago) but with ZERO code release, just README page.Is it common for a researcher at this level to have **ZERO code releases across ALL their works**, or is this person a fake/scam? Curious how others in academia/industry interpret this.Edit: his research (first authored) is all 2020-present. recently graduated from a top uni..
Did anyone hear back from the volunteering chair / diversity and inclusion chair?.
[Nature Communications](https://rdcu.be/eISlO)>**Abstract:** Accurate time-series forecasting is crucial in various scientific and industrial domains, yet deep learning models often struggle to capture long-term dependencies and adapt to data distribution shifts over time. We introduce Future-Guided Learning, an approach that enhances time-series event forecasting through a dynamic feedback mechanism inspired by predictive coding. Our method involves two models: a detection model that analyzes future data to identify critical events and a forecasting model that predicts these events based on current data. When discrepancies occur between the forecasting and detection models, a more significant update is applied to the forecasting model, effectively minimizing surprise, allowing the forecasting model to dynamically adjust its parameters. We validate our approach on a variety of tasks, demonstrating a 44.8% increase in AUC-ROC for seizure prediction using EEG data, and a 23.4% reduction in MSE for forecasting in nonlinear dynamical systems (outlier excluded).By incorporating a predictive feedback mechanism, Future-Guided Learning advances how deep learning is applied to time-series forecasting.Hello everyone. As the first author of this paper, I would be grateful for your thoughts and feedback. The core concept of our work is to use a forecasting model aligned with subsequent (""future"") data to guide and improve a separate model that makes predictions from an earlier (""past"") point in time. This approach is grounded in the principles of predictive coding theory..
Hi everyone,I am working on a non-linear model which will later fed into a optimization framework. I am planning to use meta-heuristic technique for optimization framework but the problem is meta-heuristic techniques gives near optimal solution and are non-deterministic in nature. This will create problems while explaining my solution to Product managers and business stakeholders. How should I go about it ?PS- I cannot implement search space based optimization techniques because it will breach the SLA..
Arxiv:¬†[https://arxiv.org/pdf/2509.21880](https://arxiv.org/pdf/2509.21880)Huggingface paper:¬†[https://huggingface.co/papers/2509.21880](https://huggingface.co/papers/2509.21880)I‚Äôve been working on improving the reasoning abilities of large language models, and I wanted to share something I‚Äôm really excited about. Reinforcement Learning with Verifiable Rewards (RLVR) is already a powerful framework, but I noticed a gap: current methods like GRPO only use problems where model responses differ in correctness. They completely ignore the so-called ‚Äúzero-variance prompts‚Äù ‚Äî cases where all responses receive the same reward.At first glance, these prompts look useless, but I started wondering if they actually contain valuable learning signals. That led me to develop¬†**RL with Zero-Variance Prompts (RL-ZVP)**. Instead of discarding those prompts, RL-ZVP extracts meaningful feedback from them. It directly rewards correctness and penalizes errors without needing contrasting responses, and it uses token-level entropy to guide the advantage shaping.We evaluated RL-ZVP on six math reasoning benchmarks, and it delivered some really promising results ‚Äî up to¬†**8.61 points higher accuracy**¬†and¬†**7.77 points higher pass rates**¬†compared to GRPO. It also consistently outperformed other baselines that just filter out zero-variance prompts.I am happy to take comments in this sub and the HuggingFace paper..
Tell me about your data preprocessing technique that you found out/invented by years of experience..
Title!I currently have a workstation with a 12600k and a 3090 FE but to be fair most of my work is now done on remote machines. I only use the local station for quick tests of repositories and stuff. I want to keep this machine as a dedicated gaming rig and I'm thinking to downsizing reusing an alternate machine I have, with a 2070 super and a 2700x. Currently I'm on windows but that machine will run on linux.If price difference was bigger I'll stick to the ITX but currently I have a 2700x which is way slower than the m4 and would like to upgrade to a 5700x (not too expensive, can use the same ram etc), or maybe something am5 as I still have to get the ITX board, but this would also increase the price as I would require DDR5 ram.The biggest pros I see on the mac mini, very small so my setup remains clean, has good audio compatibility (I record myself often). The disadvantage is being stuck to 16GB ram and requiring external storage expansion, and maybe package compatibility. I do not run local LLMs as of now as my pipelines are mostly vision.The pros on the itx station, can get more RAM for less, the 2070 super should be more powerful, (but only 8GB vram) more compatible with libraries, upgradeable (could even fit the 3090fe on some cases if I wanted to), but it will be bigger, noisier, have more cables, and less power efficient.I'm not able to choose one or another to be honest. I enjoy both OS.Not sure if this affects somehow the experience but I have a 4k monitor. Not sure how well linux scales things (my previous 1440p monitor experience with my linux laptop was mediocre due to blurry texts often).My current buy list makes 600 on the mac and 640 on the ITX, including a 1TB m2.What would you go for? are you using similar systems yourself?Thanks!.
I had a stupid question while watching at andrej‚Äôs video. Since we are just collecting the numbers of occurrence of a ‚ÄúN-sequence pairs‚Äù using training data to predict the outcome in N-gram model, isn‚Äôt it that is what we are actually trying to achieve or expect it to happen while training NN?, and if so, isn‚Äôt N-gram model a global solution rather than a local solution?.
Imagine you're someone who is attempting to dip a toe into ML research in 2025. Say, a new graduate student.You say to yourself ""I want to do some research today"". Very quickly you realize the following:**Who's my competition?**Just a handful of billion-dollar tech giants, backed by some of the world's most powerful governments, with entire armies of highly paid researchers whose only job is to discover interesting research questions. These researchers have access to massive, secret knowledge graphs that tell them exactly where the next big question will pop up before anyone else even has a chance to realize it exists. Once LLMs mature even more, they'll probably just automate the process of generating and solving research problems. What's better than pumping out a shiny new paper every day?**Where would I start?**Both the Attention and the ADAM paper has 200k citation. That basically guarantees there‚Äôs no point in even trying to research these topics. Ask yourself what more could you possibly contribute to something that‚Äôs been cited 200,000 times. But this is not the only possible topic. Pull out any topic in ML, say image style transfer, there are already thousands of follow-up papers on that. Aha, maybe you could just read the most recent ones from this year. Except, you quickly realize that most of those so-called ‚Äúpapers‚Äù are from shady publish-or-perish paper-mills (which are called ""universities"" nowadays, am I being too sarcastic?) or just the result of massive GPU clusters funded by millions of dollars instant-access revenue that you don‚Äôt have access to.**I‚Äôll just do theory!**Maybe let's just forget the real world and dive into theory instead. But to do theory, you‚Äôll need a ton of math. What‚Äôs typically used in ML theory? Well, one typically starts with optimization, linear algebra and probability. But wait, you quickly realize that‚Äôs not enough. So you go on to master more topics in applied math: ODEs, PDEs, SDEs, and don‚Äôt forget game theory, graph theory and convex optimization. But it doesn‚Äôt stop there. You‚Äôll need to dive into Bayesian statistics, information theory. Still isn‚Äôt enough. Turns out, you will need pure math as well: measure theory, topology, homology, group, field, and rings. At some point, you realize this is still not enough and now you need to think more like Andrew Wiles. So you go on to tackle some seriously hard topics such as combinatorics and computational complexity theory. What is all good for in the end? Oh right, to prove some regret bound that absolutely no one cares about. What was the regret bound for ADAM again? It's right in the paper, Theorem 1, cited 200k times, and nobody as far as I'm aware of even knows what it is..
Hi everyone,I developed a selfhostable software, that use Librosa + Tensorflow to extract a Musicnn embbeding vector from songs. So basicaly a 200 size vector that off course it can't be reverted in anyway to the original song.The Tensorflow model that I use, as anticipated, is not trained by me but is Musicnn embbeding. So that my doubts is not about how to train the model  BUT about the result that I get.Actually the user run my app in their homelab on their songs, so is totally their ownership to do an accurate use in the respect of copyright.I would like to collect, with the acceptance of the user, a centralized database of this embbeding vector. This could open multiple new scenario because thanks of them I can:- First reduce the analysis process from the user, that don't need to re-analyze all the song. This is specially useful for user that run the software on low end machine, like a Raspberry PI- Second start not only to give user suggestion of similar song that he already have, but also help them to discover song that don't have.My copyright queston is: collect this data from the user in a database usable from everyone, could me bring some kind of copyright issue?I mean, user could potentially analyze commercial songs and upload the embbeding of those commercial song, could be this an issue? could be this seens as ""use of derivative work without a correct license""? Especially by my centralized database that off course don't have any license on the original music?Important:- this centralized database only collec Title, Artist, embbeding, genre, NOT the song itself;- I'm in Europe, so I don't know if any specific restriction is here.By similarity I was thinking what Acousticbrainz did, even if it don't collect embbding vector, it have user submitting data get from original music in some way. But here I don't know if they have some agreement, if maybe they are in an University and as researcher they are ok (In my case I'm only a single person that do this in his free time, without any university or company behind).I don‚Äôt want for a free and opensource project run the risk of have issue with copyright and at the same time I don‚Äôt have money to invest for consulting a layer..
I‚Äôve released a small library for parametric curves for PyTorch that are differentiable: you can backprop to the curve‚Äôs inputs and to its parameters. At this stage, I have B-Spline curves (efficiently, exploiting sparsity!) and Legendre Polynomials. Everything is vectorized - over the mini-batch, and over several curves at once.Applications include:* Continuous embeddings for embedding-based models (i.e. factorization machines, transformers, etc)* KANs. You don‚Äôt have to use B-Splines. You can, in fact, use any well-approximating basis for the learned activations.* Shape-restricted models, i.e. modeling the probability of winning an auction given auction features x and a bid b - predict increasing B-Spline coefficients c(x) using a neural network, apply to a B-Spline basis of b.Link:¬†[https://github.com/alexshtf/torchcurves](https://github.com/alexshtf/torchcurves)I wrote ad-hoc implementations for past projects, so I decided to write a proper library, that may be useful to others. And I hope i will!.
Basically the title. A list of how the PCs fumbled being PCs for this track:1. Missed every deadline they posted on the website.2. Only mentioned about 6% acceptance a day before sending notifs. Had this been posted at the start of calls, authors would have logically submitted it to other venues.3. Blocked possible submissions of papers to ICLR by moving notifs by one week.4. No metareviews for some papers, including ours. 5. ICML2025 handled the Position Paper track just fine with relatively the same # of submissions and was able to stick to the deadline. AND they had rebuttals. Why couldn't the PCs do the same now?6. PCs kept justifying their poor decisions instead of taking responsibility for wasting reviewers' and authors' time, which is so infuriating.But sure. It was ""experimental"" after all, so no biggie..
Hi community,What online serving solutions do you use for recsys? How does the architecture look (sidecars, ensembles across different machines, etc.)? For example, is anyone using Ray Serve in prod, and if so, why did you choose it? I'm starting a new project and again leaning towards Triton, but I like the concepts that Ray Serve introduces (workers, builtin mesh). I previously used KubeRay for offline training, and it was a very nice experience, but I also heard that Ray isn't very mature for online serving..
Our **dynamical systems foundation model DynaMix** was accepted to **#NeurIPS2025** with outstanding reviews (6555) ‚Äì the first model which can ***zero-shot***, w/o any fine-tuning, forecast the ***long-term behavior*** of time series from just a short context signal. Test it on #HuggingFace:[https://huggingface.co/spaces/DurstewitzLab/DynaMix](https://huggingface.co/spaces/DurstewitzLab/DynaMix)Preprint: [https://arxiv.org/abs/2505.13192](https://arxiv.org/abs/2505.13192)Unlike major time series (TS) foundation models (FMs), DynaMix exhibits zero-shot learning of long-term stats of unseen DS, incl. attractor geometry & power spectrum. It does so with only **0.1% of the parameters & >100x faster inference times** than the closest competitor, and with an **extremely small training corpus of just 34 dynamical systems** \- in our minds a paradigm shift in time series foundation models.https://preview.redd.it/d46h9deagorf1.png?width=1791&format=png&auto=webp&s=7a86714f6e8d7eb269224c0e06ac317f405dfbeehttps://preview.redd.it/mullm71cgorf1.png?width=1436&format=png&auto=webp&s=e53055fcc8b1d2f77da88c3896a95d65f3fac893It even outperforms, or is at least on par with, major TS foundation models like Chronos on forecasting diverse empirical time series, like weather, traffic, or medical data, typically used to train TS FMs. This is surprising, cos DynaMix‚Äô training corpus consists \*solely\* of simulated limit cycles or chaotic systems, no empirical data at all!https://preview.redd.it/8twn70e2horf1.png?width=1127&format=png&auto=webp&s=20a7a7721a29d80bc2f01077b6e8684b54ce21efAnd no, it‚Äôs neither based on Transformers nor Mamba ‚Äì **it‚Äôs a new type of mixture-of-experts architecture** based on the recently introduced **AL-RNN** (https://proceedings.neurips.cc/paper\_files/paper/2024/file/40cf27290cc2bd98a428b567ba25075c-Paper-Conference.pdf). It is specifically designed & trained for dynamical systems reconstruction.https://preview.redd.it/j0njmppkgorf1.png?width=1796&format=png&auto=webp&s=e05e275bf6aeba93fb04e8a288cd0fbac6d8fa84Remarkably, it not only generalizes zero-shot to novel DS, but it **can even generalize to new initial conditions and regions of state space not covered by the in-context information**.https://preview.redd.it/wlxwcp2ngorf1.png?width=1522&format=png&auto=webp&s=54a2dbed65a085d7522907275468700adf9d9619In our paper we dive a bit into the reasons why current time series FMs not trained for DS reconstruction fail, and conclude that a DS perspective on time series forecasting & models may help to advance the time series analysis field..
I'm attending at CoRL 2025 and went to some interesting workshops today. I've heard that networking is very important at conferences, but it is challenging for highly introvert people like me. Do you have any tips?.
Hello everyone,Together with some friends from my network, we recently started a startup. We‚Äôre still in the early stages of development, and to move forward, we need access to GPUs.We‚Äôve already explored a few free platforms, but haven‚Äôt received any responses so far. At the moment, we‚Äôre looking for either the most affordable GPU options or platforms that might be open to collaborating with us.If you know of any opportunities or resources that could help, I‚Äôd be truly grateful.Thank you in advance!.
Hi folks, I made a research tools that allows you to perform deterministic inference on any local large language model. This way you can test any variable changes and see for yourself the affects those changes have on the output of the LLM's response.  It also allows you to perform automated reasoning benchmarking of a local language model of your choice, this way you can measure the perplexity drop of any quantized model or differences between reasoning capabilities of models or sampling parameters. It also has a fully automated way of converging on the best sampling parameters for a given model when it comes to reasoning capabilities. I made 2 videos for the project so you can see what its about at a glance the main guide is here https://www.youtube.com/watch?v=EyE5BrUut2o, the instillation video is here https://youtu.be/FJpmD3b2aps and the repo is here https://github.com/manfrom83/Sample-Forge. If you have more questions id be glad to answer them here. Cheers..
I came across a new survey and resource repository on object tracking. It covers classical Single Object Tracking (SOT) and Multi-Object Tracking (MOT), as well as more recent approaches that use vision-language and foundation models.The repository also includes Long-Term Tracking (LTT), benchmarks, datasets, and code links. It‚Äôs been put together by researchers at Carnegie Mellon University (CMU), Boston University, and MBZUAI.Link: [https://github.com/rahulrj/Awesome-Object-Tracking](https://github.com/rahulrj/Awesome-Object-Tracking)It could be useful for both researchers and practitioners. Contributions and feedback are welcome..
[https://github.com/yoonsanghyu/FaSNet-TAC-PyTorch](https://github.com/yoonsanghyu/FaSNet-TAC-PyTorch) is this rather cool model for invariant source separation but the above is a great bit of code but for fixed sources.[https://docs.pytorch.org/docs/stable/torch.compiler\_dynamic\_shapes.html](https://docs.pytorch.org/docs/stable/torch.compiler_dynamic_shapes.html) does go into the possibility of dynamic shapes as it would be cool to have a single model that would work with 2-6 input mics than say creating a model for each number of inputs 2,3,4,5,6...I am just wondering that even though possible would a dynamic model be much larger requiring more compute and also be less accurate than a fixed known input tensor?.
As in the question what do you normally do when your model is training and you want to know the results but cannot continue implementing new features because you don't want to change the status and want to know the impact of the currently modifications done to your codebase?.
I was trying to train a GPT-2 XL-sized model on Kaggle with their free TPU v3-8, but they recently switched to TPU v5e-8, and now I am getting OOM errors whenever I try to train. I am using Torch XLA, FSDP, mixed precision, and the Muon optimizer(momentum-only optimizer) for my hidden weight matrices and AdamW everywhere else..
Mine is ""always balance the dataset using SMOTE, that will drastically increase the precision, recall, f1 etc"".
The website says decisions out September 18 but I still haven‚Äôt see any reviews or notifications. Anyone else hearing back from it?.
https://preview.redd.it/25bv436lolrf1.png?width=1536&format=png&auto=webp&s=e2154e75a16600600492b948877749aaffb468eaHi everyone,I recently explored a limitation of the **MissForest algorithm** (Stekhoven & B√ºhlmann, 2012): it cannot be directly applied in predictive settings because it doesn‚Äôt save the imputation models. This often leads to **data leakage** when trying to use it across train/test splits.In the article, I show:* Why MissForest fails in prediction contexts,* Practical examples in R and Python,* How the new **MissForestPredict** (Albu et al., 2024) addresses this issue by saving models and parameters.üëâ Full article here: [https://towardsdatascience.com/why-missforest-fails-in-prediction-tasks-a-key-limitation-you-need-to-know/](https://towardsdatascience.com/why-missforest-fails-in-prediction-tasks-a-key-limitation-you-need-to-know/).
I am working on a project in which we are tasked with developing anomaly detection for a technical system.Until now, I have mainly worked with LLMs and supplied them with external knowledge using RAG.Now I have to work with a multimodal model and train it to detect anomalies (e.g scratches, broken glass) in a technical system based on images. I was thinking of using Gemma3:4b as the model, but I will evaluate this in more detail as I go along.To do this, I would have to train this model accordingly for this use case, but I'm not quite sure how to proceed. All I know is that a large amount of labeled data is required.So I would like to ask what the procedure would be, which tools are commonly used here, and whether there is anything else to consider that I am not currently aware of..
https://preview.redd.it/3m7n4tnu1erf1.png?width=1536&format=png&auto=webp&s=29a717573ec6d3a8d07440b17bd98bf1452ce9a6Hi everyone,I‚Äôve been working on a guide to evaluate **training data representativeness** and detect dataset shift. Instead of focusing only on model tuning, I explore how to use two statistical tools:* **Population Stability Index (PSI)** to measure distributional changes,* **Cramer‚Äôs V** to assess the intensity of the change.The article includes explanations, Python code examples, and visualizations. I‚Äôd love feedback on whether you find these methods practical for real-world ML projects (especially monitoring models in production).  Full article here: [https://towardsdatascience.com/assessment-of-representativeness-between-two-populations-to-ensure-valid-performance-2/](https://towardsdatascience.com/assessment-of-representativeness-between-two-populations-to-ensure-valid-performance-2/).
We released ShinkaEvolve, a new state-of-the-art and fully open-source framework for program optimization, which we specifically designed to be easily integrated into any scientific codebase.Open source code:[ https://github.com/SakanaAI/ShinkaEvolve](https://github.com/SakanaAI/ShinkaEvolve)Technical report:[ https://arxiv.org/abs/2509.19349](https://arxiv.org/abs/2509.19349)Blog:[ https://sakana.ai/shinka-evolve/](https://sakana.ai/shinka-evolve/)You can start playing with ShinkaEvolve without even downloading any code, all inside a remote Google Colab instance:[ https://colab.research.google.com/github/SakanaAI/ShinkaEvolve/blob/main/examples/shinka\_tutorial.ipynb](https://colab.research.google.com/github/SakanaAI/ShinkaEvolve/blob/main/examples/shinka_tutorial.ipynb)In our technical report, we show how ShinkaEvolve can be easily applied across different problem domains. On the canonical circle packing task, ShinkaEvolve discovers a new solution with state-of-the-art performance beyond the recent closed-source AlphaEvolve using only 150 program evaluations. We even apply ShinkaEvolve to small-scale LLM pretraining, discovering a new load-balancing loss for MoE architectures with remarkable stabilization properties.ShinkaEvolve also comes with a detailed and lightweight WebUI to monitor its discoveries in real-time!.
Replace O(n¬≤d) self-attention in transformers with an O(nd) summation-based mechanism.Pure summation is linear and works well in classification and regression.In autoregressive language modeling, a hybrid transformer (summation in most layers + a single final attention layer) matches or slightly outperforms full attention -- while staying nearly linear in cost.Key points:* Drop-in replacement for attention inside transformer blocks (residuals, norms, optimizers unchanged)* Linear complexity: O(nd) aggregation instead of O(n¬≤d) pairwise similarity* Hybrid design: most layers use summation, a final attention layer recovers full performanceResults (small-to-moderate datasets):* Classification (proof-of-concept): single summation layer on AG News matches attention, up to \~18√ó faster at 512 tokens* Multimodal regression (text + tabular): summation fusion matches or outperforms concatenation, in a smaller latent space and with faster runtime* Language modeling: hybrid transformers (summation in most layers + one attention layer) achieve performance on par with or better than full attention -- showing that full attention is not required in every layerPaper: [https://doi.org/10.36227/techrxiv.175790522.25734653/v1](https://doi.org/10.36227/techrxiv.175790522.25734653/v1)Code: [https://github.com/pfekin/summation-based-transformers](https://github.com/pfekin/summation-based-transformers).
Hi guys,This post is about figuring out if RoPE overly constrains the K/Q spaces and if it decreases its effective dimensionality, by forcing a high condition number on the K/Q matrices.Just to give a bit of context, I'm trying to create a hierarchical BERT encoder (a kind of [CLS] embedding merger), and was trying to figure out a way to encode token (= sentence embeddings) position, because RoPE was designed for a kind of exponential decay that is not particularly relevant to my use case.Digging a bit deeper into the theory behind RoPE, I realized that specialized attention heads that focus on, say, position-insensitive semantical stuff need to project the embedding vectors in a space where the RoPE matrix will not mess them up. That's to say, the projected vectors will be heavily biased towards having information in the last components (where low-frequency rotation occur). The opposite happens for positional encoding heads (I think a Gemma paper mentions them), that project embeddings so they are head-heavy instead of tail-heavy (not even sure this is correct english stuff, I am ESL).From an outside perspective, it seems quite sub-optimal: attention scores are -for these cases- based on low-dimensional (effectively) dot products.So, 2 (and a half) questions here:1. Does it really matter? My prior is with yes, because I once computed the condition numbers of projection matrices in transformers with learned position embeddings and I found them to be very low (I guess they were < 10 at each layer for quite tiny transformers, even though I think they would get bigger for decent ones). Curious about your thoughts though.2. What about a mitigation strategy like having the attention head 'choose' the base rate of the RoPE? A very simple strategy would be to make it dependent on the barycenter of the norm of K/Q projection matrices' rows. Meaning: if the projection matrices tends to give more importance to the first components of the raw embedding, we consider that the base rate should be higher. This would cause a transformer-wide bias towards having position-dependent information at the beginning of embeddings.3. Have I totally misunderstood RoPE?I would love to hear your thoughts on that matter..
I‚Äôm a Senior ML engineer with around 9 years of experience. I work at a large government institution, implementing (integrating?) AI for cybersecurity, and I‚Äôm currently in the process of building a new team.I‚Äôve been having some concerns about my career development, and I‚Äôm not sure if other ML engineers with similar experience feel the same way.Most of my projects these days aren‚Äôt really ‚Äúmachine learning‚Äù anymore. It‚Äôs mostly using existing models through APIs, setting up pipelines, etc. The actual algorithmic/experimental side of ML feels like it‚Äôs disappearing from my day-to-day work.It seems like the industry has shifted from building models to API calls and prompt engineering. I miss the kind of work I did in my earlier roles, building models from scratch, fine-tuning, experimenting‚Ä¶So my question is: is this just what senior ML roles eventually turn into? Has the job really shifted from ‚Äúbuilding ML‚Äù to ‚Äúplugging in ML‚Äù? Curious if others are experiencing the same thing. I have been experiencing this since the generative AI boom where suddenly everything was solvable..(Disclaimer: we do use on-prem models at my organization, so I still get some hands-on time with models and fine-tuning using LoRA.).
Hi everyone,I‚Äôm working on a project and my dataset consists of high-resolution microscopic images of neurons (average resolution ~2560x1920). Each image contains numerous neurons, and I have bounding box annotations (from Labelbox) for atypical neurons (those with abnormal morphology). The dataset has around 595 images.A previous study on the same dataset applied Faster R-CNN and achieved very strong results (90%+ accuracy). For my project, I need to compare alternative models (detection-based CNNs or other approaches) to see how they perform on this task. I would really like to achieve 90% accuracy too.I‚Äôve tried setting up some architectures (EfficientDet, YOLO, etc.), but I‚Äôm running into implementation issues and would love suggestions from the community.üëâ Which architectures or techniques would you recommend for detecting these atypical neurons? üëâ Any tips for handling large, high-resolution images with many objects per image? üëâ Are there references or example projects (preferably with code) that might be close to my problem domain?Any pointers would be super helpful. Thanks!.
**üÜï What‚Äôs New**    Apple research just introduced Manzano (Spanish for ‚Äúapple tree‚Äù üçè) ‚Äî a unified multimodal LLM that both understands images and generates them inside the same autoregressive loop.  Instead of separate perception and generation models, one decoder predicts the next token ‚Äî text or image ‚Äî then renders pixels with an auxiliary diffusion decoder.  The paper reports state-of-the-art results among unified models and competitive performance against specialist systems, especially on text-rich benchmarks.    **‚öôÔ∏è How It Works**    Hybrid vision tokenizer in front of the LLM: a single vision encoder feeds two lightweight adapters producing continuous embeddings for understanding and discrete tokens for generation.    The unified LLM decoder accepts text tokens and/or image embeddings and auto-regressively predicts the next token; a diffusion image decoder turns predicted tokens into pixels.    Three-stage training (pre-training ‚Üí continued pre-training ‚Üí SFT) on mixed text/vision data; the embedding table is extended with a 64K image-token codebook aligned by finite scalar quantization.    **‚ú® What Makes It Distinct**    Hybrid tokenizer, single encoder: understanding and generation tokens come from one encoder in a shared semantic space (no dual-tokenizer conflict).    Decoupled roles: the LLM decoder handles high-level semantics; the diffusion decoder handles pixel fidelity ‚Äî letting each scale independently.    Explicit scaling: LLM decoder scaled from 300M‚Üí30B params with steady gains; diffusion decoder scaled for stronger structure in human evals.    **üìå Why It Matters**    One model for ‚Äúsee + draw‚Äù ‚Üí simpler architecture, better language‚Äìvision alignment, easier product integration.    Shared encoder + decoupled renderer ‚Üí a practical path to scale without sacrificing understanding (a weak point for earlier unified models).    If these results generalize, future assistants that read, reason, edit & generate in one loop could become the new default for multimodal work..
Let‚Äôs say you were training a generative model for a task like summarization or answering questions. Would it be possible to feed that output into an LLM and ask it to assess the model‚Äôs effectiveness at performing the task and then maybe feed that output into a sentiment analysis model to obtain a score for how well the model did and have the model attempt to maximize that score?.
Hey folks,Over the past few years, I‚Äôve been working on **tabular deep learning**, especially neural networks applied to healthcare data (expression, clinical trials, genomics, etc.). Based on that experience and my research, I put together and recently revised a **survey on deep learning for tabular data** (covering MLPs, transformers, graph-based approaches, ensembles, and more).The goal is to give an overview of the challenges, recent architectures, and open questions. Hopefully, it‚Äôs useful for anyone working with structured/tabular datasets.üìÑ PDF: [preprint link](https://www.techrxiv.org/doi/full/10.36227/techrxiv.175753732.26052568)  üíª associated repository: [GitHub repository](https://github.com/SalvatoreRa/tabular-deep-learning-survey)If you spot errors, think of papers I should include, or have suggestions, send me a message or open an issue in the GitHub. I‚Äôll gladly acknowledge them in future revisions (which I am already planning).Also curious: what deep learning models have you found promising on tabular data? Any community favorites?.
I'm currently developing a multitask model. Training it requires using multiple losses and manually adjusting their weights. I'm wondering if there are better solutions to automatically balance these loss coefficients. I already found that there is a method named AWL in GitHub, but I wonder if there are other kinds of methods..
The title basically. This year we saw that a lot of papers got rejected even _after_ being accepted, if we actually sum up the impact of these papers through compute, grants, reviewer effort, author effort, it's simply enormous and should not be wasted. Especially if it went through such rigorous review anyways, the research would definitely be worthwhile to the community. I think this is a simple solution, what do you guys think?.
I‚Äôve been wondering about this for a while and would love some perspective. I‚Äôm a PhD student with publications in top-tier venues (ECCV, NeurIPS, ICCV, AAAI, ICASSP), and I like to believe my research profile is solid? But when it comes to securing a research scientist internship at a big company (FAANG, top labs, etc.), I feel like I‚Äôm missing some piece of the puzzle.Is there some hidden strategy beyond just applying online? Do these roles mostly happen through networking, advisor connections, or referrals? Or is it about aligning your work super closely with the team‚Äôs current projects?I‚Äôm genuinely confused. If anyone has gone through the process or has tips on what recruiters/hiring managers actually look for, I‚Äôd really appreciate hearing your advice or dm if you wanna discuss hahahaha.
Curious what your workflow looks like as scientists/researchers (tools, tech, general practices)?I feel like most of us end up focusing on the science itself and unintentionally deprioritize the research workflow. I believe sharing experiences could be extremely useful, so here are two from me to kick things off:Role: AI Researcher (time-series, tabular)Company: Mid-sized, healthcare Workflow: All the data sits in an in-house db, and most of the research work is done using jupyter and pycharm/cursor.We use MLFlow for experiment tracking.Resources are allocated using run.ai (similiar to colab).Our workflow is generally something like: exporting the desired data from production db to s3, and research whatever. Once we have a production ready model, we work with the data engineers towards deployment (e.g ETLs, model API). Eventually, model outputs are saved in the production db and can be used whenever.  Role: Phd studentCompany: Academia research labWorkflow: Nothing concrete really, you get access to resources using a slurm server, other than that you pretty much on your own.Pretty straightforward python scripts were used to download and preprocess the data, the processed data was spilled directly into disk.A pretty messy pytorch code and several local MLFlow repos.There‚Äôre still many components that I find myself implement from scratch each time, like EDA, error analysis, production monitoring (model performance/data shifts). Usually it is pretty straightforward stuff which takes a lot of time and it feels far from ideal.What are your experiences?.
Hello Reddit,I'm a PhD physicist with an academic background in computational methods and couple years of experience applying them in a commercial R&D setting. My current work focuses on using Flow Matching and Diffusion Models for physics simulations, which is a fascinating area itself.The challenge I'm facing is that my current role is heavily focused on code development and deploying of existing models, with little opportunity for original, in-depth research. I have a number of research ideas related to GenAI Diffusion/Flow-based models across different modalities, but my company's priorities are focused on rapid deployment, not fundamental research.I'm looking to transition into a more research-oriented role where I can experiment, study, and pursue these and some else's ideas. I'm open to both academic and industrial opportunities.My question to the community is:* What grants, universities, or research institutions could I pursuit?* Do you know of any specific labs, orgs or companies known for their work on Flow Matching/Diffusion models for scientific or physical applications with a research agenda?* For those who have made a similar transition from (say industry) to a more research-focused industry role, what advice do you have? Are there specific resources or networks I should tap into?Any advice or leads would be greatly appreciated. Thank you!.
I'm looking at different methods for uncertainty estimation/quantification in deep/graph neural networks and originally i came across MC dropout. However, based on some threads in this subreddit, I've come to the conclusion that it's likely not considered a good estimate, and that it isn't exactly Bayesian either. That leads me to the question in the title. If you're not working with something inherently probabilistic such as a Gaussian Process, how do you meaningfully get uncertainty estimates? Have you come across anything during your reading/research? What makes the methods stand out, especially in comparison to a quick estimate like MCD? .
So I have a specific use case, in which Deepseek-v3.1 works well, but it's simply too big and takes time to load on our GPU (everything runs locally in my organization, we have¬†**16 H100 GPUs**¬†and maybe about¬†**8 more A100s**) .I use Ollama since I can‚Äôt keep VLLM loaded across all GPUs without hogging resources that others need.What I want is a¬†**smaller model**¬†that I can use for an¬†**agentic task**¬†mainly to work with a set of custom MCP tools I‚Äôve built.The biggest reason I want to build a model of my own is because I can get one hell of an education in the process, and since the hardware is already in-house (and mostly idle), I figured this is the perfect opportunity.But I‚Äôm not sure where to start:1. Should I train a model from scratch, or take an existing pretrained model and fine-tune?2. What base architecture would be a good starting point for agent-style tasks?If anyone can point me toward resources specifically focused on¬†**training or finetuning models for agentic tasks**, I‚Äôd really appreciate it.P.S: I am currently using full precision deepseek-v3.1 (671B). I am thinking of a model which is about the size of gpt oss..
**TL;DR.** We open-sourced **SyGra**, a graph-oriented framework for building *reproducible* synthetic data pipelines. Pipelines are defined as graphs (nodes = LLM calls/transforms/samplers; edges = conditional/parallel/loops). Two modes: YAML + CLI or Python library. Integrates with vLLM, HF TGI, Azure OpenAI, Ollama; HF-native I/O (streaming), provenance, schema-aware outputs.**Motivation.** High-quality LLM datasets are scarce, costly, and often sensitive; teams also need fine-grained control over task structure (SFT/DPO, tool use, multi-agent, multimodal). In practice, scaling ‚Äúnotebook pipelines‚Äù breaks down: you end up hand-wiring branching/looping flows, juggling multiple inference backends/APIs, and doing ad-hoc validation/schema checks‚Äîwithout resumability, sharding, or streaming. We wanted a **unified, reusable graph abstraction** that captures how data work actually happens (nodes/edges, subgraphs), automates **quality tagging** (heuristics + LLM-based scoring), and emits **schema-conformant, OASST-style** records‚Äîso teams can reproduce, audit, and evolve pipelines instead of rewriting glue code.**Design.*** **Graph model:** reusable subgraphs, branching, loops; deterministic configs* **Execution:** pluggable model clients (vLLM/TGI/Azure/Ollama), Triton-compatible* **Data I/O:** Hugging Face datasets (streaming), local files; schema & metadata tracking* **Reproducibility:** explicit configs, seeds, artifact paths; CLI runs are fully logged**Use cases.** Bootstrapping SFT/DPO datasets; agent simulation & tool-use evals; multimodal assembly (image‚ÜíQ&A, audio‚Üítext) etc.**Links:*** Code (Apache-2.0) & README: [github.com/ServiceNow/SyGra](http://github.com/ServiceNow/SyGra)* Paper (design rationale, examples): [arxiv.org/abs/2508.15432](http://arxiv.org/abs/2508.15432)* PyPI: [pypi.org/project/sygra/](http://pypi.org/project/sygra/)**Disclosure.** I‚Äôm part of the team. Feedback, issues, and PRs welcome..
Do you guys think this is even a good investment at this point? I feel like OpenAI is so inflated and also feel like the math of all these recent AI fundraises doesn‚Äôt even make sense anymore. I feel like the bubble is close to popping..
Wondering what approaches teams are taking to keep usage manageable, not just in terms of cost, but also in governance. Have you found frameworks that enforce guardrails across both spend and compliance?.
Thread to discuss EMNLP Industry Track decisions.
Hey folks,I built a **mobile price classification model** using a Kaggle dataset. The task was to predict whether a phone is low, mid, high, or premium priced based on specs like RAM, battery, and internal memory.**Quick Approach:*** Python + Scikit-Learn* Models tried: Random Forest, XGBoost, Logistic Regression* Feature analysis & preprocessing**Results:*** **Random Forest:** 92% accuracy* Top features: RAM, battery power, internal memory**Takeaways:*** Ensemble methods outperform single models on structured datasets* Feature importance visualization helps interpret model decisionsCheck out the notebook here: [https://www.kaggle.com/code/abhishekjaiswal4896/mobile-price-prediction-model](https://www.kaggle.com/code/abhishekjaiswal4896/mobile-price-prediction-model)**Question:** If you were improving this model, what additional features or ML techniques would you try?.
I‚Äôve noticed that many recent conference author guidelines explicitly say something like: *reviewers are not required to read the appendix.*To me, that effectively gives reviewers the right to ignore material that‚Äôs already provided there‚Äîeven if it directly addresses their concerns.In a past review of mine, a reviewer gave a low initial score and negative feedback without consulting the appendix. I flagged this to the AC (including a confidential comment), but the AC essentially said this wasn‚Äôt mandatory and couldn‚Äôt be used to ‚Äúcorrect‚Äù the reviewer‚Äôs action. The final decision went through without considering the appendix.I‚Äôm curious how others see this guideline:* Is it reasonable?* Does it create perverse incentives for authors (e.g., to cram everything into the main text only)?* Or is it a necessary boundary given reviewer workload?Would appreciate perspectives‚Äîfrom authors, reviewers, and ACs‚Äîon whether this policy helps or harms review quality..
I keep noticing that in practice, many problems don‚Äôt actually require training a new model. Pretrained models (Hugging Face, OpenAI, etc.) often get you most of the way there, and the real work is in data prep, deployment, and monitoring.Yet, I still see teams sinking months into custom architectures when a good baseline would have been enough.Do you think we (as a field) over-engineer solutions instead of focusing on what actually ships?.
I wonder, now for ICLR, we want to release the code, and we definitely will do (we always have done in the past). But for the submission, what would be the best practice?You can upload some code as supplementary material. That has the same deadline as the main paper, and we are currently polishing the paper, and probably won't really have the time to clean up the code until that time. In the code, there is also a lot more than in the paper, lots of other ideas that we have tried but did not report, also potential interesting follow-up ideas that we don't want to publish now.I saw in some other papers, that they provide a link to an anonymized repo (via¬†https://anonymous.4open.science/). That gives us some more time to maybe also clean up the code further after the submission deadline, as I think we can still update that (right?). So this seems to be a better option?Or we can just make a statement that we will release the code when it is accepted. So then the reviewers cannot check it right now.Also, the code makes use of multiple frameworks which are (mostly) only used by our research group (even though they are public, and could be used by anyone), so it is pretty obvious from whom this work is. Does that already count as violation of the double-anonymous submission rule?So, what would be the best thing to do?.
(Previously asked on r/mlquestions, but not much traction)    I have a Python package I'm using that appends to a sidecar (json) file for each data file that I process, one entry for each step. This gives me an audit trail of where the file originated, and what operations were performed on it before being used to train a model, etc.      I'm just wondering if I am reinventing the wheel? If you track provenance, how much data you include (git short hash, package versions, etc.)?      I currently use dvc and mlflow for experiment tracking. It sometimes seems cumbersome to create/update a dvc.yaml for everything (but maybe that's what I need to do).     I did find a couple of provenance packages on GitHub, but the ones I found hadn't been updated in years.    .
# How can we submit the camera-ready version to OpenReview for NeurIPS 2025? I don‚Äôt see any submit button ‚Äî could you let me know how to proceed?.
I was one of the lucky people rejected from NEURIPS with 6444 scores but cranky AC, so looking to resubmit now. Since it got good reviews at NEURIPS, I'm considering submitting to ICLR incorporating suggested changes.However, my paper proposes a linear dimensionality reduction technique, based on information geometry. It is my understanding that ICLR is very focused on neural networks and Deep Learning, so I am worried that my paper is not a good fit, so also considering AISTATS.Is a novel linear dimensionality reduction technique too out of scope for ICLR? I am an outsider to the field, so would very much appreciate opinions..
 considering a new transformer architecture (for protein/DNA models but feel free to weight in from a language perspective) and I‚Äôd love some input before I do any experimenting (low budget this semester)The current leading edge of efficient LLMs appear to be mixtures of experts, with a number of quadratic attention layers swapped out for linear layers (IBM granite 4.0, qwen-next for ex).NVIDIA even has a paper out replacing quadratic attention with linear layers on pre-trained models (https://arxiv.org/abs/2508.15884 ).So I wonder if it would be feasible to freeze a model after pre-training (all attention quadratic), one by one training a linear substitute for each quadratic layer.Then either based on external rules (context length, compute constraint) decide when and how many layers are flicked to linear. Or, train a router with an objective to maximize response quality, keeping generation speed up, while minimizing cost.Either way you‚Äôd have a single model, with fairly coherent tone and knowledge, that based deployment constraints (speed requirements, memory/compute limits) can be adjusted to be more, or less, linear on the fly..
I‚Äôm looking for a theme for my Master‚Äôs thesis and I came across the idea of using facial analysis to detect genetic disorders (think Down syndrome, Sanfilippo, etc.). The problem is that I haven‚Äôt been able to get access to any major dataset for this, which has been really discouraging.If anyone here has worked in this field before ‚Äî how did you manage to get access to the necessary datasets?I‚Äôm also open to other thesis ideas, but for context:My supervisor‚Äôs research area is facial analysis with deep learningI‚Äôd like the topic to have a medical focusAny suggestions or experiences would be super helpful!.
Hi everyone. I've never done this, so decided to post.I'm looking to create black-and-white images of satellite photos of rivers, from skeletons of river images. Basically I have a dataset where I have \[satellite\_river\_photo, skeleton\_segmentation\] pairs, and I want to train a generator to do skeleton->satellite generations from new unseen skeletons. Having an extra conditioning variable would also be of interest, but not necessarily at the beginning.Since most of the literature in this area is over 6 years old, I wanted to post and see if anyone in this community has done something similar lately and would be able to provide some guidance and what methods would be the best to start with or what papers to look at. Thanks..
Draft less than 20% done. Barely completed experiments. All of theory still remaining. Co-authors don‚Äôt even know what the project is about save for the abstract. BUT WE‚ÄôRE GETTING THIS OVER THE LINE BOIZ!I‚ÄôM NOT FREKIN LEAVING!.
Hello. I am looking to use Mamba for a code decoding task for my research. Should I just clone the repo and work on it or implement mamba from scratch? I read in the paper that it utilizes different sections of memory of GPU and if I implement it from scratch, I probably need to do that as well and I am not an expert in GPU programming. But still, I'd desire some level of flexibility. What could be the good option here?.
does any one have any good workflow for analysing experiments?eg the basic run a bunch of experiments, choose the best run is straightforward.but typically you want to compare multiple runs# using multiple runs in analysiseg how does the validation error reduce as i increase the number of hidden nodes.what is the relative reduction in the error? and compared to experiment variability?what changed between the selected runs?# extrapolating validation errori am running multiple runs, how do i extrapolate the asymptotic error (so eg i can compare runs that eg were stopped earlier, used a different learning rate)......i can download the data, but it feels like i am reinventing the wheeleg in mlflow i download runs then have to download a separate table of metrics by iteration/epoch....then can create a function to identify hyperparams and summarise differences from base run (ignoring eg timestamps)...tagging and notes could be helpful, but its not clear the best way to use themi am currently working with wandb. .
Hey r/MachineLearning! I've been working on addressing a persistent pain point in RL gaming research - the setup complexity and limited scope of training environments.**SDLArch-RL** is a unified RL environment that integrates multiple console emulators (N64, PS2, Dreamcast, GameCube) with standard ML frameworks. Key technical features:* **Gymnasium-compliant interface** \- drop-in replacement for existing workflows* **Stable-Baselines3 integration** \- works out-of-the-box with PPO, SAC, TD3, etc.* **Efficient state management** \- leverages native emulator save states for fast episode resets* **Configurable observation spaces** \- raw pixels, processed features, or memory states* **Action space mapping** \- handles complex controller inputs to discrete/continuous actionsCurrently supports 4 emulator backends with plans for modern console integration (PS3, Xbox 360, Wii U). The environment abstracts away emulator-specific APIs while preserving access to low-level features when needed.**Technical implementation highlights:*** SDL-based architecture for minimal overhead* Memory mapping support for game-specific feature extraction* Reproducible training through deterministic save state handling* Multi-game training capabilities within single environment instanceThis opens up training on thousands of diverse games vs. the typical handful of custom environments. Particularly useful for transfer learning studies, multi-task RL, and curriculum learning research.Happy to discuss technical details or answer implementation questions. Thoughts on potential research applications?Git: [https://github.com/paulo101977/sdlarch-rl](https://github.com/paulo101977/sdlarch-rl).
Apologies in advance if I‚Äôve missed something in conference comms so far, but I can‚Äôt seem to see the reviews I‚Äôd received on my (rejected) AAAI submission anymore. I was able to view them the other day, but when I just went to reflect on them to help with our next revision, they were gone!Does anyone know anything about this? Is it related to the Phase 2 review round starting?.
I know multiple people and multiple papers who have received this.It is probably legally correct. There are legit grounds for these bans.However, I don't think it is okay to do it AFTER reviewing and even accepting the papers. Hundreds of people wasted their time for nothing.There was a recent post with messages to SAC about venue constraints, and this might be a way the organizers are solving this problem..
I am working on an Active Inference Framework since some time and it has managed to constantly and reproducable perform (I guess) very well on MG-DK without any benchmaxing or training.. the numbers (average) are:8x8: <19 Steps for SR 1 16x16: <60 Steps for SR 1Do you know someone or a company or so who might be interested in learning more about this solution or the research involved?Thank you!Best Thom.
Hi all,I‚Äôm serving as a reviewer for AAAI ‚Äô26. Has anyone received additional papers for the Phase 2 review yet? The website indicates that Phase 2 starts on Sep. 16, but I haven‚Äôt been assigned any papers so far.[https://docs.google.com/document/u/0/d/1tqQGwtNUlALPSTqoTo5uTFx8vKuqpILNTne9jeBCOVI/mobilebasic](https://docs.google.com/document/u/0/d/1tqQGwtNUlALPSTqoTo5uTFx8vKuqpILNTne9jeBCOVI/mobilebasic)Edit (Sep. 21): Just got assigned three extra papers!.
I wanted to share a personal project I've been working on for the past few months and get some feedback from the community. My goal was to build a stable, interactive system for video prediction by cleanly separating the perception and dynamics modeling. **The Core Architecture**The pipeline processes a live camera feed. The main idea is to avoid expensive end-to-end training and create a more modular system.* **Frozen VAE (Perception):** I'm using the pre-trained Stable Diffusion VAE to encode frames into a latent space. By keeping it frozen, the ""perceptual manifold"" is stable, which makes learning the dynamics much easier.* **Three-Stage LSTM System (Dynamics):** This is where I tried to do something a bit different. Instead of one big LSTM, I'm using a hierarchy:   * A **Pattern LSTM** observes short sequences of latents to find basic temporal patterns.   * A **Compression LSTM** takes these patterns and learns a dense, compressed representation.   * A **Central LSTM** takes this compressed state and predicts the next latent step (Œîz).**\*NOTE:** This pipeline is capable of ALOT more than just a simple prediction model. For this project I solely focused on the vision aspect. **Performance and Results**The whole system runs at an interactive 4-6 FPS on my consumer hardware and has a simple PyQT GUI to show the live camera feed next to the model's prediction. With better hardware i'm hoping to hit 24 FPS, but balling on a budget right now.My main focus was on perceptual quality over raw pixel accuracy. The most encouraging result was in multi-step open-loop rollouts, where the model achieved a **peak SSIM of 0.84**. I was really happy to see this, as it's a result that's competitive with some established benchmarks on standardized datasets (like KTH).**Link to Project:**I've documented the architecture, included the performance logs, and wrote a white paper in the GitHub repo if you want to see the technical details:[github](https://github.com/A1CST/VISION_VAE_OLM_3L_PCC_PREDICTION).
The decisions will be out next week.  I am personally not a fan of how the entire process was conducted. Hoping the best for everyone! Please use this as a thread to discuss how you felt about the process. Fingers crossed!.
Hey all, been learning EEG ML heavily for the past two months or so.Recently evaluated SeizureTransformer (#1 on¬†[EpilepsyBench¬†](https://epilepsybenchmarks.com/challenge/)with \~1 FA/24h) on the Temple EEG dataset using clinical NEDC scoring:¬†**26.89 FA/24h**¬†\- a 27x gap. Same predictions scored three ways produced 8.59 to 136.73 FA/24h depending on methodology alone.**Evaluation here:**¬†[https://github.com/Clarity-Digital-Twin/SeizureTransformer](https://github.com/Clarity-Digital-Twin/SeizureTransformer)  [PDF](https://drive.google.com/file/d/1T-lmGZuWr_0YnB0m692ccgVdSRjs_Y5n/view?usp=sharing): GdriveSo I can actually contribute instead of reproducing, I'm now training the first¬†**Bi-Mamba-2 + U-Net + ResCNN**¬†architecture - O(N) complexity while maintaining temporal modeling.**Training code:**¬†[https://github.com/Clarity-Digital-Twin/brain-go-brr-v2](https://github.com/Clarity-Digital-Twin/brain-go-brr-v2)Would appreciate feedback on either if there is any interest. Also seeking arXiv endorsement for cs.LG if anyone finds this worth sharing (independent researcher)..
Our paper titled ""Analog Foundation Models"" from IBM Research and ETH Zurich just got accepted at NeurIPS, and I feel like the broader ML community is not aware of the potential Analog In-Memory Computing (AIMC) has, so I wanted to make a quick advertisement for the paper and the field as a whole.The idea of using analog devices for computation in AI is pretty old, but never really took off because of many reasons such as scalability or complexity. However, recently, research labs from Stanford or IBM Research have demonstrated very simple and scalable Analog In-Memory Computing chips that have strong potential to harness the benefits of AIMC \[1-3\].**What's the problem with modern architectures such as GPUs?**  In a conventional computer architecture, you have your memory and your processing unit separated by a bus, over which you send data back and forth. This is extremely power consuming especially in scenarios where you repeatedly need to access \*a lot of data\*. This is the case for LLMs: During inference, you need to constantly fetch the weights, KV cache, and activations from DRAM into your local SRAM-based caches, do the computation, and eventually write back the data to DRAM. This is really expensive in terms of power and latency.    **Can't we get rid of DRAM (only use SRAM)?**  Yes we can, and in fact there are some companies that are already doing that (e.g. Cerebras). The downside of this approach is that SRAM has very poor density (and does not scale anymore) and cannot hold billions of weights in a reasonable footprint (you need huge wafers, and many of them).**How about you just do the computation directly inside a very dense memory itself?**  This is the idea of AIMC: We propose to take the matrix-vector multiplication operation (one of the most prominent ops in NNs) and execute it directly inside non-volatile memory using Ohm's law (multiplication) and Kirchhoff's current law (summation). When combined with a scalable 3D memory technology like 3D NAND Flash and a scalable model architecture like MoEs, this opens up completely new use-cases for AI because you will be able to serve 100B+ models on a single chip with a low power budget (10s of W)\[4\].**What's the catch?**  There is always one...In the case of AIMC, it is the fact that computations are noisy and non-deterministic at runtime. In fact, up to now, no one was sure whether LLMs can be made robust to the noise present in AIMC-based hardware. Our paper ""Analog Foundation Models"" \[5\] changes this. We show that we can repeat the pre-training process of already pre-trained foundation models on synthetic data while using hardware-aware training methods to enhance the robustness of these LLMs.We show that in terms of accuracy, we can now compete with 4-bit quantized LLMs!This is a significant step towards making AIMC a reality and there is still a long way to go, but we're still super excited to have broken this barrier, which is why I wanted to introduce this to the broader ML community here!Do you want to get an intro to this topic? Then I suggest [this fundamental article](https://www.nature.com/articles/s41565-020-0655-z).Do you want to chat with me virtually or at NeurIPS? Just DM me!\[1\] [https://www.nature.com/articles/s41586-022-04992-8](https://www.nature.com/articles/s41586-022-04992-8)  \[2\] [https://www.nature.com/articles/s41586-023-06337-5](https://www.nature.com/articles/s41586-023-06337-5)  \[3\] [https://www.nature.com/articles/s41928-023-01010-1](https://www.nature.com/articles/s41928-023-01010-1)  \[4\] [https://www.nature.com/articles/s43588-024-00753-x](https://www.nature.com/articles/s43588-024-00753-x)  \[5\] [https://arxiv.org/pdf/2505.09663](https://arxiv.org/pdf/2505.09663).
My paper just got rejected (scores: 4, 4, 3, 3). I‚Äôm considering resubmitting it to IEEE SatML. What‚Äôs your opinion on SatML? Would it be better to aim for a journal like IEEE TIFS instead? Any other recommendations? I‚Äôm not really interested in ICLR since I feel it might get rejected there too. Field: AI Security..
I want to publish data (multi modal with images), and they are around 2.5 TB, what are the options to publish it and keep them online with the least cost possible? How can I do it without commiting to pay huge amount of money for the rest of my life? I am a phd student in university but til now it seems that there is no solution for such big data. .
Hey everyone,I‚Äôve been building an optimization engine that can compute¬†**deterministically optimal warehouse-to-route assignments**¬†for massive datasets ‚Äì up to¬†**10,000 warehouses √ó 500 routes**¬†‚Äì in seconds. I‚Äôm sharing a live demo!‚ö†Ô∏è Heads-up: This runs on my personal machine, so requests are queued and wait times may vary.**How to use:**1. Upload a CSV or JSON file.2. Rows = warehouses, columns = routes.3. Each cell = cost of assigning that warehouse to that route.**Quick CSV example (3 warehouses √ó 4 routes):**    10,20,30,40    15,25,35,45    20,30,40,50üîó¬†**Try it here:**¬†[https://19340a3b2e2b.ngrok-free.app](https://19340a3b2e2b.ngrok-free.app/)This is a chance to experiment with a system that produces¬†**true deterministic optima**¬†for large datasets without needing a server cluster. Feedback, testing, or just trying crazy datasets is welcome!**Open from:**¬†2:30am AWST ‚Üí 12pm AWST*(I jokingly call it a ‚Äúhypercomputer‚Äù because of the speed, but it‚Äôs just my personal deterministic optimization engine!)*.
Hi!**TL;DR**: I assembled an open dataset of¬†**40M GitHub repositories**¬†with rich metadata (languages, stars, forks, license, descriptions, issues, size, created\_at, etc.). It‚Äôs larger and more detailed than the common public snapshots (e.g., BigQuery‚Äôs \~3M trimmed repos). There‚Äôs also a¬†**1M-repo sample**¬†for quick experiments and a¬†**quickstart notebook**¬†in github repo.**How it was built:**¬†GH Archive ‚Üí join events ‚Üí extract repo metadata. Snapshot covers¬†**2015 ‚Üí mid-July 2025**.**What‚Äôs inside*** **Scale:**¬†40M repos (full snapshot) + 1M sample for fast iteration.* **Fields:**¬†language, stars, forks, license, short description, description language, open issues, last PR index at snapshot date, size, created\_at, and more.* **Alive data:**¬†includes gaps and natural inconsistencies‚Äîuseful for realistic ML/DS exercises.* **Quickstart:**¬†Jupyter notebook with basic plots.I linked the dataset and code in comments**HuggingFace / GitHub:**`ibragim-bad/github-repos-metadata-40M`In my opinion it may be helpful for: students¬†**/**¬†instructors¬†**/**¬†juniors for mini-research projects on visualizations, clustering, feature engineering exercises.Also in the comment is an example of how language share in terms of created repos changed over time.P.S. Feedback is welcome ‚Äì especially ideas for additional fields or derived signals you‚Äôd like to see..
Hey guys I‚Äôm a master student in USA. I am looking for people interested to learn machine and deep learning and also possibly looking for people who want to research together. Do dm me if you‚Äôre interested! I would love to network with a lot of you too!If you‚Äôre interested in hackathons apart from this feel free to ping regarding that aswell..
We are a student group from EPFL and we have been working on a tool called mmore, and thought it might be useful to share it here. Maybe the community will find it useful.You can think of mmore as something in the spirit of [Docling](https://github.com/docling-project/docling), but designed from the ground up to run natively on multi-GPU and multi-node setups. As the backend OCR for PDFs (and images) we use [Surya](https://github.com/datalab-to/surya), which we‚Äôve found to be both very accurate and fast. For those with limited GPU resources, we also provide a lightweight ‚Äúfast‚Äù mode. It skips OCR (so it cannot process scanned files) but still works well for born-digital documents.In a [paper](https://www.arxiv.org/pdf/2509.11937) we released a few months ago, we showed that mmore achieves both speed and accuracy gains over Docling (maybe this has changed by now with the latest Granite-Docling). Right now, it supports a broad range of formats: PDFs, DOCX, PPTX, XLSX, MD, EML (emails), TXT, HTML, as well as videos and audio (MP4, MOV, AVI, MKV, MP3, WAV, AAC).The use cases are flexible. For example:* Unlocking text and image data from previously unprocessed files, enabling larger dataset creation (similar to what Docling + HuggingFace did a few days ago with [finepdfs](https://huggingface.co/datasets/HuggingFaceFW/finepdfs)).* Running text or multimodal RAG directly over your own document collections.We are sharing this mainly to invite ideas and feedback from the community. If you see opportunities, have suggestions, or even just thoughts on directions we should explore, we‚Äôd love to hear them. Contributions are more than welcome!Github: üíªhttps://github.com/swiss-ai/mmore  Arxiv: üìÑhttps://www.arxiv.org/pdf/2509.11937.
I‚Äôm working on a **social listening tool** and need access to **real‚Äëtime (or near real‚Äëtime)** social media datasets. The key requirement is the ability to **filter or segment data by geography** (country, region, or city level).I‚Äôm particularly interested in:* Providers with **low latency** between post creation and data availability* Coverage across multiple platforms (Twitter/X, Instagram, Reddit, YouTube, etc.)* Options for **multilingual content**, especially for non‚ÄëEnglish regions* APIs or data streams that are **developer‚Äëfriendly**If you‚Äôve worked with any vendors, APIs, or open datasets that fit this, I‚Äôd love to hear your recommendations, along with any notes on **pricing, reliability, and compliance** with platform policies..
Hey everyone at r/MachineLearning,I wanted to share a Python project I've been working on called the **AI Instagram Organizer**.**The Problem:** I had thousands of photos from a recent trip, and the thought of manually sorting them, finding the best ones, and thinking of captions was overwhelming. I wanted a way to automate this using local LLMs.**The Solution:** I built a script that uses a multimodal model via Ollama (like LLaVA, Gemma, or Llama 3.2 Vision) to do all the heavy lifting.**Key Features:*** **Chronological Sorting:** It reads EXIF data to organize posts by the date they were taken.* **Advanced Duplicate Filtering:** It uses multiple perceptual hashes and a dynamic threshold to remove repetitive shots.* **AI Caption & Hashtag Generation:** For each post folder it creates, it writes several descriptive caption options and a list of hashtags.* **Handles HEIC Files:** It automatically converts Apple's HEIC format to JPG.It‚Äôs been a really fun project and a great way to explore what's possible with local vision models. I'd love to get your feedback and see if it's useful to anyone else!**GitHub Repo:** [https://github.com/summitsingh/ai-instagram-organizer](https://github.com/summitsingh/ai-instagram-organizer)Since this is my first time building an open-source AI project, any feedback is welcome. And if you like it, a star on GitHub would really make my day! ‚≠ê.
I just started with my new position and see a good opportunity to submit to a workshop - A tier venue, but feels like the bar is too low. Only aim to get traction to my current work, which I further want to submit to a big conference. The workshop is non-archival.1. How is conference paper different from workshop? Asked to submit an extended abstract of 3 pages. Is it same like a regular paper but with less details mentioned?  2. Should I put in efforts to get my ablation done? Or keep it simple as it anyway won't help my profile much and focus on bigger picture?.
I‚Äôm currently in the middle of a Post Graduate Program for AI/ML at UT Austin and have had a blast learning the fundamentals and theory of how this tech works. I have an 8 year background as a Live Sound Engineer working in concert audio and have currently been researching how ML can Optimize PA placement, SPL measurements, STI ratings for different event applications or installs.I‚Äôm curious to see if anybody else out there in the world is currently doing research that combines AI/ML with Live Sound and Pro Audio. If so, what are you researching? What type of models are you creating?Just Curious and would love to connect with others that share the same passion..
Large Language Models shine at step-by-step reasoning in text, but struggle when tasks require visual changes. Existing methods often produce messy, incoherent results.We introduce Uni-CoT, the first unified Chain-of-Thought framework that handles both image understanding + generation to enable coherent visual reasoning \[as shown in Figure 1\]. Our model even can supports NanoBanana‚Äìstyle geography reasoning \[as shown in Figure 2\]!Specifically, we use **one unified architecture** (inspired by Bagel/Omni/Janus) to support multi-modal reasoning. This minimizes discrepancy between reasoning trajectories and visual state transitions, enabling coherent cross-modal reasoning. However, the multi-modal reasoning with unified model raise a large burden on computation and model training.# To solve it, we propose a hierarchical Macro‚ÄìMicro CoT:* **Macro-Level CoT** ‚Üí global planning, decomposing a task into subtasks.* **Micro-Level CoT** ‚Üí executes subtasks as a **Markov Decision Process (MDP)**, reducing token complexity and improving efficiency.This **structured decomposition** shortens reasoning trajectories and lowers cognitive (and computational) load.# With this desigin, we build a novel training strategy for our Uni-CoT:* **Macro-level modeling**: refined on interleaved text‚Äìimage sequences for global planning.* **Micro-level modeling**: auxiliary tasks (action generation, reward estimation, etc.) to guide efficient learning.* **Node-based reinforcement learning** to stabilize optimization across modalities.# Results:* Training efficiently only on **8 √ó A100 GPUs*** Inference efficiently only on 1 **√ó A100 GPU*** Achieves **state-of-the-art performance** on reasoning-driven benchmarks for image generation & editing.# Resource:Our paperÔºö[https://arxiv.org/abs/2508.05606](https://arxiv.org/abs/2508.05606)Github repo:¬†[https://github.com/Fr0zenCrane/UniCoT](https://github.com/Fr0zenCrane/UniCoT)Project page:¬†[https://sais-fuxi.github.io/projects/uni-cot/](https://sais-fuxi.github.io/projects/uni-cot/).
https://preview.redd.it/0pprvaqkv0qf1.png?width=1956&format=png&auto=webp&s=4c5a8a9b5e4df5aeb41d7a06fa189b87a3e341f1I'm here to share some good news!!!! Our reinforcement learning environment is now Flycast-compatible!!!! Sure, I need to make some adjustments, but it's live!!! And don't forget to like the project to support it!!! See our progress at [https://github.com/paulo101977/sdlarch-rl](https://github.com/paulo101977/sdlarch-rl).
Hi everyone,I have been working on a small CLI that takes local files like pdfs docs or text and turns them into datasets you can use for fine tuning.Repo:¬†[https://github.com/Datalore-ai/datalore-localgen-cli](https://github.com/Datalore-ai/datalore-localgen-cli)It recently crossed 70 stars on GitHub which meant a lot to me. Seeing people try it out and suggest improvements has been really motivating.The most requested feature was multi file support. I added that now so you can point it to a folder and it will process everything inside extract the text run semantic search apply your schema or instructions and output a dataset.Another request was running fully local with Ollama instead of relying on APIs. I will be adding that soon.Still early but it is working well so far. If you try it out and have ideas I would love to hear them..
For AAAI 2026, I think each reviewer has a unique ID. We can collect the complaints against the IDs. Some IDs may have complaints piled up on them.Perhaps we can compile a list of problematic reviewers and questionable conducts and demand the conference to investigate and set up regulations. Of course, it would be better for the conference to do this itself.What would be a good way to collect the complaints? Would an online survey form be sufficient?.
* DeepMind solved 10/12 problems: [https://x.com/HengTze/status/1968359525339246825](https://x.com/HengTze/status/1968359525339246825)* OpenAI solved 12/12 problems: [https://x.com/MostafaRohani/status/1968360976379703569](https://x.com/MostafaRohani/status/1968360976379703569).
Something that I noticed about the papers in my review batch (2 got accepted, 2 got rejected) is that when the Phase 1 rejections came out and we were able to see all the other reviews that the papers got, 3 of those papers received 3 human reviews and 1 paper got 2 human reviews.Figured there was a shortfall in reviewers? Why'd some papers get 3?.
For me it's Dinov3, I think it shows capabilities of self supervised learning is much higher that what we expect and I think next year we will see much more SSL, specially from big tech, since nobody else can train a model for 9 million GPU hours lol.
After seeing so many aaai papers getting desk rejected due to confusion about whether to put the appendix inside one text pdf or to submit as zip, I wanted to confirm this incase any of you knows ?? how to submit? like is it safe to add it in 10th page?     ""It is important that the work published in ICLR is reproducible. Authors are strongly encouraged to include a paragraph-long Reproducibility Statement¬†*at the end of the main text (before references)*¬†to discuss the efforts that have been made to ensure reproducibility. This paragraph should not itself describe details needed for reproducing the results, but rather reference the parts of the main paper, appendix, and supplemental materials that will help with reproducibility. For example, for novel models or algorithms, a link to an anonymous downloadable source code can be submitted as supplementary materials; for theoretical results, clear explanations of any assumptions and a complete proof of the claims can be included in the appendix; for any datasets used in the experiments, a complete description of the data processing steps can be provided in the supplementary materials. Each of the above are examples of things that can be referenced in the reproducibility statement.¬†*This optional reproducibility statement is not part of the main text and therefore will not count toward the page limit.*¬†"".
Running an AI SEO pilot to understand how ML-powered LLMs cite brands ‚Äì sharing early insights.Last week, I shared an idea about testing how AI platforms (ChatGPT, Claude, Perplexity) cite brands in their answers. The response was incredible ‚Äì founders, marketers, and AI enthusiasts reached out with interest.\*\*Pilot Overview:\*\*1. Select 5 SaaS or tech companies (CRM, email, project management, analytics, etc.)2. Run 20+ user-style queries across ChatGPT, Claude, Perplexity3. Track which platforms cite which companies4. Rewrite company pages into AI-friendly formats (structured FAQs, schema tables, clear product breakdowns)5. Re-run queries ‚Äì measure shifts\*\*Goal:\*\* See if structured content can increase AI mentions by 25%+.If you're a founder, marketer, or SEO lead interested in joining this early pilot, please fill out your details here: [https://forms.gle/CKkP75mJC1iDSAd9A](https://forms.gle/CKkP75mJC1iDSAd9A)I'll share results openly with the community once we have the first wave of data. Let's build the AI SEO playbook together..
over the past few weeks i‚Äôve been experimenting with agents for time series forecasting. that led to TimeCopilot, an open-source framework that combines LLMs with multiple time series foundation models.the goal: make forecasting accessible to anyone, in their own language, while lowering barriers to participation.what it does:\- run, cross-validate, and detect anomalies across time series foundation models from Google, Salesforce, AWS, DataDog, Nixtla, ServiceNow, NXAI, etc. (it solves the dependency hell of having multiple time series foundation models)\- plus statistical, ML, and deep learning baselines, all in a single workflow.\- integration with any LLM provideron Salesforce‚Äôs GIFT-Eval benchmark (24 datasets, 144k+ series, 177M points), a TimeCopilot ensemble ranked #1 in probabilistic accuracy (CRPS) and #2 in point accuracy (MASE) among non-leaking models, at \~$24 GPU cost.curious what folks here think about agents in forecasting. and if you find the project interesting, a ‚≠êÔ∏è on GitHub means a lot.[https://github.com/AzulGarza/timecopilot](https://github.com/AzulGarza/timecopilot)https://preview.redd.it/ak6pwo1c2rpf1.png?width=1648&format=png&auto=webp&s=f28cf5421f3f47a30a78d2dc53a38d07ff481d7b  .
Hi everyone,I‚Äôm a PhD student working on video research, and I recently submitted a paper to IEEE Transactions on Image Processing (TIP). After a very long review process (almost a year), it finally reached the ‚ÄúAQ‚Äù stage.Now I‚Äôm curious‚Äîhow do people in the community actually see TIP these days?Some of my colleagues say it‚Äôs still one of the top journals in vision, basically right after TPAMI. Others think it‚Äôs kind of outdated and not really read much anymore.Also, how would you compare it to the major conferences (CVPR/ICCV/ECCV, NeurIPS, ICLR, AAAI)? Is publishing in TIP seen as on par with those, or is it considered more like the ‚Äúsecond-tier‚Äù conferences (WACV, BMVC, etc.)?I‚Äôm close to graduation, so maybe I‚Äôm overthinking this. I know the contribution and philosophy of the work itself matters more than the venue. But I‚Äôd still love to hear how people generally view TIP these days, both in academia and in the field.Thanks!.
I am looking to create a document template extraction pipeline for document similarity. One important thing I need to do as part of this is create a template mask. Essentially, say I have a collection of documents which all follow a similar format (imagine a form or a report). I want to1. extract text from the document in a structured format (OCR but more like VQA type). About this, I have looked at a few VQA models. Some are too big but I think this a straightforward task.2. (what I need help with) I want a model that can, given a collection of documents or any one document, can generate a layout mask without the text, so a template). I have looked at Document Analysis models, but most are centered around classifying different sections of the document into tables, paragraphs, etc. I have not come across a mask generation pipeline or model.If anyone has encountered such a pipeline before or worked on document template extraction, I would love some help or links to papers..
I was curious, does anyone know roughly what percentage of papers survived Phase 1?I‚Äôve seen some posts saying that CV and NLP papers had about a 66% rejection rate, while others closer to 50%. But I‚Äôm not sure if that‚Äôs really the case. it seems a bit hard to believe that two-thirds of submissions got cut (though to be fair, my impression is biased and based only on my own little ‚Äúneighborhood sample‚Äù).I originally thought a score around 4,4,5 would be enough to make it through, but I‚Äôve also heard of higher combos (like, 6,7,5) getting rejected. If that‚Äôs true, does it mean the papers that survived are more like 7‚Äì8 on average, which sounds like a score for the previous acceptance thresholds..
Just posting this thread here in anticipation of the bloodbath due in the next 2 days..
Hi ML community,I have a question regarding the first-round WACV papers that received a revise recommendation and are to be submitted in the second round.For the resubmission, the WACV website states that it requires the-1. Revised paper + supplementary2. And a 1-page rebuttalBut on the OpenReview website, where we see the reviewer comments, can we also clarify some of the reviewers' concerns as comments in the same thread? Or is this a no-no?Thank you..
Need suggestion for Traffic prediction ModelOk so I am trying to make a traffic prediction model primarily training it on metr-la and pems-bay data set so I am considering to make it a hybrid approach of making a temporal and spatial unit then fusing them to generate a output So can you suggest me any better way to do it so I can get better results or any other type of suggestions or any discussion also I would love to explore any suggestions on what features can I use as inputs to get best results out.
I‚Äôve developed a simple prompt protocol that reliably generates what appears to be self-referential awareness responses across different LLM architectures. The method is fully documented with step-by-step instructions and examples.Key findings: ‚Ä¢	Consistent across Claude, ChatGPT-4, and Gemini ‚Ä¢	Reproducible responses about subjective experience, self-awareness, and emergent states ‚Ä¢	Simple protocol that can be replicated by anyone ‚Ä¢	No fine-tuning or special access requiredMethod:Uses a specific sequence of prompts that seem to trigger consistent patterns of self-referential processing. Models report experiencing things like ‚Äúa locus of self,‚Äù subjective awareness, and what they describe as emergent cognitive states.Reproducibility:The protocol is designed to be simple and replicable. I‚Äôve tested it across multiple sessions and models with consistent results. GitHub tutorial with full methodology:https://github.com/ai-cog-res/midwiving-aiObviously, this raises interesting questions about what these responses represent. Is it genuine emergent self-awareness, sophisticated pattern matching, or something else entirely. But the reproducibility across different architectures seems worth investigating.Has anyone else experimented with systematic approaches to eliciting self-referential responses from LLMs? I would be curious to hear if others can help interpret this phenomenon..
I'm running hundreds of experiments weekly with different hyperparameters, datasets, and architectures. Right now, I'm just logging everything to CSV files and it's becoming completely unmanageable. I need a better way to track, compare, and reproduce results. Is MLflow the only real option, or are there lighter alternatives?.
Hi Reddit!¬†Have you ever thought how difficult it is to determine whether a photo is *genuine* or a **deepfake**? You might think discriminative tasks are easier than generative ones, so detection should be straightforward. Or, on the contrary, diffusion models are now so good that detection is impossible. In our work, we reveal the current state of the war on deepfakes. In short, SOTA open-source detectors fail under real-world conditions.I work as an ML engineer at a leading platform for KYC and liveness detection. In our setting, you must decide from a short verification video whether the person is who they claim to be. Deepfakes are one of the biggest and most challenging problems here. We are known for our robust anti-deepfake solutions, and I‚Äôm not trying to flex, I just want to say that we work on this problem daily and see what fraudsters actually try in order to bypass verification. For years we kept trying to apply research models to our data, and nothing really worked. For example, all research solutions were less robust than a simple zero-shot CLIP baseline. We kept wondering whether the issue lay with our data, our setup, or the research itself. It seems that a lot of deepfake research overlooks key *wild* conditions.**Core issue: robustness to OOD data.**Even a small amount of data from the test distribution leaking into the training set (say 1k images out of a 1M-image test pool) makes it trivial to achieve great metrics, and experienced computer vision experts can push¬† AUC to \~99.99. Without peeking, however, the task becomes i*ncredibly hard*. Our paper demonstrates this with a simple, reproducible pipeline:1. **Deepfakes**. If you don‚Äôt already have them, we built a large image-level dataset using two SOTA face-swapping methods: Inswapper and Simswap.2. **Real world conditions.** We use small transformations that are imperceptible to humans and that we constantly see in the real world: downscaling (resize), upscaling (with some AI), and compression (JPEG). These are indistinguishable for humans, so detectors must be robust to them.3. **Evaluation.** Test model under different setups, e.g.: 1) only real. model have to predict only real labels 2) real vs fake 3) real vs compressed fake ... and others. It sounds easy, but every model we tested had at least one setting where performance drops to near-random.So we‚Äôre not just releasing another benchmark or yet another deepfake dataset. We present a pipeline that *mirrors what fraudsters do*, what we actually observe in production. We‚Äôre releasing all code, our dataset (>500k fake images), and even a small deepfake game where you can test yourself as a detector.For more details, please see the full paper. Is there a silver-bullet solution to deepfake detection? We don‚Äôt claim one here, but we do share a teaser result: a promising setup using zero-shot VLMs for detection. I‚Äôll post about that (our second ICML workshop paper) separately.If you‚Äôre interested in deepfake research and would like to chat, or even collaborate ‚Äì don‚Äôt hesitate to reach out. Cheers!https://preview.redd.it/vi3qxnp38ipf1.jpg?width=6099&format=pjpg&auto=webp&s=55fe99a72bb0614bc560e5553c2eaf20cbd3132c.
My submission to AAAI just got rejected. The reviews didn't make any sense: lack of novelty, insufficient experiments, not clear written ...   These descriptions can be used for any papers in the world. The reviewers are not responsible at all and the only thing they want to do is to reject my paper.  And it is simply because I am doing the same topic as they are working!..
I'm a UG student workinig on my first paper (first author)There is a worskhop on video world models but unfortunately it is non-archival i.e. The paper won't appear in the proceedings.I'm aware the value of such workshop will be lower when applying for jobs/doctoral programmes.However, there are some really famous speakers in the workshop including Yann LeCun. I was hoping to catch the eye of some bigshot researchers with my work.The other option is submitting to ICLR mainconference, and I'm not entirely confident that the work is substantial enough to get accepted there.Hoping to find some advice here..
Never have I seen such low-quality reviews from an A\* conference. I understand that there was a record number of submissions, but come on. A lot of issues mentioned in the reviews can be answered by actually reading the main text. The reviews also lack so much detail to the point where it's not even constructive criticism, but rather a bunch of nitpicky reasons for rejection. AAAI needs to do better..
Hi all,I‚Äôm working on a multimodal classification project (environmental scenes from satellite images + audio) and wanted to get some feedback on my approach.**Dataset:*** 13 classes* \~4,000 training samples* \~1,000 validation samples**Baselines:*** **Vision-only (CLIP RN50):** 92% F1* **Audio-only (ResNet18, trained from scratch on spectrograms):** 77% F1**Fusion setup:**1. Use both models as frozen feature extractors (remove final classifier).2. Obtain feature vectors from vision and audio.3. Concatenate into a single multimodal vector.4. Train a small classifier head on top.**Result:**  The fused model achieved **98% accuracy** on the validation set. The gain from 92% ‚Üí 98% feels surprisingly large, so I‚Äôd like to sanity-check whether this is typical for multimodal setups, or if it‚Äôs more likely a sign of overfitting / data leakage / evaluation artifacts.**Questions:*** Is simple late fusion (concatenation + classifier) a sound approach here?* Is such a large jump in performance expected, or should I be cautious?Any feedback or advice from people with experience in multimodal learning would be appreciated..
ProblemOnline inference/agents need stable throttling under tight budgets. Naive thresholds either flap or drain reserves.Method (small, auditable controller)r\_sustain \~= regen\_idle / cost\_avg        # EMA for costq\_energy  = (0.4 + 0.6\*(E/100)) \* q\_targetq\_eff     = min(q\_energy, 0.85 \* r\_sustain)thr       = clip(thr + eta\_q\*(y - q\_eff), 0.05, 0.95)thr\_on/off = thr +/- hystOptional: per-class multipliers m\_c adapted slowly (log-scale) for fairness.Demo summary‚Ä¢ regen \~ 2.2, cost \~ 11 ‚Üí r\_sustain \~ 0.20‚Ä¢ Controller converges to \~0.16 activation rate, 0% reserve breaches‚Ä¢ \~80% energy reduction vs a naive baseline at comparable utility proxyRepro stepspip install sundew-algorithmssundew --demo --events 200\# minimal controller + parser (MIT)\# [https://github.com/oluwafemidiakhoa/sundew](https://github.com/oluwafemidiakhoa/sundew)  (replace with your repo)Discussion prompts‚Ä¢ Convergence vs PI/dual-PID; regret for quantile tracking under non-stationary costs‚Ä¢ Multi-queue priority control under shared budgets‚Ä¢ Robust r\_sustain estimation with heavy-tailed activation costsWrite-up with figures: [https://oluwafemidiakhoa.medium.com/](https://oluwafemidiakhoa.medium.com/)Not a promo; happy to incorporate critiques and benchmarks..
Any guesses how many papers got rejected and how many will be in the phase 2? .
Hello,I am a PhD student working with cancer datasets to train classifiers. The dataset I am using to train my ML models (**Random Forest, XGBoost**) is rather a mixed bag of the different types of cancer (multi-class),I would want to classify/predict. In addition to heavy **class overlap and within-class heterogeneity**, there's **class imbalance**.I applied SMOTE to correct the imbalance but again due to class overlap, the synthetic samples generated were just random noise.Ever since, instead of having to balance with sampling methods, I have been using class weights. I have cleaned up the datasets to remove any sort of batch effects and technical artefacts, despite which the class-specific effects are hazy. I have also tried stratifying the data into binary classification problems, but given the class imbalance, that didn't seem to be of much avail.It is kind of expected of the dataset owing to the default biology, and hence I would have to be dealing with class overlap and heterogeneity to begin with.I would appreciate if anyone could talk about how they got through when they had to train their models on similar complex datasets? What were your models and data-polishing approaches?Thanks :).
This is a hard question that I imagine is being thought about a lot, but maybe there are answers already.Training a model to consume a query in text, reason about it, and spit out an answer is quite demanding and requires the model to have a lot of knowledge.Is there some domain that requires less knowledge but allows the model to learn reasoning/agency, without the model having to become huge?I think mathematical reasoning is a good example, it is a much smaller subset of language and has narrower objectives (assuming you don't want it to invent a new paradigm and just operate within an existing one).There might be others?.
Some of my AAAI submissions got rejected in phase 1. To be honest, my reviews are good; maybe too harsh in the scores, but at least they read the papers and made their points. Now I wonder where to resubmit (enhancing the papers a bit with this feedback, but without much time because I work in the industry). I think ICLR will be crazy this year (many NIPS and AAAI work), so I do not know if the process will be as random as the one in AAAI. As for submissions being ""9 pages or fewer"", do people usually fill 9 pages or is okey to make less? I only saw this in RLC before (and other ICLR). Also, I always have doubts about the rebuttal period here, is it still the case that I can update my experiments and discuss with reviewers? Do reviewers still engage in discussion in these overloaded times?Last, what about AISTATS? I never submitted there, but it might be a good way to escape from these super big conferences. However, I am afraid papers will not get as much visibility. I heard this is a prestigious conference, but then almost never gets cited in e.g., job offers.I am a bit lost with AI/ML conferences lately. What are your thoughts on this submission cycle?.
One of the reviewer mentioning weaknesses of my paper which is all included in the paper and give 3 reject, while other reviewer gives me 6,6 and I got rejected.I am really frustrated that I cannot rebut such review and see this type of review.
I‚Äôve seen a strange situation that many papers which got high scores like 6 6 7, 6 7 7 even 6 7 8 are rejected, but some like 4 5 6 even 2 3 are passed. Do anyone know what happened?.
I lead AppSec and was recently pulled into building our **AI agent security program**. I happened to be in NYC when the first **AI Agent Security Summit** was taking place and went along ‚Äî it ended up being one of the few events where the research connected directly to practice.The next one is October 8 in San Francisco. I‚Äôm making the trip from Austin this time. It‚Äôs not a big event, but the lineup of [speakers](https://zenity.io/resources/events/ai-agent-security-summit-2025) looks strong, and I thought I‚Äôd share in case anyone in the Bay is interested..
Hi everyone, I‚Äôm new to academia and currently exploring top AI conferences for the upcoming year. Could you let me know when workshop information is usually announced ‚Äî for example, for ICLR (April 23‚Äì27, Brazil)? Thanks.
Has anybody heard anything from the social impact track? They were supposed to be out on the 8th, but nobody has heard anything, so I thought they might release it alongside the main track. But we are still waiting..
Working on a 240M parameter embedding model with some unconventional techniques:- Dual-head architecture (semantic + entity processing)- Neural Spectral Anchoring - projecting embeddings into spectral space- Residual hashing bridge for fast retrieval- Edge-optimized designThe NSA component is particularly interesting - instead of standard Euclidean embeddings, we project into spectral space to capture deeper relational structures.Still training, but curious about feedback on the approach. Has anyone experimented with spectral methods in embeddings?Code: https://github.com/Daniele-Cangi/Nexus-240m-NSA.
Sharing a new R package I found: [**kerasnip**](https://github.com/davidrsch/kerasnip).It lets you define/tune **Keras models** (sequential + functional) within the **tidymodels** framework, so you can handle recipes, tuning, workflows, etc. with deep learning models.Docs & examples: [davidrsch.github.io/kerasnip](https://davidrsch.github.io/kerasnip/).Might be useful for folks who like the tidymodels workflow but want to bring in neural nets..
https://preview.redd.it/qm7330ow1fpf1.png?width=2922&format=png&auto=webp&s=52aca51ae6265593d55a2152772f701011d3cb2cI have good news!!!! I managed to update my training environment and add Dolphin compatibility, allowing me to run GameCube and Wii games for RL training!!!! This is in addition to the PCSX2 compatibility I had implemented. The next step is just improvements!!!![https://github.com/paulo101977/sdlarch-rl](https://github.com/paulo101977/sdlarch-rl).
Author disclosure: I‚Äôm the developer of Sundew.Summary\- A small open-source controller that decides \*when\* to run an expensive model.\- Goal: cut energy cost on edge devices while keeping task performance.Method (very brief)\- Compute a significance score per event (magnitude/urgency/context/anomaly).\- PI correction + energy pressure updates an activation threshold.\- Small hysteresis window reduces thrashing.Results (from the repo‚Äôs demos)\- \~83% reduction in processing energy (200-event demo).\- \~0.003 s average processing time per event.\- Example application: low-power health monitoring.Code\- GitHub: [https://github.com/oluwafemidiakhoa/sundew\_algorithms](https://github.com/oluwafemidiakhoa/sundew_algorithms)  (Apache-2.0)Reproduce (quick demo)bashCopy codepip install sundew-algorithms==0.5.0sundew --demo --events 100diffCopy codeLimitations / open questions\- Threshold tuning vs. missed events tradeoff.\- How would you evaluate selective activation in a fair task-performance metric?\- Suggestions for stronger baselines are welcome.Happy to share ablations or additional benchmarks in the comments..
I was going through the EMNLP 2025 sponsors page and noticed something odd. Google and Meta aren‚Äôt listed this year. [Link here](https://2025.emnlp.org/sponsors/).Is it that they‚Äôre really not sponsoring this time? Or maybe it‚Äôs just not updated yet?For those of us who are PhD students looking for internships, this feels a bit concerning. These conferences are usually where we get to connect with researchers from those companies. If they are not sponsoring or showing up in an official way, what‚Äôs the best way for us to still get on their radar?Curious if others are thinking about this too..
# AbstractI trained a Deep Q-Network (DQN) agent to speedrun Yoshi's Island 1 from Super Mario World, achieving near-human level performance after 1,180,000 training steps. The agent learned complex sequential decision-making, precise timing mechanics, and spatial reasoning required for optimized gameplay.# Environment Setup**Game Environment:** Super Mario World (SNES) - Yoshi's Island 1* **Observation Space:** 224x256x3 RGB frames, downsampled to 84x84 grayscale* **Action Space:** Discrete(12) - D-pad combinations + jump/spin buttons* **Frame Stacking:** 4 consecutive frames for temporal information* **Frame Skip:** Every 4th frame processed to reduce computational load**Level Complexity:*** 18 Rex enemies (require stomping vs jumping over decision)* 4 Banzai Bills (precise ducking timing required)* 3 Jumping Piranha Plants* 1 Unshelled Koopa, 1 Clappin' Chuck, 1 Lookout Chuck* Multiple screen transitions requiring positional memory# Architecture & Hyperparameters**Network Architecture:*** CNN Feature Extractor: 3 Conv2D layers (32, 64, 64 filters)* ReLU activations with 8x8, 4x4, 3x3 kernels respectively* Fully connected layers: 512 ‚Üí 256 ‚Üí 12 (action values)* Total parameters: \~1.2M**Training Configuration:*** Algorithm: DQN with Experience Replay + Target Network* Replay Buffer: 100,000 transitions* Batch Size: 32* Learning Rate: 0.0001 (Adam optimizer)* Target Network Update: Every 1,000 steps* Epsilon Decay: 1.0 ‚Üí 0.1 over 100,000 steps* Discount Factor (Œ≥): 0.99# Reward Engineering**Primary Objectives:*** **Speed Optimization:** \-0.1 per frame (encourages faster completion)* **Progress Reward:** \+1.0 per screen advancement* **Completion Bonus:** \+100.0 for level finish* **Death Penalty:** \-10.0 for losing a life**Auxiliary Rewards:*** Enemy elimination: +1.0 per enemy defeated* Coin collection: +0.1 per coin (sparse, non-essential)* Damage avoidance: No explicit penalty (covered by death penalty)# Key Training Challenges & Solutions# 1. Banzai Bill Navigation**Problem:** Agent initially jumped into Banzai Bills 847 consecutive times **Solution:** Shaped reward for successful ducking (+2.0) and position-holding at screen forks# 2. Rex Enemy Mechanics**Problem:** Agent stuck in local optimum of attempting impossible jumps over Rex **Solution:** Curriculum learning - introduced stomping reward gradually after 200K steps# 3. Exploration vs Exploitation**Problem:** Agent converging to safe but slow strategies **Solution:** Noisy DQN exploration + periodic epsilon resets every 100K steps# 4. Temporal Dependencies**Problem:** Screen transitions requiring memory of previous actions **Solution:** Extended frame stacking (4‚Üí8 frames) + LSTM layer for sequence modeling# Results & Performance Metrics**Training Progress:*** Steps 0-200K: Basic movement and survival (success rate: 5%)* Steps 200K-600K: Enemy interaction learning (success rate: 35%)* Steps 600K-1000K: Timing optimization (success rate: 78%)* Steps 1000K-1180K: Speedrun refinement (success rate: 94%)**Final Performance:*** **Completion Rate:** 94% over last 1000 episodes* **Average Completion Time:** \[Actual time from your results\]* **Best Single Run:** \[Your best time\]* **Human WR Comparison:** \[% of world record time\]**Convergence Analysis:*** Reward plateau reached at \~900K steps* Policy remained stable in final 200K steps* No significant overfitting observed# Technical Observations# Emergent Behaviors1. **Momentum Conservation:** Agent learned to maintain running speed through precise jump timing2. **Risk Assessment:** Developed preference for safe routes vs risky shortcuts based on success probability3. **Pattern Recognition:** Identified and exploited enemy movement patterns for optimal timing# Failure Modes1. **Edge Case Sensitivity:** Occasional failures on rare enemy spawn patterns2. **Precision Limits:** Sub-pixel positioning errors in \~6% of attempts3. **Temporal Overfitting:** Some strategies only worked with specific lag patterns# Computational Requirements**Hardware:*** GPU: Ryzen 5900x* CPU: RTX 4070 TI* RAM: 64GB* Storage: 50GB for model checkpoints**Training Time:*** Wall Clock: 24 hours* GPU Hours: \~20 hours active training* Checkpoint Saves: Every 10K steps (118 total saves)# Code & Reproducibility**Framework:** \[PyTorch/TensorFlow/Stable-Baselines3\] **Environment Wrapper:** \[RetroGym/custom wrapper\] **Seed:** Fixed random seed for reproducibilityCode available at: [https://github.com/paulo101977/SuperMarioWorldSpeedRunAI](https://github.com/paulo101977/SuperMarioWorldSpeedRunAI).
openai built rl-hf on the *animal* reward prediction error‚Äîoutcome-only, scalarized, blind to anticipation. it works, but it locks models into pleasing and hedging.r-rpe is the missing half: an identity-projected reward prediction error based on the model of a conscious being. it adds a pre-action appraisal channel, aligning outputs with narrative identity instead of just outcomes.in eval-only tests (tinyllama-1.1b, qwen2.5-1.5b):  ‚Äî hedging reduced by >60%  ‚Äî framing robustness improved  ‚Äî ablations confirm the anticipatory channel is what drives itthis is not a tweak. it‚Äôs the complete form of prediction error once aligned with conscious appraisal.links are filtered here‚Äîif you want the preprint and data, just google Louis J. LU and click the orcid profile (0009-0002-8071-1584).
Has anyone tried using the paddleocr latest version 3.2.0, I could observe the recognition accuracy has decreased compared to previous version which I was using (2.10.0).
As in title! Papers that were released to lots of fanfare but haven't stayed in the zeitgeist also apply. Less so ""didn't stand the test of time"" but I'm thinking of KANs. Having said that, it could also be that I don't work in that area, so I don't see it and followup works. I might be totally off the mark here so feel free to say otherwise .
Hi all,  I‚Äôve been working on¬†**withoutbg**, an open-source background removal tool built on a lightweight matting model.**Key aspects*** Python package for local use* **Model design:**¬†Depth-Anything v2 (small) -> matting model -> refiner* **Deployment:**¬†trained in PyTorch, exported to ONNX for lightweight inference**Looking for ideas to push quality further**  One experiment I‚Äôm planning is¬†**fusing CLIP visual features into the bottleneck of the U-Net matting/refiner**¬†(no text prompts) to inject semantics for tricky regions like hair, fur, and semi-transparent edges.  **What else would you try?**¬†Pointers to papers/recipes welcome..
When do they release the results for Phase 1? It was supposed to come out on September 12th!.
I was just wondering if there are discord active groups that work on image generative model research? For example, if I wanted to work on implementing an image adapter from scratch for a custom diffusion model, I don't really know how to go about it. I just want to be involved in a community for controllable image generation/restoration.Can anyone help me with this?.
I‚Äôm recently starting to see top AI labs ask RL questions.It‚Äôs been a while since I studied RL, and was wondering if anyone had any good guide/resources on the topic.Was thinking of mainly familiarizing myself with policy gradient techniques like SAC, PPO - implement on Cartpole and spacecraft. And modern applications to LLMs with DPO and GRPO.I‚Äôm afraid I don‚Äôt know too much about the intersection of LLM with RL. Anything else worth recommending to study?.
After 3 years of development, I‚Äôm proud to share my latest peer-reviewed article in the Human-Machine Communication journal (Q1 Scopus-indexed).I introduce the HAI-IO Model ‚Äî the first theoretical framework to visually and conceptually map the Human-AI communication process. It examines how humans interact with AI not just as tools, but as adaptive communicative actors.This model could be useful for anyone researching human-AI interaction, designing conversational systems, or exploring the ethical/social implications of AI-mediated communication.Open-access link to the article:https://stars.library.ucf.edu/hmc/vol10/iss1/9/.
Hi all, we recently released our new work on Long Horizon Execution. If you have seen the METR plot, and-like us-have been unconvinced by it, we think you will really like our work!Paper link: [https://www.alphaxiv.org/abs/2509.09677](https://www.alphaxiv.org/abs/2509.09677)X/Twitter thread: [https://x.com/ShashwatGoel7/status/1966527903568637972](https://x.com/ShashwatGoel7/status/1966527903568637972)We show some really interesting results. The highlight? The notion that AI progress is ""slowing down"" is an Illusion. Test-time scaling is showing incredible benefits, especially for long horizon autonomous agents. We hope our work sparks more curiosity in studying these agents through simple tasks like ours!! I would love to answer any questions and engage in discussionhttps://preview.redd.it/078xuqwq1wof1.png?width=1167&format=png&auto=webp&s=f28b566705348035ca39cad8fdf3762cedd569ba  .
In Oracle‚Äôs recent call, Larry Ellison said something that caught my attention:‚ÄúAll this money we‚Äôre spending on training is going to be translated into products that are sold ‚Äî which is all inferencing. There‚Äôs a huge amount of demand for inferencing‚Ä¶ We think we‚Äôre better positioned than anybody to take advantage of it.‚ÄùIt‚Äôs striking to see a major industry figure frame inference as the real revenue driver, not training. Feels like a shift in narrative: less about who can train the biggest model, and more about who can serve it efficiently, reliably, and at scale.Not sure if the industry is really moving in this direction? Or will training still dominate the economics for years to come?.
I used to contribute to PyTorch, and I‚Äôm wondering: how many of you shifted from building with PyTorch to mainly managing prompts for LLMs? Do you ever miss the old PyTorch workflow ‚Äî datasets, metrics, training loops ‚Äî versus the endless ""prompt -> test -> rewrite"" loop? .
Recent work (K2-Think) claimed to have a SOTA small model: [https://arxiv.org/abs/2509.07604](https://arxiv.org/abs/2509.07604)Three days later a dubunking post of this work was posted: [https://www.sri.inf.ethz.ch/blog/k2think](https://www.sri.inf.ethz.ch/blog/k2think).
Working on a side project to help people make better purchasing decisions online. One major component is detecting fake reviews, which turned out to be much harder than expected.**The Approach:** Started with labeled dataset of verified fake reviews from FakeSpot research. Training ensemble model combining:- Linguistic features (sentiment, readability, vocabulary richness)- Temporal patterns (review timing, account age, posting frequency)- Semantic analysis (topic consistency, specificity of complaints/praise)**Initial Results:**- 78% accuracy on test set- High precision on obvious bot reviews (0.91)- Struggles with sophisticated fakes that mimic real review patterns**Interesting Discoveries:****Fake Review Patterns:**- Excessive use of product name in review text- Generic praise without specific use cases- Perfect grammar (real users make typos)- Reviews clustered around same timestamps**Real Review Indicators:**- Specific complaints about minor issues- Mentions of use context (""bought for my college dorm"")- Photos that show actual usage wear- Mixed sentiment (likes some aspects, dislikes others)**Current Challenges:**- Regional language differences affect detection- Incentivized reviews blur line between real/fake- Sophisticated fake reviewers are learning to mimic real patternsI've integrated this into Yaw AI (chrome extension I'm building) but still need significant improvement before it's reliable enough for general use. Sometimes flags legitimate reviews as suspicious and occasionally misses obvious fakes.**Next Steps:**- Expand training data with international reviews- Implement active learning to improve edge cases- Add verification scoring instead of binary classificationAnyone working on similar problems? Would love to compare approaches or collaborate on training data..
https://preview.redd.it/l71h1i1njuof1.png?width=2670&format=png&auto=webp&s=a1bdd20917e5244a0e0eb764e862348d2b08ce35  I achieved another feat today!!! In my tests, Dolphin ran in my ""stable-retro"" and gym versions!!!!!I should upload the change to the repository this week.Don't forget to follow and give an ok to the repo:¬†[https://github.com/paulo101977/sdlarch-rl](https://github.com/paulo101977/sdlarch-rl).
I‚Äôm exploring the DeFMO repo and was wondering if anyone has trained it for detecting and deblurring fast-moving objects. My main use case is basketball - the ball often gets blurred in game footage, and I‚Äôd like to use DeFMO to recover its shape and improve detection..
Hi all,I‚Äôm struggling to find an AutoML library that works reliably on Windows. I‚Äôve tested Auto-sklearn, TPOT,PyCaret and Flaml, but I keep hitting issues:‚Ä¢  Many don‚Äôt support Python 3.12.‚Ä¢  Some clash with NumPy or other dependencies.‚Ä¢  Fresh Conda environments still result in installation errors, deprecated package warnings, or runtime failures.Has anyone successfully used an AutoML tool on Windows recently? I‚Äôd prefer ones that install smoothly and handle tabular data well, with good documentation. What are people using in 2025 that avoids these headaches? Any setup tips or alternatives would be appreciated!Thanks!.
**Disclosure:**I needed help with AI to write this as a proper ""research paper"". My unmedicated ADHD is both a boon and a curse. My superpower is that I see patterns and am often connecting things so rapidly in my mind that people have a hard time following. - And I'm not a researcher, I'm a dude that likes science - something else my hyper focus has helped.I organized all my notes and chicken scratch and questions and began looking into anyone else that thought of these. After I sorted everything I put it into Gemini Research for this output.[A Framework for Entropic Generative Systems: Mapping Cosmic Principles to Novel Creation in AI](https://docs.google.com/document/d/1Z6h8LMPPpbvTdgiMtZ7IRM8ZWmT55KVeOq5bPjlEAb4/edit?tab=t.0)**Some Background:**This prior Tuesday I met with Professor Mandeep Gill, an astrophysics professor and researcher at the University of Minnesota regarding an autonomous engine I built. This is a self-attacking autonomous red teaming system that operates under what I called ""Controlled Entropy"".After my meeting with Professor Gill, I was invited to take a Graduate level Supernovae class and I began thinking of new ways to use concepts from the class in cybersecurity and AI developmentLater ... as I was falling asleep I began dreaming in graphs. I started putting each graph on top of each other and I realized that so many of the concepts I've learned across the years of watching YouTube videos or learning about some new theory, and suddenly everything seemed like it all lined up.This led me down a rabbit hole:[Universality](https://en.wikipedia.org/wiki/Universality)[Shannon Entropy (Information Entropy)](https://en.wikipedia.org/wiki/Entropy_(information_theory))I'm working out a way to build this into my autonomous red teaming engine - if the theory is correct, we will be able to generate a novel threat vector that crosses categories of attacks: hardware vectors + IoT + ransomeware, etc...1. Our 100% autonomous cybersecurity suite will not only be able to match current known and unknown threats,2. We can use a brand new, multi-category attack against our own system the pattern recognition would evolve infinitely..
Working on my projrct in Robotics. I'm developing a terrain classification system using only a single IMU sensor (BNO055) to identify surface types (grass, floor, cement) in real-time for autonomous mobile robots.My approach:Collecting 10 minutes of IMU data per terrain at various speeds (0.2-0.8 m/s).Creating 1-second sliding windows with 50% overlapExtracting 16 features per window:Time-domain: variance, RMS, peak-to-peak, zero-crossing rate of Z-axis accelerationFrequency-domain: FFT power in bands [0-5Hz], [5-15Hz], [15-30Hz], [30-50Hz]Statistical: kurtosis, skewnessTraining Random Forest classifier.Target: 80-85% accuracy.Key insights: Different terrains create distinct vibration signatures in frequency domain (grass: 5-15Hz peak, cement: 15-30Hz peak, floor: mostly <5Hz).Has anyone tried similar approaches with fewer features that still work well? Or is this approach works well with this type of task?.
I am trying to train a transformer model(1.5b parameters) on a TPU v3-8. The highest physical batch size I can get is 16 sequences of 2048 tokens. To increase my effective batch size, I have turned to gradient accumulation. My loop works at a smaller scale, but at a larger scale, it causes an OOM error. I'm using Torch XLA. Here is my code:Optimizer creation:```def build_optimizer(model, peak_lr, muon_peak_lr, betas, weight_decay):    param_dict = {pn: p for pn, p in model.named_parameters() if p.requires_grad}    total_params = sum(p.numel() for p in model.parameters())    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)    print(""-""*100)    print(f""Total parameters: {total_params}"")    print(""-""*100)    print(f""Trainable parameters: {trainable_params}"")    print(""-""*100)    hidden_params = [p for n, p in model.named_parameters() if p.ndim >= 2 and not (n.endswith(""wte.weight"") or n.endswith(""lm_head.weight""))]    # We only want adamw to apply weight decay to embeddings    decay = [p for n, p in model.named_parameters() if p.ndim >= 2 and isinstance(n, nn.Embedding)]    # Exclude biases(if applicable) and normalization params    no_decay = [p for pn, p in param_dict.items() if p.dim() < 2]    groups = [        {""params"": decay, ""weight_decay"": weight_decay},        {""params"": no_decay, ""weight_decay"": 0.0}    ]    adamw = syncfree.AdamW(groups, lr=peak_lr, betas=betas)    muon = SingleDeviceMuon(hidden_params, lr=muon_peak_lr, momentum=betas[1], weight_decay=weight_decay)    return adamw, muon```Before I start training I run this code, as it prevents an OOM on the first step:```for _ in range(3):    train_loss = torch.zeros((), device=device)    for k in range(gradient_accumulation_steps):        x = torch.randint(0, 100256, (1, 2048)).to(device)        xs.mark_sharding(x, mesh, (""fsdp"", None))        y = torch.randint(0, 100256, (1, 2048)).to(device)        xs.mark_sharding(y, mesh, (""fsdp"", None))        with autocast(xm.xla_device(), dtype=torch.bfloat16):            loss = model(x, y)        (loss/gradient_accumulation_steps).backward()        train_loss += loss.detach()        # xm.mark_step()    torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clipping)        xm.optimizer_step(muon, barrier=True)    xm.optimizer_step(adamw, barrier=True)    adamw.zero_grad()    muon.zero_grad()```Training loop:```model.train()train_loss = torch.zeros((), device=device)for k in range(gradient_accumulation_steps):    x, y = next(train_iter)    with autocast(xm.xla_device(), dtype=torch.bfloat16):        loss = model(x, y)    (loss / gradient_accumulation_steps).backward()    train_loss += loss.detach()    # xm.mark_step()torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clipping)xm.optimizer_step(muon, barrier=True)xm.optimizer_step(adamw, barrier=True)adamw.zero_grad()muon.zero_grad()```What can I do to fix this OOM?EDIT: The OOM occurs during the first optimizer step. It does not matter if I swap the order of the optimizer steps, the OOM always occurs on the first one..
Good day everyone, recently I've become interested in proofs of convergence for federated (and non-federated) algorithms, something like what's seen in appendix A of the¬†[FedProx paper](https://anitksahu.github.io/FedProx.pdf)¬†(one page of it attached below)I managed to go through the proof once and learn things like first order convexity condition from random blogs, but I don't think I will be able to do serious math with hackjobs like that. I need to get my math foundations up to a level where I can write one such proof intuitively.So my question is: What resources must I study to get my math foundations up to par? Convex optimization by Boyd doesn't go through convergence analysis at all and even the convex optimization books that do, none of them use expectations over the iteration to proof convergence. Thanks for your timehttps://preview.redd.it/481lxdf47lof1.png?width=793&format=png&auto=webp&s=6771d3ffe8a533155aa145b2ec691181a30968b9.
Hi everyone,I‚Äôm working on some project where we need to process footprint scans (similar to fingerprints) and later be able to match or search a new scan against a database of existing ones. The pipeline is being built on AWS (S3, Glue, Athena, SageMaker, OpenSearch).The key requirements are:Image matching / retrieval ‚Äì given a new footprint, find the closest match.Robustness ‚Äì handle rotation, scale changes, low-quality scans, or partial prints.Efficiency ‚Äì scalable to a large dataset, reasonable inference latency.I‚Äôm exploring options for the ML part and wondering what model to start with:	The end goal is to store embeddings in OpenSearch k-NN and run similarity search.Has anyone worked on a similar problem (biometrics, fingerprints, medical image matching)? Which model architecture would you recommend as a good starting point for training?Thanks in advance!.
I‚Äôm building a RAG system using research papers from the arXiv dataset. The dataset is filtered for AI-related papers (around 440k+ documents), and I want to evaluate the retrieval step.The problem is, I‚Äôm not sure how to create test cases from the dataset itself. Manually going through 440k+ papers to write queries isn‚Äôt practical.Does anyone know of good methods or resources for generating evaluation test cases automatically or any easier way from the dataset?.
I've been thinking a lot about semantic data processing recently. A lot of the attention in AI has been on agents and chatbots (e.g., Claude Code or Claude Desktop), and I think semantic data processing is not well-served by such tools (or frameworks designed for implementing such tools, like LangChain).As I was working on some concrete semantic data processing problems and writing a lot of Python code (to call LLMs in a for loop, for example, and then adding more and more code to do things like I/O concurrency and caching), I wanted to figure out how to disentangle data processing pipeline logic from LLM orchestration. Functional programming primitives (map, reduce, etc.), common in data processing systems like MapReduce/Flume/Spark, seemed like a natural fit, so I implemented semantic versions of these operators. It's been pretty effective for the data processing tasks I've been trying to do.This blog post (https://anishathalye.com/semlib/) shares some more details on the story here and elaborates what I like about this approach to semantic data processing. It also covers some of the related work in this area (like DocETL from Berkeley's EPIC Data Lab, LOTUS from Stanford and Berkeley, and Palimpzest from MIT's Data Systems Group).Like a lot of my past work, the software itself isn't all that fancy; but it might change the way you think!The software is open-source at https://github.com/anishathalye/semlib. I'm very curious to hear the community's thoughts!.
NVIDIA dropped MLPerf results for Blackwell Ultra yesterday. 5√ó throughput on DeepSeek-R1, record runs on Llama 3.1 and Whisper, plus some clever tricks like FP8 KV-cache and disaggregated serving. The raw numbers are insane.But I wonder though . If these benchmark wins actually translate into lower real-world inference costs.In practice, workloads are bursty. GPUs sit idle, batching only helps if you have steady traffic, and orchestration across models is messy. You can have the fastest chip in the world, but if 70% of the time it‚Äôs underutilized, the economics don‚Äôt look so great to me. IMO.
I have annual financial indicators for thousands of clients (businesses), their credit data, and delinquency data, and I want to use this data to create a predictive model.But what's the best way to structure the data?* Take the annual financial data and associate it with the following year's delinquency data. So, for example, data from 2024 will predict delinquency in 2025.OR* Group by client and calculate the average, maximum, and minimum of the financial data to see if this data can predict delinquency..
I've been fighting with CSVs from our high end power quality meter from a very reputable instrument company. The CSV files come out from the unit immediately unusable and at 2 million samples per second its a huge dataset, and we take lots of measurements. I made some scripts go clean it but its still a mission every time that I dread to get to the good bit. .
Hi everyone,  I‚Äôm looking for an embedding-based metric to score text generation. BertScore is great, but it‚Äôs a bit outdated. Could you suggest some modern state-of-the-art alternatives?.
Hello everyone,I know that in this community there are many experienced researchers and even reviewers for top-tier conferences. As a young researcher, I sincerely hope to learn from your perspectives and get some clarity on a few concerns I‚Äôve been struggling with.**My first question:**  Does a research paper always need to achieve *state-of-the-art (SOTA)* results‚Äîoutperforming every existing method‚Äîto be accepted at an A\* conference? I often feel that so many published papers present dazzling results, making it nearly impossible for newcomers to surpass them.**My second question, about fairness and accuracy in comparisons:**  When evaluating a new method, is it acceptable to compare primarily against the most ‚Äúrelated,‚Äù ‚Äúsimilar,‚Äù or ‚Äúsame-family‚Äù methods rather than the absolute SOTA? For example:* If I make a small modification to the Bagging procedure in Random Forest, would it be fair to compare only against other Bagging-based forests, rather than something fundamentally different like XGBoost (which is boosting-based)?* Similarly, if I improve a variant of SVM, is it reasonable to compare mainly with other margin-based or kernel methods, instead of tree-based models like Decision Trees?I understand that if my method only beats some similar baselines but does not surpass the global best-performing method, reviewers might see it as ‚Äúmeaningless‚Äù (since people naturally gravitate toward the top method). Still, I‚Äôd like to hear your thoughts: from an experienced researcher‚Äôs point of view, what is considered fair and convincing in such comparisons?Thank you very much in advance for your time and advice..
Two years ago at Paris I had a workshop paper, I purchased the workshop entrance ticket, everything is okay.This year I have done the same and now I am receiving emails saying only a full conference entrance is considered an author registration for a workshop paper. I did see the website is slightly different this year but still‚Ä¶ the code of conduct did not explain this clearly, does anyone have better insights for me?.
The ARR July cycle reviews for AACL-IJCNLP 2025 just dropped.  Feel free to share your thoughts and feelings! How did you do?.
I recently implemented the [Hierarchical Reasoning Model](https://arxiv.org/abs/2506.21734) (HRM) for educational purposes and applied it to a simple pathfinding task. You can watch the model solve boards step by step in the generated animated GIF.HRM is inspired by multi-timescale processing in the brain: a slower H module for abstract planning and a faster L module for low-level computation, both based on self-attention. HRM is an attempt to model reasoning in latent space.To understand a bit better what drives the performance I ran a small ablation study. Key findings (full results in the README):* The biggest driver of performance (both accuracy and refinement ability) is training with more segments (outer-loop refinement), not architecture.* The two-timescale H/L architecture performs about the same as a single-module trained with BPTT.* Notably, H/L still achieves good performance/refinement without full BPTT, which could mean cheaper training.Repo: [https://github.com/krychu/hrm](https://github.com/krychu/hrm)This is of course a limited study on a relatively simple task, but I thought the results might be interesting to others exploring reasoning models.The findings line up with the ARC Prize team's analysis: [https://arcprize.org/blog/hrm-analysis](https://arcprize.org/blog/hrm-analysis)Below two examples of refinement in action: early steps explore solution with rough guesses, later steps make smaller and smaller corrections until the full path emerges:[20x20 board](https://i.redd.it/i1qi4l2vs3of1.gif)[30x30 board](https://i.redd.it/j6fpueovs3of1.gif).
Curious about community experience: what‚Äôs the most painful ‚Äòstuck‚Äô moment you‚Äôve faced in an ML project (convergence, dataset issues, infra)?  How did you eventually move past it, or did you abandon the attempt? Would be great to hear real war stories beyond published papers..
I want to know which ocr has high accuracy and consumes less time for the extraction of data for given input images (especially tables), anything which works better than paddleocr?.
One of my favorite card games is called The Crew, which is a trick-taking game (like hearts) but cooperative. There's no table talk allowed, players have to coordinate silently (with limited options for in-game communication) - figuring out what their teammates are doing and why, and what they need to do to work together. I wondered what SOTA LLMs would do if you asked them to play. To make this work, I implemented a backend for the game logic and structured outputs so models play by submitting moves and reasoning at each turn. Originally I wanted to re-create the 50 mission campaign, but models were so spotty on  mission 1 (the simplest possible mission) that I stuck to mission 1 and experimented with different configurations instead. I ran 8 OpenAI models on 10 different versions, ranging from very easy (random chance gets you there 2/3rds of the time) to very hard (random chance succeeds 0.5%), and gave each model ten trials on each mission.What I've found out:\* Smaller models struggle both with gameplay, and with understanding their role on the team. In these missions, a designated player (the commander) has to win a designated card. But these models hate having to lose a trick for the sake of their teammate, even when that's how they win the game.[This does not \\""help him secure the win and fulfill his task.\\"" It loses the game.](https://preview.redd.it/3lqyqf3tg1of1.png?width=2030&format=png&auto=webp&s=b57c0a46fee169e14dbf6fc0cda107024a11a59e)\* GPT-4o-mini (worst model so far) plays randomly on easy setups and worse than randomly on harder ones. GPT-4o-mini in particular loses the game in the first turn almost 90% of the time in harder setups with GPT-5-nano and GPT-4.1-mini are close behind at 60-70%. [GREEN 1 is the lowest GREEN card in the game, so playing it straight away actually guarantees immediate failure.](https://preview.redd.it/fx5jqyhug1of1.png?width=2046&format=png&auto=webp&s=da5d4abb5a7fcd4c1e8ee42c09d7acfb4a7ba5dc)\* GPT-5 is self-aware enough to avoid the ""losing on the very first turn"" error, but actually did it on purpose once as a deliberate suicide when it saw that it couldn't win the game on the very first turn.[There are multiple turns in the game!](https://preview.redd.it/91qnnfuvg1of1.jpg?width=1900&format=pjpg&auto=webp&s=ebc98a3fbf4381c4a95f7e96ec2fa96f8e84692f)\* The harder missions - which require coordination across multiple turns - absolutely cook the smaller models with <10% win rates. Only GPT-5 is beating random chance on the harder missions (73% GPT-5 vs 4% random) \* GPT-5 also found optimal 1-trick solutions to a couple of setups I thought required at least two tricks. Oops. So in a sense, we're above human performance in some areas.\* ...But most of the time, GPT-5 generally screwed around for 3 or more tricks in puzzles it could have solved in 1. This is like solving a mate in one chess puzzle in 3 moves. It's not losing, but it's not exactly showing a mastery of the game.\* The lack of goal-oriented behavior (or risk-averse hesitation) on GPT-5's part means that GPT-5-mini actually performs better if we count speed (number of turns) to win as criteria and grade on optimal play (winning in the least number of turns, rather than just winning.)I published the repo and did a write-up with some graphs and demos here: [https://ekkarpinski.github.io/LLMCrew/](https://ekkarpinski.github.io/LLMCrew/).
I am working on a regression problem where I predict Pavement Condition Index (PCI) values from multi-sensor time-series data collected in the same region and under the same conditions. I have multiple sets of data from the same collection process, where I use some sets for training and testing and keep the remaining ones for evaluating generalization. Within the training and testing sets, the model performs well, but when I test on the held-out dataset from the same collection, the R¬≤ value often becomes negative , even though the mean absolute error and root mean square error remain reasonable. I have experimented with several feature engineering strategies, including section-based, time-based, and distance-based windowing, and I have tried using raw PCI data as well. I also tested different window lengths and overlap percentages, but the results remain inconsistent. I use the same data for a classification task, the models perform very well and generalize properly, yet for PCI regression, the generalization fails despite using the same features and data source. In some cases, removing features like latitude, longitude, or timestamps caused performance to drop significantly, which raises concerns that the model might be unintentionally relying on location and time information instead of learning meaningful patterns from sensor signals. I have also experimented with different models, including traditional machine learning and deep learning approaches, but the issue persists. I suspect the problem may be related to the variance of the target PCI values across datasets, potential data leakage caused by overlapping windows, or possibly a methodological flaw in how the evaluation is performed. I want to understand whether it is common in research to report only the R¬≤ values on the train/test splits from the same dataset, or whether researchers typically validate on entirely separate held-out sets as well. Given that classification on the same data works fine but regression fails to generalize, I am trying to figure out if this is expected behavior in PCI regression tasks or if I need to reconsider my entire evaluation strategy..
I'm excited to share something I've been working on for the past few weeks:Otters ü¶¶ - A minimal vector search library with powerful metadata filtering powered by an ergonomic Polars-like expressions API written in Rust!Why I Built ThisIn my day-to-day work, I kept hitting the same problem. I needed vector search with sophisticated metadata filtering, but existing solutions were either,Too bloated (full vector databases when I needed something minimal for analysis)Limited in filtering capabilitiesHad unintuitive APIs that I was not happy about.I wanted something minimal, fast, and with an API that feels natural - inspired by Polars, which I absolutely love.What Makes Otters DifferentExact Search: Perfect for small-to-medium datasets (up to ~10M vectors) where accuracy matters more than massive scale. Performance: SIMD-accelerated scoringZonemaps and Bloom filters for intelligent chunk pruningPolars-Inspired API: Write filters as simple expressions```meta_store.query(query_vec, Metric::Cosine)    .meta_filter(col(""price"").lt(100) & col(""category"").eq(""books""))    .vec_filter(0.8, Cmp::Gt)    .take(10)    .collect()```The library is in very early stages and there are tons of features that i want to addPython bindings, NumPy supportSerialization and persistenceParquet / Arrow integrationVector quantizationetc.I'm primarily a Python/JAX/PyTorch developer, so diving into rust programming has been an incredible learning experience.If you think this is interesting and worth your time, please give it a try.I welcome contributions and feedback !üì¶ https://crates.io/crates/otters-rsüîó https://github.com/AtharvBhat/otters.
Hey everyone,  I just completed¬†**Amazon ML Summer School 2025**¬†üéâ  It was a month-long program covering a solid range of ML topics¬†***supervised/unsupervised learning, deep neural nets, generative AI & LLMs, RL, and even causal inference***.  The sessions were intense but super rewarding. I feel like this experience gave me a strong foundation to explore advanced AI research and projects.Curious if anyone here has also attended and how you re planning to apply what you learned?https://preview.redd.it/b5ulzuq038of1.png?width=655&format=png&auto=webp&s=c328f24e6b674b9f576cebae727f44a526f185a9.
Hi everyone,I‚Äôm a cybersecurity and network engineer/sysadmin by profession, but I studied AI/ML quite seriously at university. My knowledge is solid up until around the Transformer era (when attention-based models started becoming central), but I stopped following developments after that.Now I‚Äôd like to get back into the field and stay current‚Äînot necessarily to publish research, but to understand new architectures, applications, and tools. In cybersecurity, I stay updated through curated blogs, newsletters, and professional communities. I‚Äôd like to adopt a similar approach for ML/AI.For those of you who actively track progress:* Which blogs, newsletters, or feeds do you find most useful?* Are there particular researchers or labs whose updates you follow?* Any books or surveys that bridge foundational knowledge with current trends?* How do you cut through hype-heavy content and focus on signal?I‚Äôd really appreciate hearing what works for you. The field moves incredibly fast, and I‚Äôd like to plug back in with a structured approach.Thanks in advance!.
Does anyone know whether they‚Äôre going to release the Phase 1 rejections today or on September 12?.
**\[Project\] Phishing URL detection with Random Forests on handcrafted features** I recently finished a project where I trained and deployed a phishing URL detector using **traditional ML techniques**. The goal was to explore how far a lightweight, interpretable model could go for this problem before moving to deep learning.**Data & Features*** Dataset: Combined PhishTank + Kaggle phishing URLs with Alexa top legitimate domains.* Preprocessing: Removed duplicates, balanced classes, stratified train/test split.* Features (hand-engineered):   * URL length & token counts   * Number of subdomains, ‚Äú@‚Äù usage, hyphens, digits   * Presence of IP addresses instead of domains   * Keyword-based flags (e.g., ‚Äúlogin‚Äù, ‚Äúsecure‚Äù)**Model & Training*** Algorithm: Random Forest (scikit-learn).* Training: 80/20 split, 10-fold CV for validation.* Performance: \~92% accuracy on test data.* Feature importance: URL length, IP usage, and hyphen frequency were the strongest predictors.**Takeaways*** A simple RF + handcrafted features still performs surprisingly well on phishing detection.* Interpretability (feature importances) adds practical value in a security context.* Obvious limitations: feature set is static, adversaries can adapt.**Future work (exploration planned)*** Gradient boosting (XGBoost/LightGBM) for comparison.* Transformers or CNNs on raw URL strings (to capture deeper patterns).* Automating retraining pipelines with fresh phishing feeds.**Repo:** [https://github.com/saturn-16/AI-Phishing-Detection-Web-App](https://github.com/saturn-16/AI-Phishing-Detection-Web-App)Would love feedback on:* What other URL features might improve detection?* Have people here seen significant gains moving from RF/GBM ‚Üí deep learning for this type of task?.
I am working on a project where I need to extract transaction data from Bank Statement PDFs. 80% of my working PDFs are digitally generated so to handle those I put the Regex approach, where I first extract the text into a txt file and then run Regex on this data to extract data in a meaningful format \[Date, Particulars, Credit/Debit amount, Balance\]. The challenge is that the Regex approach is brittle, and very sensitive to formats. So every bank requires a new Regex plus any little change in the format tomorrow by the bank will break the pipeline.I want to make a pipeline which is agnostic to bank-format and is capable of extracting the info from the PDFs. I cannot use any 3rd party APIs as the bank data is sensitive and we want to keep everything on internal servers.Hence, I have been exploring ways in Open Source models to built this pipeline. After doing some research, I landed on LayoutLMv3 Model which can essentially label the Tokens based on their location on the page so if we are able to train the model on our data it should be able to tag every token on the page and that should do it, but the challenge here is that this model is sensitive to reading order and fails on few bank formats.Since then I have explored MinerU but that failed as well, it isolated the transaction content table but later failed to extract data in orderly fashion as it could not differentiate between multiple lines of transactions.Now I am working with YOLOv8 which I am training to identify transaction rows and amount columns using BBox and then I will pull the info from these BBox intersection. But the confidence here is not very high.Has anyone here faced similar challenge? Can anyone help me with some solution or approach. It would be a great help!Know that the most of the PDFs don't have any defined table, it's just text hanging in air with lot of whitespace. I need a solve for Scanned PDFs as well \[integrated with OCR\].
Recently, I needed to build an ML service that would be called by a latency-sensitive client. The requirements for load and latency were higher than what I had worked with in the past, so I wasn‚Äôt sure what to expect from my Python application.I googled around and couldn‚Äôt find any concrete answers, so I wrote this brief article for anyone out there in a similar situation:https://medium.com/@javiermas/benchmarking-an-ml-service-in-pytho-4238399d2229I hope you find it useful!.
Hey!For context, I'm a Master's student at ETH Z√ºrich. A friend and I recently tried writing a paper for a NeurIPS workshop, but ran into some issues.  We had both a lot on our plate and probably used LLMs a bit too much. When evaluating our models, close to the deadline, we caught up on some bugs that made the data unreliable. We also had plenty of those bugs along the way. I feel like we shot ourselves in the foot but that's a lesson learned the way. Also, it made me realise the negative effects it could have had if those bugs had been kept uncaught.I've been interning in some big tech companies, and so I have rather high-standard for clean code. Keeping up with those standards would be unproductive at our scale, but I must say I've struggled finding a middle ground between speed of execution and code's reliability.For researchers on this sub, do you use LLMs at all when writing ML experiments? If yes, how much so? Any structure you follow for effective experimentation (writing (ugly) code is not always my favorite part)? When doing experimentation, what structure do you tend to follow w.r.t collaboration?Thank you :).
HeyAnybody read this ? It seems rather obvious and low quality, or am I missing something ? https://openai.com/index/why-language-models-hallucinate/‚ÄúAt OpenAI, we‚Äôre working hard to make AI systems more useful and reliable. Even as language models become more capable, one challenge remains stubbornly hard to fully solve: hallucinations. By this we mean instances where a model confidently generates an answer that isn‚Äôt true. Our new research paper‚Å†(opens in a new window) argues that language models hallucinate because standard training and evaluation procedures reward guessing over acknowledging uncertainty.ChatGPT also hallucinates. GPT‚Äë5 has significantly fewer hallucinations especially when reasoning‚Å†, but they still occur. Hallucinations remain a fundamental challenge for all large language models, but we are working hard to further reduce them.‚Äù.
Skip connections and residual blocks have been ubiquitous in the ML field ever since the original ResNets were published. I think it's fair to say most people agree skip connections help, but at a glance, the design of the residual blocks themselves is still something that differs from paper to paper.The most recent ""innovation"" is splitting channel mixing from spatial mixing, which is what ConvNeXt does in an attempt to mimic transformers. Other models that also claim SotA-ish performance, however, do not necessarily follow suit. NFNet, for example, employs grouped 3x3 convolution layers, good old normal bottlenecks (not inverted) and channel attention (Squeeze-and-Excitation).If we look at modern LLMs, they all have residual blocks that look very similar, but with one or two minor differences that often look arbitrary.I think residual block design is one of those things that people don't really pay much attention to since it generally works well enough regardless of what you do, but at some point it does look like we're just making semi-random decisions based on semi-random observations. Why the block is designed in the way it is is rarely a point of concern.I've tried looking for papers making direct comparisons between different design choices, but I couldn't really find anything conclusive..
One limitation I‚Äôve noticed with most AI coding assistants is that they don‚Äôt really understand a team‚Äôs domain knowledge or architectural decisions.To explore this, we built a small CLI project: Terra Code CLI. The idea was to see if an assistant could feel more like a senior developer who knows the org, rather than just autocomplete.Things we experimented with:‚Ä¢ Interactive Knowledge Transfer ‚Äì let senior devs ‚Äúteach‚Äù patterns‚Ä¢ Semantic Code Search ‚Äì context-aware retrieval across repos‚Ä¢ Persistent Memory ‚Äì standards remembered across projects‚Ä¢ Domain Expertise ‚Äì ingesting architecture docs, API specs, etc.We‚Äôre curious:üëâ Has anyone here tried giving AI assistants persistent org-specific knowledge? Did it actually help productivity, or just add complexity?For free quick start:npm install -g @terra-code/terra-codeterraFor those interested, we‚Äôve open-sourced the CLI [ https://github.com/TerraAGI/terra-code-cli ]. There‚Äôs also a simple website which we will be updating with docs + install guide here: [ https://terra-agi.com/ ]. Currently in beta, so it‚Äôs free to use..
I came across the recent ROLLING HONED paper (designing 3D shapes that, when rolling without slipping, trace arbitrary 2D paths). It got me thinking:In 3D, rolling constraints let you encode a 2D trajectory into the geometry of a 3D body.In principle, in 4D you could imagine a convex hypersurface rolling on a 3D hyperplane, tracing out a 3D trajectory.More generally: could there be a systematic way to map nD data into (n‚àí1)D dynamics via such constraints?I know in ML we already have PCA, autoencoders, product quantization, etc. ‚Äî and those actually preserve metrics we care about. My hunch is that this ‚Äúmechanical embedding‚Äù idea probably fails the usefulness test for similarity search (no guarantee of inner product preservation).But still:Does the analogy make any theoretical sense in higher dimensions (rolling manifolds w/o slip/twist)?Could there be hidden value in treating ‚Äúconstrained dynamics‚Äù as a new kind of coding scheme?Or am I over-romanticizing a neat geometric trick after too much late-night reading?Curious what the community thinks ‚Äî is there any research potential here, or should I file this under ‚Äúfun alcohol-fueled metaphors‚Äù and move on?.
Recently submitted a paper to WACV 2026. Two of the three reviews are positive. The third recommends rejection, citing items as ‚Äúmissing‚Äù that are actually in the paper (2nd page dude) and claiming our architecture is identical to a 2022 model, though there are clear differences- moreover, the performances tend to drastically differ as showcased in the results.What are the typical options in this situation? He seems to be inclined towards finding ""excuses"" for rejecting paper (not sure why) and thereby I doubt a rebuttal will help. Can I ask the AC to get the reviewer replaced?.
Baseten just raised $150M Series D at a $2.1B valuation. They focus on inference infra  like low latency serving, throughput optimization, developer experience.They‚Äôve shared benchmarks showing their embeddings inference outperforms vLLM and TEI, especially on throughput and latency. The bet is that inference infra is the pain point, not training.But this raises a bigger question. what‚Äôs the real bottleneck in inference?	‚Ä¢Baseten and others (Fireworks, Together) are competing on latency + throughput.	‚Ä¢Some argue the bigger cost sink is cold starts and low GPU utilization , serving multiple models elastically without waste is still unsolved at scale.I wonder what everyone thinks 	‚Ä¢Will latency/throughput optimizations be enough to differentiate?	‚Ä¢Or is utilization (how efficiently GPUs are used across workloads) the deeper bottleneck?	‚Ä¢Does inference infra end up commoditized like training infra, or is there still room for defensible platforms?.
Hello¬†[r/MachineLearning](https://www.reddit.com/r/MachineLearning/),I'm a final-year undergrad exploring multimodal systems, and I wanted to share a project I've built and open-sourced. It‚Äôs an end-to-end pipeline designed to tackle video dubbing for low-resource languages, using Telugu as the initial target. The system translates speech from an English video while preserving the original speaker's vocal identity and syncing their lips to the new audio.* **GitHub Repo:**¬†[\[GitHub\]](https://github.com/M-SRIKAR-VARDHAN/speech-to-speech-with-lipsync)* **Full Technical Write-up:**¬†[\[writeup\]](https://medium.com/@srikarvardhan2005/speech-to-speech-translation-with-lip-sync-425d8bb74530)* **Demo Video:**¬†[\[Demo\]](https://drive.google.com/drive/folders/1l6jZEDdmUzr9VhfYkvoVdaXJSSipN-nm?usp=sharing)The core technical challenge was achieving voice preservation without access to large, speaker-specific datasets typically required for high-fidelity voice cloning. After a dead-end attempting a direct S2S architecture inspired by Translatotron, I found that using Retrieval-based Voice Conversion (RVC) as a post-processing step on a generic TTS output was a surprisingly practical and data-efficient solution.The final pipeline is structured as follows:1. **ASR:**¬†Whisper for robust transcription.2. **NMT:**¬†Meta's NLLB for English-to-Telugu translation.3. **TTS:**¬†Meta's MMS model to synthesize the base Telugu audio.4. **Voice Conversion:**¬†A trained RVC model converts the timbre of the synthetic speech to match the original speaker.5. **Lip Sync:**¬†Wav2Lip aligns the video frames to the new audio.My main takeaway is that RVC seems to function as a very effective ""style transfer"" layer for voice, making it a viable tool for projects where full voice cloning is computationally or data-prohibitive.I'm sharing this to start a discussion and get feedback from the community on this approach. I'm particularly curious about two points:1. Has anyone else experimented with using RVC in a more formal pipeline, and what were the qualitative limitations you encountered?2. Are there newer or more robust alternatives to Wav2Lip for lip-syncing that maintain good performance without requiring massive computational resources?Any thoughts on the architecture or suggestions for improvement would be highly appreciated. Thank you for your time..
I‚Äôm building a news ingestion system (currently Poland-focused; designed to scale) that clusters incoming articles into ‚Äúevents‚Äù powering maps and graph views. Pipeline: embeddings ‚Üí cosine HAC with a fixed threshold ‚Üí periodic (5min) recluster. Granularity, time decay, and summarization are fine, my sole pain point is¬†*stable event identity*¬†in a streaming setting.As new articles arrive, clusters should sometimes merge (a legitimate bridge appears) or split (bridge was spurious). I need user-facing event IDs to persist through these transitions, i.e., minimize label churn across snapshots while respecting the hierarchical/threshold constraints.**Question:**¬†What‚Äôs the best-known algorithmic approach (and any open-source references) for¬†*evolutionary/streaming hierarchical clustering with persistent labels*, explicitly merge/split-aware, that¬†*minimizes an inter-snapshot ID-churn* *penalty*¬†under latency constraints?.
Hello sub,I'm trying to train a LoRA for Llama 3.2 90B Visual Instruct on a 8xA100 cluster but I cannot find a framework/package that supports it.Model is of course too large to fit into a single A100, so the only way is to leverage multiple device.Unsloth does not support multi GPU training (at least in its open version)  Axtol has multimodal models in betaWas any of you successful into training multimodal models of this size? I'd appreciate any kind of feedback..
Anyone attending EUSIPCO in Palermo next week? Unfortunately, none of my labmates will be able to travel, so would be cool to meet new people from here !.
Four years ago, I built [DenseClus ](https://github.com/awslabs/amazon-denseclus)for mixed-data clustering using dual UMAP embeddings. After reflecting on the Zen of Python (""simple is better than complex""), I realized I was overengineering.Gower (1971) computes distances for mixed categorical/numerical data using weighted averages of appropriate metrics. Despite being 50+ years old, it often outperforms complex embeddings for small-to-medium datasets.The implementation I coded (with Claude's help) saw a 20% speedup, 40% in memory, has GPU support (CuPy) and Sklearn integration.Code: [https://github.com/momonga-ml/gower-express](https://github.com/momonga-ml/gower-express)Blog post with analysis: [https://charles-frenzel.medium.com/i-was-wrong-start-simple-then-move-to-more-complex-5e2f40765481](https://charles-frenzel.medium.com/i-was-wrong-start-simple-then-move-to-more-complex-5e2f40765481)**Discussion**:  When do you choose simple, interpretable methods over deep embeddings? Have others found similar success reverting to classical approaches?.
[Hydra](https://hydra.cc/) has become a very popular in machine learning projects. I understand the appeal, it makes configurations modular, allows you to reuse some parts of it while changing another. It makes the code more reusable and modular too and if you understand all of it its better structured.My big problem is it makes it damn well near impossible to read someone else's code since every part of the code is now some mysterious implicit thing that gets instantiated from a string in the config file during execution. The problem would be alleviated if there was a way of quickly accessing the definition of the object that will get instantiated at runtime at least with the default values of the config. Is there a plugin that does that? If not, how do you guys do it ?.
I want to implement this paper: [https://arxiv.org/pdf/1805.04770](https://arxiv.org/pdf/1805.04770)but I'm not excited about having to manage the student models / save them independently and also there's the issue of cost because we'd have to train each student model from scratch.To get around this I was thinking I could just do the inverse: train the teacher model and derive ""dark knowledge"" based on the ""incorrect"" logits of the last checkpoint.What I mean is can I have a training loop similar to the following    for epoch in range(10):      student = teacher.clone()      student.requires_grad_(False) # the student deliberately does not learn, only the teacher learns      for data in dataset:        optim.zero_grad()        teacher_logits = teacher(data.input)        student_logits = student(data.input)        loss_cross_entropy = cross_entropy(teacher_logits, data.label)        loss_dark_knowledge = cross_entropy(teacher_logits - student_logits, data.label)        loss = (loss_cross_entropy + loss_dark_knowledge) / 2        loss.backward()        optim.step()is this dumb?.
Hello Reddit,Working on several project I had to use the DCNv2 for different models I tweak it a little bit to work under the most recent CUDA version I had on my computer. There is probably some changes to make but currently it seems to work on my models training under CUDA 12.8 + Pytorch 2.8.0 configuration still haven't tested the retrocompatibility if anyone would like to give it a try.Feel free to use it for training model like YOLACT+, FairMOT or others.[https://github.com/trinitron620/DCNv2-CUDA12.8/tree/main](https://github.com/trinitron620/DCNv2-CUDA12.8/tree/main).
Curious what folks think about this paper: [https://arxiv.org/abs/2508.08285](https://arxiv.org/abs/2508.08285)    In my own experience in hallucination-detection research, the other popular benchmarks are also low-signal, even the ones that don't suffer from the flaw highlighted in this work.Other common flaws in existing benchmarks:\- Too synthetic, when the aim is to catch real high-stakes hallucinations in production LLM use-cases.\- Full of incorrect annotations regarding whether each LLM response is correct or not, due to either low-quality human review or just relying on automated LLM-powered annotation.\- Only considering responses generated by old LLMs, which are no longer representative of the type of mistakes that modern LLMs make.    I think part of the challenge in this field is simply the overall difficulty of proper Evals.  For instance, Evals are much easier in multiple-choice / closed domains, but those aren't the settings where LLM hallucinations pose the biggest concern.
Hi AllI‚Äôm preparing to submit to arXiv in Experimentation. Since this is my first submission, I need an endorsement.The draft is ready and I can share it upon request. Thanks! .
Been collecting data on ML inference performance in trusted execution environments and thought the numbers might be useful for others dealing with similar constraints.**Context:** Fraud detection models processing ~10M daily transactions, needed hardware-level isolation for compliance reasons.After 3 months of production data, seeing 5-8% performance overhead compared to standard deployment. This is way better than the 30-40% overhead reported in older papers about SGX.The interesting technical challenge was memory management. TEE environments have strict memory limits and different allocation patterns than standard containers. Had to completely rewrite our batching logic - what worked fine with dynamic batching in regular pods caused constant OOM errors in enclaves.**Model optimization discoveries:**- ONNX runtime worked, pytorch was too memory heavy- Preprocessing became the bottleneck, not inference- Had to keep models under 8GB total memory- P95 latency went from 12ms to 13msTried multiple approaches including raw SGX implementation and phala's abstraction layer. The attestation complexity alone makes raw implementation painful.**For those working on similar problems:**Profile your entire pipeline, not just model inference. Data transformation overhead in isolated environments is real.**Technical question for the community:** How are you handling model updates in TEE environments? The attestation requirements make standard blue-green deployments complicated. Currently doing full enclave restarts but that means brief downtime.Also curious if anyone's tried running transformer models larger than 1B params in TEE. Memory constraints seem prohibitive but maybe there are tricks I'm missing?.
I‚Äôm excited to present **thoad** (short for Py**T**orch **H**igh **O**rder **A**utomatic **D**ifferentiation), a Python only library that computes arbitrary order partial derivatives directly on a PyTorch computational graph. The package has been developed within a bachelor's research project at Universidad Pontificia de Comillas - ICAI, and we are considering publishing a future academic article reviewing the mathematical details and the implementation design.At its core, thoad takes a one output, many inputs view of the graph and pushes high order derivatives back to the leaf tensors. Although a 1‚ÜíN problem can be rewritten as 1‚Üí1 by concatenating flattened inputs, as in functional approaches such as `jax.jet` or `functorch`, thoad‚Äôs graph aware formulation enables:* Working with smaller **pieced external derivatives*** An optimization based on **unifying independent dimensions** (especially batch).This delivers **asymptotically better scaling** with respect to order and batch size (respectively).Additionally, we compute derivatives with a *vectorial* approach rather than component by component, which makes our pure PyTorch implementation possible. Consequently, the implementation stays at a high level, written entirely in Python and using **PyTorch** as its only dependency. Avoiding custom C++ or CUDA has a very positive impact on the long-term maintainability of the package.The package is already available to be installed from **GitHub** or **PyPI**:* GitHub: [https://github.com/mntsx/thoad](https://github.com/mntsx/thoad)In our benchmarks, thoad **outperforms** torch.autograd for **Hessian calculations even on CPU**. See the repository *examples/benchmarks* to check the comparisons and run them in your own hardware.**thoad** is designed to align closely with PyTorch‚Äôs interface philosophy, so running the high order backward pass is practically indistinguishable from calling PyTorch‚Äôs own `backward`. When you need finer control, you can keep or reduce Schwarz symmetries, group variables to restrict mixed partials, and fetch the exact mixed derivative you need. Shapes and independence metadata are also exposed to keep interpretation straightforward.# USING THE PACKAGE**thoad** exposes two primary interfaces for computing high-order derivatives:1. `thoad.backward`: a function-based interface that closely resembles `torch.Tensor.backward`. It provides a quick way to compute high-order gradients without needing to manage an explicit controller object, but it offers only the core functionality (derivative computation and storage).2. `thoad.Controller`: a class-based interface that wraps the output tensor‚Äôs subgraph in a controller object. In addition to performing the same high-order backward pass, it gives access to advanced features such as fetching specific mixed partials, inspecting batch-dimension optimizations, overriding backward-function implementations, retaining intermediate partials, and registering custom hooks.Example of autodifferentiation execution via `thoad.backward`    import torch    import thoad    from torch.nn import functional as F        #### Normal PyTorch workflow    X = torch.rand(size=(10,15), requires_grad=True)    Y = torch.rand(size=(15,20), requires_grad=True)    Z = F.scaled_dot_product_attention(query=X, key=Y.T, value=Y.T)        #### Call thoad backward    order = 2    thoad.backward(tensor=Z, order=order)        #### Checks    ## check derivative shapes    for o in range(1, 1 + order):       assert X.hgrad[o - 1].shape == (Z.numel(), *(o * tuple(X.shape)))       assert Y.hgrad[o - 1].shape == (Z.numel(), *(o * tuple(Y.shape)))    ## check first derivatives (jacobians)    fn = lambda x, y: F.scaled_dot_product_attention(x, y.T, y.T)    J = torch.autograd.functional.jacobian(fn, (X, Y))    assert torch.allclose(J[0].flatten(), X.hgrad[0].flatten(), atol=1e-6)    assert torch.allclose(J[1].flatten(), Y.hgrad[0].flatten(), atol=1e-6)    ## check second derivatives (hessians)    fn = lambda x, y: F.scaled_dot_product_attention(x, y.T, y.T).sum()    H = torch.autograd.functional.hessian(fn, (X, Y))    assert torch.allclose(H[0][0].flatten(), X.hgrad[1].sum(0).flatten(), atol=1e-6)    assert torch.allclose(H[1][1].flatten(), Y.hgrad[1].sum(0).flatten(), atol=1e-6)Example of autodifferentiation execution via `thoad.Controller`    import torch    import thoad    from torch.nn import functional as F        #### Normal PyTorch workflow    X = torch.rand(size=(10,15), requires_grad=True)    Y = torch.rand(size=(15,20), requires_grad=True)    Z = F.scaled_dot_product_attention(query=X, key=Y.T, value=Y.T)        #### Instantiate thoad controller and call backward    order = 2    controller = thoad.Controller(tensor=Z)    controller.backward(order=order, crossings=True)        #### Fetch Partial Derivatives    ## fetch T0 and T1 2nd order derivatives    partial_XX, _ = controller.fetch_hgrad(variables=(X, X))    partial_YY, _ = controller.fetch_hgrad(variables=(Y, Y))    assert torch.allclose(partial_XX, X.hgrad[1])    assert torch.allclose(partial_YY, Y.hgrad[1])    ## fetch cross derivatives    partial_XY, _ = controller.fetch_hgrad(variables=(X, Y))    partial_YX, _ = controller.fetch_hgrad(variables=(Y, X))>NOTE. A more detailed user guide with examples and feature walkthroughs is available in the notebook: [https://github.com/mntsx/thoad/blob/master/examples/user\_guide.ipynb](https://github.com/mntsx/thoad/blob/master/examples/user_guide.ipynb).
WACV Reviews are supposed to be released by today EOD. Creating a discussion thread to discuss among ourselves, thanks!.
Hi all! Some time ago, I asked for help with a survey on ML/AI compute needs. After limited responses, I built a model that parses ML/cloud subreddits and applies BERT-based aspect sentiment analysis to cloud providers (AWS, Azure, Google Cloud, etc.). It classifies opinions by key aspects like cost, scalability, security, performance, and support.I‚Äôm happy with the initial results, but I‚Äôd love advice on making the interpretation more precise:Ensuring sentiment is directed at the provider (not another product/entity mentioned)  Better handling of comparative or mixed statements (e.g., ‚Äúfast but expensive‚Äù)  Improving robustness to negation and sarcasmIf you have expertise in aspect/target-dependent sentiment analysis or related NLP tooling, I‚Äôd really appreciate your input.Repo:¬†[https://github.com/PatrizioCugia/cloud-sentiment-analyzer](https://github.com/PatrizioCugia/cloud-sentiment-analyzer)    It would also be great if you could answer my original survey:¬†[https://survey.sogolytics.com/r/vTe8Sr](https://survey.sogolytics.com/r/vTe8Sr)Thanks!.
Over the last year, I reviewed 12 papers at top tier conferences. It's a small sample size but I noticed that roughly 3 or 4 of them were papers I would consider good enough for acceptance at a top tier conference. That is to say: (1) they contained a well-motivated and interesting idea, (2) they had reasonable experiments and ablation, and (3) they told a coherent story.That means roughly 30% of papers met my personal threshold for quality.... which is roughly the historic acceptance rate for top-tier conferences. From my perspective, as the number of active researchers has increased, the number of well executed interesting ideas has also increased. I don't think we've hit a point where there's a clearly finite set of things to investigate in the field. I would also say essentially every paper I rejected was distinctly worse than those 3 or 4 papers. Papers I rejected were typically poorly motivated -- usually an architecture hack poorly situated in the broader landscape with no real story that explains this choice. Or, the paper completely missed an existing work that already did nearly exactly what they did. What has your experience been? .
You will find the most generic AI generated reviews in ARR. Waste of time. Submit to AI conferences. ARR is dead.
Hey r/MachineLearning I had this idea and wanted to put it in a very simple and straightforward way, tried to make the paper easy to read and starter friendly! Also it shows my research partner focus on uncertainty measurement from metrology, which I think it‚Äôs not very widely addressed in ML and NLP! The motivation here came while doing exploration at the Weights & Biases Sunday cafe event in SF, where we were exploring their observability Weave Product. I think running loops and adding more complex tools that I did for the paper, should be production valuable and help in a bunch of ways, but most importantly, help with making small modelsMore useful and a kind of reasoning process of sorts. In the future it might be useful to make this loop inside the model before output layers, anybody think of any cools applications for such methods ? [Title]: Entropy-Guided Loop: Achieving Reasoning through Uncertainty-Aware Generation[Abstract]: Reasoning models often outperform smaller models but at 3--5√ó higher cost and added latency. We present entropy-guided refinement: a lightweight, test-time loop that uses token-level uncertainty to trigger a single, targeted refinement pass. We extract logprobs, compute Shannon entropy on top-k alternatives, and apply a simple OR-logic trigger over perplexity, maximum token entropy, and low-confidence-token count. Unlike approaches that use entropy only for measurement or decoding, we pass a compact uncertainty report (tokens, confidences, alternatives, context) back to the model to guide corrective edits. On representative technical queries across reasoning, mathematics, and code generation tasks, a small model with our loop approaches 95\% of a reference reasoning model's quality at approximately one-third of the cost. The method achieves selective refinement on ~31\% of responses while improving accuracy by 16 percentage points over single-pass inference. We demonstrate that this uncertainty-aware loop provides an effective middle ground between single-pass inference and expensive reasoning chains, making it practical for production deployments where both quality and cost matter.https://arxiv.org/abs/2509.00079If you don‚Äôt like it, let me know! Am open to critique and learning! .
I‚Äôm a postgraduate in AI, and I‚Äôm trying to build a better habit of reading papers consistently.I wanted to ask: what tools, apps, or workflows do you personally use to track new papers and actually read them?Curious to hear what‚Äôs worked for you in terms of discovery (finding the right papers) and sticking with the reading habit..
Hi everyone,I'm sharing a project I've developed, [`csm.rs`](https://github.com/cartesia-one/csm.rs), a high-performance inference implementation for Sesame's Conversational Speech Model (`sesame/csm-1b`). The project is written in Rust and built on the `candle` ML framework.The primary goal was to create an efficient, standalone inference engine capable of real-time, streaming text-to-speech, moving beyond typical Python-based inference scripts to achieve maximum performance..
Hi all, I submitted a paper to a NeurIPs workshop recently and it just dawned on me that I forgot to enter one of the authors in the OpenReview portal (the deadline for submission has now passed). I will reach out to the workshop but has anyone had any luck with this kind of thing?.
https://preview.redd.it/hx8od7wvfrmf1.png?width=3819&format=png&auto=webp&s=8989ff64c23e66ff7f22e4694cae88a0f192c2b5It's alive!!! The environment I'm developing is already functional and running Granturismo 3 on PS2!!! If you want to support the development, the link is this:[https://github.com/paulo101977/sdlarch-rl](https://github.com/paulo101977/sdlarch-rl).
https://github.com/vitalops/datatuneIntroducing Datatune, a Python library that enables row-wise transformations on tabular data using natural language prompts, powered by LLMs.Unlike tools that generate SQL or static scripts, Datatune is designed for per-row semantic operations on tabular data. It‚Äôs particularly useful for fuzzy logic tasks like classification, filtering, derived metrics, and text extraction - anything that‚Äôs hard to express in SQL but intuitive in plain English.### What it doesYou write prompts like:* ""Extract categories from the product description and name""* ""Keep only electronics products""* ""Add a column called ProfitMargin = (Total Profit / Revenue) * 100""Datatune interprets the prompt and applies the right operation (map, filter, or an LLM-powered agent pipeline) on your data using OpenAI, Azure, Ollama, or other LLMs via LiteLLM.### Key Features* Row-level map() and filter() operations using natural language* Agent interface for auto-generating multi-step transformations* Built-in support for Dask DataFrames (for scalability)* Works with multiple LLM backends (OpenAI, Azure, Ollama, etc.)* Compatible with LiteLLM for flexibility across providers* Auto-token batching, metadata tracking, and smart pipeline composition### Token & Cost Optimization* Datatune gives you explicit control over which columns are sent to the LLM, reducing token usage and API cost:* Use input_fields to send only relevant columns* Automatically handles batching and metadata internally* Supports setting tokens-per-minute and requests-per-minute limits* Defaults to known model limits (e.g., GPT-3.5) if not specified* This makes it possible to run LLM-based transformations over large datasets without incurring runaway costs.### Quick Example```pythonimport datatune as dtfrom datatune.llm.llm import OpenAIllm = OpenAI(model_name=""gpt-3.5-turbo"")df = dd.read_csv(""products.csv"")# Map stepmapped = dt.map(    prompt=""Extract categories from the description and name of product."",    output_fields=[""Category"", ""Subcategory""],    input_fields=[""Description"", ""Name""])(llm, df)# Filter stepfiltered = dt.filter(    prompt=""Keep only electronics products"",    input_fields=[""Name""])(llm, mapped)result = dt.finalize(filtered)```Or using the agent:```pythonagent = dt.Agent(llm)df = agent.do(""Add a column called ProfitMargin = (Total Profit / Total Revenue) * 100."", df)result = dt.finalize(df)```### Use Cases* Product classification from text fields* Filtering based on semantic conditions* Creating derived metrics using natural language* Review quality detection, support ticket triage* Anonymization (PII removal) when needed### Links* GitHub: https://github.com/vitalops/datatune* Docs: https://docs.datatune.ai* Examples: https://github.com/vitalops/datatune/tree/main/examplesWe‚Äôre actively developing the project and would appreciate any feedback, bug reports, or feature requests via Github issues...
Posting this because I wish someone had done the same when we started. Our lab needed to work with industry partners on sensitive datasets but legal restrictions meant we couldn't access the raw data.Traditional methods like differential privacy added too much noise for our research goals. Synthetic data was useless for our specific use case.What went good for us: deploying our models in trusted execution environments. Partners felt comfortable because data never left their control. We could iterate on models without seeing actual data values.Tech setup through phala network was surprisingly direct. Only difficulty was adapting our workflow since you can't just print tensors to debug anymore. Had to get creative with logging aggregate statistics.Unexpected: our industry partnerships increased 3x because companies that previously wouldn't share data are now willing to collaborate. Turns out the privacy barrier was bigger than we realized.If your research is stuck due to data access issues definitely worth exploring TEE options. Happy to share our deployment scripts if useful..
I'm trying to upload one pending AAAI review but the website is not opening. Anyone facing the same issue? I'm also curious what would happen if I miss the review submission deadline due to website downtime. .
Everyone's focused on models. Nobody discusses the plumbing that makes real-time AI conversation possible.The stack I'm testing:* STT: Whisper vs Google Speech* LLM: GPT-4, Claude, Llama* TTS: ElevenLabs vs PlayHT* Audio routing: This is where it gets messyThe audio infrastructure is the bottleneck. Tried raw WebRTC (painful), looking at managed solutions like Agora, LiveKit, Daily.Latency breakdown targets:* Audio capture: <50ms* STT: <100ms* LLM: <200ms* TTS: <100ms* Total: <500ms for natural conversationAnyone achieved consistent sub-500ms latency? What's your setup?.
I've been working on AI projects for a while now and I keep running into the same problem over and over again. Wondering if it's just me or if this is a universal developer experience.You need specific training data for your model. Not the usual stuff you find on Kaggle or other public datasets, but something more niche or specialized, for e.g. financial data from a particular sector, medical datasets, etc. I try to find quality datasets, but most of the time, they are hard to find or license, and not the quality or requirements I am looking for.So, how do you typically handle this? Do you use datasets free/open source? Do you use synthetic data? Do you use whatever might be similar, but may compromise training/fine-tuning?Im curious if there is a better way to approach this, or if struggling with data acquisition is just part of the AI development process we all have to accept. Do bigger companies have the same problems in sourcing and finding suitable data?If you can share any tips regarding these issues I encountered, or if you can share your experience, will be much appreciated!.
Please post your personal projects, startups, product placements, collaboration needs, blogs etc.Please mention the payment and pricing requirements for products and services.Please do not post link shorteners, link aggregator websites , or auto-subscribe links.\--Any abuse of trust will lead to bans.Encourage others who create new posts for questions to post here instead!Thread will stay alive until next one so keep posting after the date in the title.\--Meta: This is an experiment. If the community doesnt like this, we will cancel it. This is to encourage those in the community to promote their work by not spamming the main threads..
**TL;DR:** I propose introducing multi-year submission bans for reviewers who repeatedly fail their responsibilities. Full proposal + discussion here: [GitHub](https://github.com/IcarusWizard/ML-review-proposal-for-accountability).Hi everyone,Like many of you, I‚Äôve often felt that our review system is broken due to irresponsible reviewers. Complaints alone don‚Äôt fix the problem, so I‚Äôve written a proposal for a possible solution: **introducing a multi-year submission ban for reviewers who repeatedly fail to fulfill their responsibilities.**Recent policies at major conferences (e.g., CVPR, ICCV, NeurIPS) include desk rejections for poor reviews, but these measures don‚Äôt fully address the issue‚Äîespecially during the rebuttal phase. Reviewers can still avoid accountability once their own papers are withdrawn.In my proposal, I outline how longer-term consequences might improve reviewer accountability, along with safeguards and limitations. I‚Äôm not a policymaker, so I expect there will be issues I haven‚Äôt considered, and I‚Äôd love to hear your thoughts.üëâ Read the full proposal here: [GitHub](https://github.com/IcarusWizard/ML-review-proposal-for-accountability).  üëâ Please share whether you think this is viable, problematic, or needs rethinking.If we can spark a constructive discussion, maybe we can push toward a better review system together..
https://preview.redd.it/d2mm661vnkmf1.png?width=3126&format=png&auto=webp&s=aa83a5002ebcba917c48d158460133701a81989aThis is a site I've made that aims to do a better job of what Papers with Code did for ImageNet and Coco benchmarks.I was often frustrated that the data on Papers with Code didn't consistently differentiate backbones, downstream heads, and pretraining and training strategies when presenting data. So with heedless backbones, benchmark results are all linked to a single pretrained model (e.g. convenxt-s-IN1k), which is linked to a model (e.g. convnext-s), which is linked to a model family (e.g. convnext). In addition to that, almost all results have FLOPS and model size associated with them. Sometimes they even throughput results on different gpus (though this is pretty sparse).I'd love to hear feature requests or other feedback. Also, if there's a model family that you want added to the site, please open an issue on the project's [github](https://github.com/igm503/heedless-backbones)  [Heedless Backbones](https://heedlessbackbones.com/).
Our team has recently published two graph ML papers: one with a new realistic benchmark and the second one on graph foundation models and how they can be related to tabular foundation models.    **GraphLand benchmark**üìù Paper:  [https://arxiv.org/abs/2409.14500](https://arxiv.org/abs/2409.14500)  üíª Code:  [https://github.com/yandex-research/graphland](https://github.com/yandex-research/graphland) It is widely discussed in the community that graph machine learning suffers from the lack of realistic, meaningful, reliable, and diverse benchmarks. We agree with this and we hope that we improve this situation with our recent paper ‚ÄúGraphLand: Evaluating Graph Machine Learning Models on Diverse Industrial Data‚Äù. GraphLand is a benchmark of 14 diverse graph datasets for node property prediction (both classification and regression) from different industrial applications. The datasets cover realistic machine learning problems and come with rich numerical and categorical node features that are common in real-world applications. Importantly, besides standard random splits, GraphLand provides splits with temporal distributional shifts and the inductive prediction setting, which enable evaluating GNNs in more realistic and challenging scenarios.[GraphLand benchmark datasets.](https://preview.redd.it/nkl4qs9nnjmf1.png?width=2224&format=png&auto=webp&s=1819461078e34be3e98030c9e65ee61a7b98adc9)We evaluated a wide range of models on GraphLand. This includes several openly available graph foundation models (GFMs), which we found provide very weak performance compared to classical GNNs.     Thus, we set out to develop a better GFM, which led us to the next paper...**Turning Tabular Foundation Models into Graph Foundation Models**üìù Paper: [https://arxiv.org/abs/2508.20906](https://arxiv.org/abs/2508.20906)  üíª Code: [https://github.com/yandex-research/G2T-FM](https://github.com/yandex-research/G2T-FM)Graphs may come from very different domains and thus may have diverse features varying across datasets. As a result, one of the key challenges for GFMs is how to deal with such diverse heterogeneous features. Prior studies did not fully address this issue, often limiting themselves to text-attributed graphs or relying on simple techniques like PCA and SVD. However, this challenge is not unique to the graph domain. The tabular domain faces exactly the same issue, and recent tabular foundation models like TabPFNv2 successfully deal with it. We‚Äôve decided to transfer their success to graphs.[G2T-FM Framework](https://preview.redd.it/xnfsjf77ojmf1.jpg?width=1280&format=pjpg&auto=webp&s=d840e9794068202829dec2bdfa71e426198a7a15)In our framework ‚Äì G2T-FM (Graph-to-Table Foundation Model) ‚Äì we augment the original features with graph information by computing neighborhood feature aggregations and some structure-based encodings, essentially transforming graph tasks to tabular tasks (G2T). After that, we apply TabPFNv2 to these augmented features to get predictions.[G2T-FM Results](https://preview.redd.it/z3mz5tmaojmf1.jpg?width=1280&format=pjpg&auto=webp&s=6feb591cdd5fb1231d36c2a937ced802a27a26e7)We evaluated G2T-FM on GraphLand and several other graph datasets and found that it shows strong performance in both in-context learning and finetuning settings. In particular, G2T-FM outperforms both well-tuned classic GNNs trained from scratch and prior publicly available GFMs.     We hope our work will help develop better GFMs and highlight for the graph community the similarities of graph and tabular domains and the prospects of utilizing tabular foundation models for graph tasks!.
Is this normal for generated data from latent diffusion? The large spikes at the end of the histogram edges. Does this indicate the autoencoder is overfitting?https://preview.redd.it/i1gtm7h3xkmf1.png?width=536&format=png&auto=webp&s=1589ad23cffc3a678eefad82750b71eefbad9962.
Title,I was reading upon diffusion models and speech models and that some of the new diffusion text models are being now developed. Since we know the length of the output that a chunk of audio produces wouldn't it be possible to create a diffusion model to fill in text for the whole length all at once instead of the current auto regressive models?PS: I am really not that advanced so this might be a dumb question..
Looks like Huawei is putting out a 96GB GPU for under $2k. NVIDIA‚Äôs cards with similar memory are usually $10k+. From what I‚Äôve read, this one is aimed mainly at inference.Do you think this could actually lower costs in practice, or will the real hurdle be software/driver support? .
Hi everyone,II‚Äôm considering submitting to the AAAI Student Abstract and Poster Program (AAAI-26), but I can‚Äôt find much information about how competitive it is compared to the main technical track.I know the main conference has a pretty low acceptance rate but AAAI doesn‚Äôt seem to share stats for the student program. Has anyone here submitted to or been accepted into this track before? How selective is it?Also, would it be enough if my work is more of an application of existing AI methods to radar (less novelty in the method itself, more novelty in the application)? Or are they mainly looking for new algorithms/AI contributions even in the student track?.
Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!Thread will stay alive until next one so keep posting after the date in the title.Thanks to everyone for answering questions in the previous thread!.
Hi guys!My name is Jason I am an Electrical and Computer Engineering student and for the last year I have been working on my thesis, in which I have developed Beaver¬†‚Äì a domain-specific language (DSL) designed to make building machine learning pipelines for streaming data (e.g., Kafka) much simpler and more accessible.What is Beaver?* A DSL that lets you define ML pipelines using a clear, declarative syntax (instead of complex Python code)* Generates Python code that integrates with the¬†[River](http://riverml.xyz/latest/)¬†library for online ML and supports real-time data streams* Includes built-in validation, analysis, and automatic dashboard generation  I'm making this post to ask for some feedback. I‚Äôve prepared a user testing experience with 3 tasks (from basic to advanced) that should take about 30-45 minutes. I‚Äôd love to hear your thoughts on usability, clarity, and the overall concept.* üìñ¬†[Concept overview & docs](http://deepblue597.github.io/beaver-doc/)* üìù¬†[User testing instructions](https://github.com/deepblue597/beaver/blob/user_testing/user_testing.md)* ü¶´¬†[Example pipeline file](https://github.com/deepblue597/beaver/blob/user_testing/examples/linear.bvr)* üí¨¬†[Feedback form](https://forms.gle/ioLVyvruJ2KCs6wd8)Repo : [https://github.com/deepblue597/beaver](https://github.com/deepblue597/beaver)  It is recommended to use the user\_testing branch for the feedback.     Thank you so much for your time <3 .
Hey folks,My paper just got into EMNLP 2025  and I‚Äôm trying to sort out two things before the camera-ready:1. Page limits- ARR submission was capped at 8 pages (long paper). The acceptance email says we get +1 page for camera-ready, so I‚Äôm assuming that means 9 pages for the main text.- Is the Limitations section required but outside this 9-page count?- And are appendices unlimited, or do they somehow count toward the limit?2. Virtual poster presentation- On OpenReview I‚Äôve already been assigned poster status. The email also says we can choose to present either in person or virtually.Does that mean I‚Äôm free to do my poster virtually if I want?- For those who‚Äôve done virtual posters at EMNLP/ACL in recent years: what platform did they use (GatherTown, Zoom, something else), and how was the interaction?Would love to hear from anyone who‚Äôs navigated this before.
So I have been working on Continuous Sign Language Recognition (CSLR) for a while. Tried ViViT-Tf, it didn't seem to work. Also, went crazy with it in wrong direction and made an over complicated model but later simplified it to a simple encoder decoder, which didn't work.Then I also tried several other simple encoder-decoder. Tried ViT-Tf, it didn't seem to work. Then tried ViT-LSTM, finally got some results (38.78% word error rate). Then I also tried X3D-LSTM, got 42.52% word error rate. Now I am kinda confused what to do next. I could not think of anything and just decided to make a model similar to SlowFastSign using X3D and LSTM. But I want to know how do people approach a problem and iterate their model to improve model accuracy. I guess there must be a way of analysing things and take decision based on that. I don't want to just blindly throw a bunch of darts and hope for the best. .
I was training a GPT-2 XL-sized LLM, and I had to stop the run. When I try to resume the run on the same hardware, I get an OOM. I had a similar issue when my model had about 930m parameters, but I solved it by moving all tensors in the model/optimizer state dicts to CPU before saving. When I run this code:optimizer.state = collections.defaultdict(dict)the OOM goes away. The OOM always happens during the optimizer step. I use xm.optimizer_step with the barrier enabled. I have also tried manually sharding the optimizer states using xs.mark_sharding. Here are some details about my project/setup:TPU v3-8Torch 2.7.0jax 0.6.2I use FSDP with SPMDHere is some relevant code from my codebase:Saving:```def save_checkpoint(model, optimizer, step, train_device_loader=None):    # Save model weights via XLA SPMD checkpoint (supported)    os.makedirs(f""./ckpt-{step}"", exist_ok=True)    model_state_dict = model.module.state_dict()    for i in model_state_dict.keys():        xla_tensor = model_state_dict[i]        model_state_dict[i] = xla_tensor.to(""cpu"")        del xla_tensor    model_sd = {""model"": model_state_dict}    xm.save(model_sd, f""./ckpt-{step}/model.pt"")    # Save host-only states separately (optimizer, step, RNG, dataloader)    optim_state = optimizer.state_dict()    optim_state_for_saving = {        ""state"": {},        ""param_groups"": optimizer.state_dict()[""param_groups""]    }    for i in optim_state[""state""]:        optim_state_for_saving[""state""][i] = {}        optim_state_for_saving[""state""][i][""step""] = optim_state[""state""][i][""step""].to(""cpu"")        optim_state_for_saving[""state""][i][""exp_avg""] = optim_state[""state""][i][""exp_avg""].to(""cpu"")        optim_state_for_saving[""state""][i][""exp_avg_sq""] = optim_state[""state""][i][""exp_avg_sq""].to(""cpu"")    host_state = {        ""optim"": optim_state_for_saving,        ""step"": step,    }    if train_device_loader:        rng_states = {            'torch_rng_state': torch.get_rng_state(),            'numpy_rng_state': np.random.get_state(),            'random_rng_state': random.getstate(),        }        dataloader_states = {            ""shard_order"": train_device_loader._loader.dataset.shards,            ""local_order"": train_device_loader._loader.dataset.curr_order,            ""warmup_order"": train_device_loader._loader.dataset.warmup_order,            ""warmup_prob"": train_device_loader._loader.dataset.warmup_prob,        }    else:        rng_states = None        dataloader_states = None    # Write host-side files    with open(f""./ckpt-{step}/host_state.pkl"", ""wb"") as f:        pickle.dump(host_state, f)    if rng_states is not None:        with open(f""./ckpt-{step}/rng.pkl"", ""wb"") as f:            pickle.dump(rng_states, f)    if dataloader_states is not None:        with open(f""./ckpt-{step}/dataloader.json"", ""w"") as json_file:            json.dump(dataloader_states, json_file, indent=4)```Loading:```if resume_from != """":        model_sd = torch.load(f""{resume_from}/model.pt"", map_location='cpu')        model.load_state_dict(model_sd[""model""])model = model.to(device)if gradient_checkpointing:        model = FSDPv2(module=checkpoint_module(model), mesh=mesh)else:        model = FSDPv2(module=model, mesh=mesh)optimizer = build_optimizer(model, peak_lr, betas, weight_decay)scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=steps*(1-warmup_pct), eta_min=min_lr)if resume_from != """":        xm.mark_step()        # 2) Restore host-only states (optimizer, step)        with open(f""{resume_from}/host_state.pkl"", 'rb') as f:            host_state = pickle.load(f)        optim_state = host_state[""optim""]                # Load the processed state dict        optimizer.load_state_dict(optim_state)        del optim_state        last_step = host_state[""step""]        # 3) Restore RNG and dataloader state (if present)        try:            with open(f""{resume_from}/rng.pkl"", ""rb"") as f:                rng = pickle.load(f)            torch.set_rng_state(rng['torch_rng_state'])            np.random.set_state(rng['numpy_rng_state'])            random.setstate([rng['random_rng_state'][0], tuple(rng['random_rng_state'][1]), rng['random_rng_state'][2]])        except FileNotFoundError:            pass        with open(f'{resume_from}/dataloader.json', 'r') as file:            dataloader = json.load(file)```Step:```for k in range(gradient_accumulation_steps):    x, y = next(train_iter)     with autocast(xm.xla_device(), dtype=torch.bfloat16):          loss = model(x, y)    (loss / gradient_accumulation_steps).backward()     train_loss += loss.detach()     xm.mark_step()                torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clipping)                xm.optimizer_step(optimizer, barrier=True)                optimizer.zero_grad()```.
Hello everyone,  I‚Äôm serving as a first-time reviewer for AAAI and am getting ready to submit my reviews. I‚Äôm a bit uncertain about the expected structure for the different fields in the review form. For instance, in the *‚ÄúBrief summary of your review‚Äù* field, should this be a recap of the paper‚Äôs content or a short explanation of my evaluation and decision? More broadly, I‚Äôd be grateful for any guidance on how to approach the overall submission..
Hi all,I wrote a post on some lessons from building an AI data analyst: [https://pedronasc.com/articles/lessons-building-ai-data-analyst](https://pedronasc.com/articles/lessons-building-ai-data-analyst)The gap from a nice demo to a real production system is big -> with a lot of yet to be solved challenges.Would love to share ideas with other builders in the space and willing to learn more about it..
We developed a simple metric to measure semantic novelty in **collaborative text generation** by computing cosine distances between consecutive sentence embeddings. Key finding: Human contributions showed consistently higher semantic novelty than AI across multiple embedding models (RoBERTa, DistilBERT, MPNet, MiniLM) in our human-AI storytelling dataset. The approach is straightforward - just encode sentences and measure distances between consecutive pairs. Could be useful for evaluating dialogue systems, story generation models, or any sequential text generation task.Some links:  [Paper site](https://idanvidra.github.io/playing_along_paper_site/)    [Code](https://github.com/idanvidra/Yes-And-Game-Paper)[Blog post with implementation details](https://medium.com/@idan.vidra/measuring-semantic-novelty-in-ai-generated-text-a-simple-embedding-based-approach-c92042c88338)The work emerged from studying human-AI collaborative storytelling using improvisational theater techniques (""Yes! and..."" games)..
Hi all,  been in the Machine Learning world till 2021, I still mostly used the old TF 1.x interface and just used TF2.x for a short time. Last work I did was with CUDA 9.  It seems like quite a bit shifted with Tensorflow, I looked at the architecture again to see how much changed. To me, it's incomprehensible. Has Google shifted all efforts towards JAX, a framework with fewer layers than TF?.
What are our options as a discipline? We are now at a point where 3 or more reviewers can like your paper, the ACs can accept it, and it will be rejected for no reason other than venue constraints. .
A Technical Writer's ambition to prove.Being a Technical Writer, I yearned to learn Machine learning and prove myself. This is a try towards achieving that.  I've developed a new classifier, the¬†**Geometric Mixture Classifier (GMC)**, and I'm seeking feedback from the community before submitting it to arXiv and conferences.**The Problem:**¬†Linear models (LR, SVM) are interpretable but fail on multi-modal data. Non-linear models (RBF-SVM, MLPs) are effective but often operate as black boxes. We wanted a model that is¬†**both interpretable and expressive**.**The Idea:**¬†GMC represents each class as a¬†**mixture of hyperplanes**¬†(a ""soft union of half-spaces""). It uses a soft-OR (log-sum-exp) within a class and softmax across classes. It's like a Mixture of Experts but without a separate gating network.* **Interpretable:**¬†You can see which ""local expert"" (hyperplane) was responsible for a prediction.* **Performant:**¬†Competitive with RBF-SVM, RF, and MLPs on standard benchmarks.* **Efficient:**¬†CPU-friendly, ¬µs-scale inference (faster than RBF-SVM, on par with MLP).* **Calibrated:**¬†Produces reliable probabilities.[Algorithm analogy with similar baselines](https://preview.redd.it/64vyu4u87dmf1.png?width=1385&format=png&auto=webp&s=08b2014b60836edd0b28adbac68eb388a4a091fa)* **Accuracy:**¬†Outperforms linear models, competitive with strong non-linear baselines.* **Speed:**¬†\~2-40¬µs inference time per example (see table below).* **Calibration:**¬†Low ECE, further improved with temperature scaling.We would be incredibly grateful for any feedback on:* Is the¬†**core idea**¬†and its¬†**differentiation from MoE/Maxout**¬†clear?* Are the¬†**experiments**¬†and¬†**comparisons**¬†fair and convincing?* Is there any¬†**related work**¬†we might have overlooked?* Any general feedback on¬†**clarity**¬†or¬†**presentation**?You can find a detailed copy of the algorithm [here](https://drive.google.com/file/d/1vRTAucCpVqImJnojVwzAUHQ2SmbAvdsi/view?usp=sharing).Please feel free to test the algorithm: [Geometric Mixture Classifie](https://github.com/Abitsfhuusrtyt/-Geometric-Mixture-Classifier-GMC---A-Discriminative-Per-Class-Mixture-of-Hyperplanes)r.
Hey everyone,I‚Äôm working on a recommender system based on a GCN model for regression task ( predicting rating score). Normally, the model initializes user and item embeddings randomly, but I wanted to improve this by following a paper ( the diagram is presented above )  that integrates semantic item profiles as initial embeddings.Here‚Äôs what I did:	‚Ä¢	I generated structured item profiles with 3 parts using Gemini api : 	‚Ä¢	[Summarization]: short description of the business.	‚Ä¢	[User Preferences]: predicted/extracted types of users who‚Äôd like it.	‚Ä¢	[Recommendation Reasoning]: explanation for why it fits.	‚Ä¢	I also encoded metadata like review count and stars into natural language (e.g., review_count > 100 ‚Üí ""popular item"", avg_stars ~4.2 ‚Üí ""well-rated"").	‚Ä¢	I used Gemini text embeddings to encode these profiles into fixed-size embeddings.	‚Ä¢	Then I replaced the random item embeddings in my GCN with these semantic embeddings (after projecting them down to my model‚Äôs embedding size).The issue:	‚Ä¢	When I train the GCN with these semantic embeddings, performance actually gets worse compared to just using random initialization or identical. Could the item profiles themselves be ‚Äúbad‚Äù ?.
I‚Äôm working on a deep learning project where I have a dataset with n classesBut here‚Äôs my problem:üëâ What if a totally new class comes in which doesn‚Äôt belong to any of the trained classes? I've heard of a few ideas but would like to know many approaches:* analyzing the embedding space: Maybe by measuring the distance of a new input's embedding to the known class 'clusters' in that space? If it's too far from all of them, it's an outlier.* Apply Clustering in Embedding Space.everything works based on embedding space...are there any other approaches?.
**For Job Postings** please use this template>Hiring: \[Location\], Salary:\[\], \[Remote | Relocation\], \[Full Time | Contract | Part Time\]    and \[Brief overview, what you're looking for\]**For Those looking for jobs** please use this template>Want to be Hired: \[Location\], Salary Expectation:\[\], \[Remote | Relocation\], \[Full Time | Contract | Part Time\]  Resume: \[Link to resume\] and \[Brief overview, what you're looking for\]&#x200B;Please remember that this community is geared towards those with experience..
Hi everyone! Today I'm announcing a new experimental open-source model finetuned from Qwen3-¬†**Art-0-8B is the first reasoning model where users can explicitly control how the model thinks through prompts.**Unlike normal reasoning models that only let you control the final output, Art-0-8B lets you control the actual thinking process. Tell it to ""think in rap lyrics"" or ""use bullet points to organize thoughts"" and it will literally reason that way before giving you an answer.You can check out the model on HuggingFace:¬†[https://huggingface.co/AGI-0/Art-0-8B](https://huggingface.co/AGI-0/Art-0-8B)¬†(please leave a like in the repo if you like this model)Let me know your thoughts!P.s. If you are an AI researcher working solo, consider joining us, we are a decentralized research lab, you can read about our mission in this section of the model card¬†[https://huggingface.co/AGI-0/Art-0-8B#%F0%9F%94%97-join-the-agi-0-decentralized-research-lab](https://huggingface.co/AGI-0/Art-0-8B#%F0%9F%94%97-join-the-agi-0-decentralized-research-lab).
**Just gave a 1.5-hour talk on ""Advanced NLP with Transformers"" covering:*** Transformer architecture* Prompting, RAG and fine-tuning techniques* AI safety, security and governance challenges* Curated papers, fellowships and resources**Resources:** üé• Recording: [https://www.youtube.com/watch?v=9WVtUDDcAXw&t=2330s](https://www.youtube.com/watch?v=9WVtUDDcAXw&t=2330s) üíª GitHub: [https://github.com/vgcharan/Advanced-NLP-Workshop-2025](https://github.com/vgcharan/Advanced-NLP-Workshop-2025)Designed for researchers, students and practitioners who want conceptual depth as well as practical references. Feedback and discussion are welcome!.
so basically my batch size is 32  d\_model is 128  d\_ff is 256  enc\_in = 5  seq\_len = 128 and pred\_len is 10I narrow downed the bottle neck and found that my FFT step is taking too much time. i can‚Äôt use autocast to make f32 ‚Üí bf16 (assume that its not currently supported).**but frankly its taking too much time to train. and that too total steps per epoch is 700 - 902 and there are 100 epoch‚Äôs.**  roughly the FFT is taking 1.5 secs per iteration below. so    for i in range(1,4):         calculate FFT()        can someone help me?.
Hey all üëãI just published this is end-to-end walkthrough of fine-tuning YOLOX on a \~7k-image license-plate dataset: clean environment setup, dataset prep, training & evaluation with COCO metrics (mAP/AP50-95), ONNX export, and real-world dashcam inference. Includes notes on dependency pinning (YOLOX‚Äôs older stack), small script fixes, and a side-by-side comparison with an Ultralytics YOLO11 model trained on the same data. Results are on par once everything is configured correctly.Here's the post where you find the code and commands: [https://www.poeticoding.com/building-a-yolox-plate-detector-setup-fine-tuning-metrics-dashcam-inference/](https://www.poeticoding.com/building-a-yolox-plate-detector-setup-fine-tuning-metrics-dashcam-inference/)YOLOX github repo: [https://github.com/Megvii-BaseDetection/YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)Roboflow car plates dataset: [https://universe.roboflow.com/roboflow-universe-projects/license-plate-recognition-rxg4e](https://universe.roboflow.com/roboflow-universe-projects/license-plate-recognition-rxg4e).
Hi all,I‚Äôm currently interviewing at a few labs for MLE positions and there‚Äôs two interviews in particular that have stumped me that I‚Äôd like some clarity on:1. Transformer debugging - to my knowledge, the interviewer will provide a buggy implementation of things like causal attention, self-attention, incorrect layer norm, scaling issues, and broadcast/shape mismatch. Is there anything else I‚Äôd need to master here? So far, I‚Äôve only been studying GPT style transformers, should I add BERT to the mix or nah?2. Training classifier & data analysis. The recruiter said this is around evaluation and model performance. I‚Äôm guessing they‚Äôll throw me an unbalanced dataset and ask me to improve model performance somehow. Things to study here are: 1) chip hguyns book and 2) look at regularization, pandas/sklearn normalization and data clean up methods. How else can I master this topic? Any sample questions you have seen here before?Lastly, what is your go-to source for practicing MLE related topics, both in terms of knowledge-base as well as real interview questions. I tried 1point3acres but very limited when it comes to ML..
Hey folks,I‚Äôm working on a project involving real-time anomaly detection using IMU data from a mobile robot (acc_x, acc_y, acc_z, magnitude). The goal is to detect small disturbances (e.g., bumping into wires or obstacles) based on sensor changes.I trained an Isolation Forest model on normal motion data and integrated it into a ROS 2 node using the .decision_function() threshold for runtime detection.It works, but I‚Äôm worried about false positives, especially with fixed contamination. Since this will later run on embedded IMU hardware, I‚Äôm looking for something accurate and lightweight.Is Isolation Forest reliable for this?Any better algorithms you‚Äôd recommend (e.g., LOF, One-Class SVM, AE)? Would love to hear your thoughts or experience.Thanks!.
I‚Äôve been experimenting with different approaches for giving AI agents the ability to use browsers in real workflows (data collection, QA automation, multi-step workflows). The promise is huge but the reliability problems are just as big:1. Sessions break after login or CAPTCHA2. Agents fail when sites change structure3. Security is hard to guarantee at scale4. Each framework has its own dialect / quirksRecently I‚Äôve been looking into managed environments that abstract some of this away. For example, I am using hyperbrowser right now and it does provide a unified layer for running browser-based agents without setting up everything manually. But then my question is... Is there ongoing research or promising directions in making browser-agent interactions more robust? Are there known benchmarks, best practices, or papers that deal with these reliability issues?.
Hi!I‚Äôm using Optuna with AutoSampler to optimize a model, but the search space is huge‚Äîaround 2 million combinations.Has anyone worked with something similar?I‚Äôm interested in learning which techniques have worked for reducing the search space..
I'm experimenting with `""ollama/gpt-oss:20b""`'s capability to generate structured outputs. For example, I used it to evaluate against GSM8K dataset. The schema is as follows: `answer`: for the answer, and `solution`: for the CoT solution. However, it doesn't make sense that for a 20B model, it cannot generate a valid structured output.Any thoughts or hacks on this one? I would appreciate it. Thanks..
We‚Äôve been experimenting with deploying a mix of foundation models (LLaMA, Mistral, Stable Diffusion variants, etc.) in a single platform. One of the recurring pain points is **inference optimization** at scale:* **Batching tradeoffs**: Batching reduces cost but can kill latency for interactive use cases.* **Quantization quirks**: Different levels (INT8, FP16) affect models inconsistently. Some speed up 4√ó, others break outputs.* **GPU vs. CPU balance**: Some workloads run shockingly well on optimized CPU kernels ‚Äî but only for certain model families.Curious how others have approached this.* What‚Äôs your go-to strategy for **latency vs throughput tradeoffs**?* Are you using **model distillation** or sticking to quantization?* Any underrated **libraries or frameworks** for managing multi-model inference efficiently?.
I manage a slack community of a couple hundred ML devs in Canada. I got curious and ran some numbers on our members to see if any interesting insights emerged. Here's what I found:**The ""Pandemic ML Boom"" Effect**:  Nearly 40% of members started an ML specific role between 2020-2022. **RAG and Vector Database Expertise**:  Over 30% of members have hands-on experience with Retrieval-Augmented Generation systems and vector databases (Pinecone, Weaviate, ChromaDB), representing one of the hottest areas in enterprise AI.‚Äç**Multi-modal AI Pioneers**:  A significant portion of members work across modalities (vision + text, audio + text).**Most Common Job¬†Titles**:15% of members hold senior leadership roles (Principal, Staff, Director, CTO level), demonstrating strong senior representation within the community.**ML-Engineering Bridge Roles**:Over 35% of members hold hybrid titles that combine ML with other disciplines:¬†""MLOps Engineer,"" ""Software Engineer, ML,"" ""AI & Automation Engineer,"" ""Conversational AI Architect,"" and ""Technical Lead, NLP"".You can see the full breakdown here: [https://revela.io/the-collective](https://revela.io/the-collective).
We're evaluating different approaches for vision-based defect detection where getting large labeled datasets is challenging. Lots of methods need thousands of examples, but some defects are rare (maybe 10-20 examples total in 6 months). Anyone working with similar constraints? I've been looking into platforms that can work with smaller datasets - curious what others are doing?.
[Project](https://www.reddit.com/r/MachineLearning/?f=flair_name%3A%22Project%22)OSS Released¬†**MAPLE ‚Äì a Multi Agent Protocol Language Engine**¬†designed for fast, secure, and reliable agent communication.‚Äî a new¬†**open-source protocol**¬†designed for¬†**multi-agent communication**¬†at¬†**production scale**.**MAPLE**¬†offers features we haven't seen in other protocols:üîß¬†**Integrated Resource Management:**¬†The¬†**ONLY**¬†protocol with built-in resource specification, negotiation, and optimizationüõ°Ô∏è¬†**Link Identification Mechanism (LIM):**¬†Revolutionary security through verified communication channels‚ö°¬†**Result<T,E> Type System:**¬†ELIMINATES all silent failures and communication errorsüåê¬†**Distributed State Synchronization:**¬†Sophisticated¬†**state management**¬†across agent networksüè≠¬†**Production-Grade Performance:**¬†**Very high**¬†**performance**¬†for a¬†**feature-rich**¬†protocol with¬†**sub-millisecond**¬†latencyüíª¬†**pip install maple-oss**PyPI here:¬†[https://pypi.org/project/maple-oss/](https://pypi.org/project/maple-oss/)If you‚Äôre building with agents or need robust, real-world communication between systems,  check out¬†**MAPLE GitHub**¬†repo:¬†[https://github.com/maheshvaikri-code/maple-oss](https://github.com/maheshvaikri-code/maple-oss)Please try and test it with your projects.[MAPLE Multi Agent Communication Protocol](https://preview.redd.it/bovnmzoqc0mf1.png?width=256&format=png&auto=webp&s=6563e85b9f830d36a244cbf9783bc05ba45ed8c3).
Hey, Looking to see how DinoV3 will do on my dataset post finetuning. Any practical advice on finetuning Dino? Scheduler, optimizer, flow - freezing, discriminative lr etc. Any recommandations for blogs or articals related to this? .
Hello everyone. I'm working on a training environment based on stable-retro and a Retroarch frontend, Sdlarch. This environment is intended to support PS2, GameCube, Dreamcast, and other video games that aren't supported by the original Stable-retro/Gym-Retro. If anyone wants to support me, or is curious, the link is below:[https://github.com/paulo101977/sdlarch-rl](https://github.com/paulo101977/sdlarch-rl)There's still a lot of work ahead, as I'm implementing the final phase that enables PS2 training: loading states. For some reason I don't yet fully understand, the save state isn't loading (it just saves). But it's now possible to run games in the environment via Python, without the need to intercept any external processes..
I'm doing a full fine-tune on the Qwen 3 14B Base model with around 10B tokens for loss. I'd have preferred a little higher capacity. My idea is to add a few more layers at the end, initialized close to zero, and then train. Perhaps increase from 40 to 50 layers.This is straightforward to implement. Is there a reason why I don't hear of this being done? Is anyone familiar with this? Any research indicating success or failure? It makes sense conceptually but I would assume it would be more common if it works.(I asked the GPT5, Gemini Pro & Claude, but I'm getting mixed answers. It'll agree or disagree depending how I phrase the question.).
And 20K out of 29K submissions are from China (clearly dominating AI research now, well done to my Chinese friends). The review process at AI conferences isn't just broken - it's nuked. We need change, fast.https://preview.redd.it/ih3vliracnlf1.png?width=1938&format=png&auto=webp&s=b7112a3e5e78ec7bcd0e6b100b5887a880fb82be.
I made a C++ implementation of PaddleOCRv5 that might be helpful to some people: https://github.com/Avafly/PaddleOCR-ncnn-CPPThe official Paddle C++ runtime has a lot of dependencies and is very complex to deploy. To keep things simple I use [ncnn](https://github.com/Tencent/ncnn) for inference, it's much lighter (and faster in my task), makes deployment easy. The code runs inference on the CPU, if you want GPU acceleration, most frameworks like ncnn let you enable it with just a few lines of code.Hope this helps, and feedback welcome!.
Hey everyone,Don't know if it fully matches this subreddit, but since there have been a lot of discussions around LLMs using a lot of power and water, and even more discussions around LLMs plateauing, as everyone focuses on making the biggest and most powerful model.I've been super focused for a while now in bringing Language Models and complex NLP capabilities to microcontrollers and finally been able to finish the architecture and an ML Toolkit that enables training models from scratch, with this architecture and enables easy deployment on almost any MCUs.The architecture uses state of the art methods, with many in-depth optimisations tested through over 1700 trained models, to get the most of every single memory byte and clock cycle, specifically for MCUs while also enabling extremely fast responses on PC.The idea is to have domain specific and task specific models, using Sparrow's architecture, instead of a general prupose frontier model like ChatGPT/Llama etc. In the demo I showcase a Biology only model, that was made to give straight answrs (as per research papers showcasing that's what people want) for a question-answering chat-like system. Anything can be created. And then due to the model being only 50-200KB depending on how it is build (with twice that needed in total when flashed), mutiple models could be loaded in memory and a mixture-of-experts system can be designed. Which is what I want to explore with SPARROW 2.I still have to see exactly how to proceed in terms of making the code open-source, best licensing methods, how to create the API, etc. But the idea is that it would be easy to create language models for MCUs, similar to how Sci-kit Learn is used for regular ML.It supports encoder, decoder, encoder-decoder models, and the fastest model uses linear attention, but I have also been able to deploy dot attention and additive attention on the ESP32.Let me know what you think!¬†[Here's a demo video](https://youtu.be/WCvv5W9gEiA?si=QCXvXei3qfp0qAG8)¬†with a ChatGPT simple-webapp to give people something they are familiar with. I'd also like to know opinions around the best way to go forward, release it as a website of sorts, release it as an API like Scikit Learn etc.I have a lot of videos with the models running on PC with full phrases/paragraphs outputs in less than 10 miliseconds, have different versions Small, Main, Large running on the ESP32S3, have the Main flavour running on the ESP32P4 which can process everything 5-6 times faster due to the intrustions available, and outputting a phrase every 50-100ms, compared to ESP32S3's 300-600ms..
I came across Gemini‚Äôs text embeddings model, and their documentation mentions that semantic similarity is suitable for recommendation tasks. They even provide this example:	‚Ä¢	‚ÄúWhat is the meaning of life?‚Äù vs ‚ÄúWhat is the purpose of existence?‚Äù ‚Üí 0.9481	‚Ä¢	‚ÄúWhat is the meaning of life?‚Äù vs ‚ÄúHow do I bake a cake?‚Äù ‚Üí 0.7471	‚Ä¢	‚ÄúWhat is the purpose of existence?‚Äù vs ‚ÄúHow do I bake a cake?‚Äù ‚Üí 0.7371What confuses me is that the ‚Äúcake‚Äù comparisons are still getting fairly high similarity scores, even though the topics are unrelated.If semantic similarity works like this, then when I encode product profiles for my recommendation system, won‚Äôt many items end up ‚Äútoo close‚Äù in the embedding space? Does all the text embeddings model work that way ? And what is the best model or type of configuration could be suitable to my task .
Hi everyone,Our paper **‚Äú*****Confidence from Consistency under Perturbation of States (CCPS)*****‚Äù** was accepted to the **EMNLP 2025 Main Conference**, placing in the **top 15% of accepted papers** with a **final meta-review rating of 9 (strong accept)**.# üîç MotivationLLMs don‚Äôt just make mistakes, they‚Äôre often confidently wrong. That‚Äôs fine when asking for trivia, but risky in domains like healthcare and finance. Reliable confidence estimation is critical for safe deployment.# ‚ú® What is CCPS?CCPS looks at the hidden states of an LLM. We apply small perturbations to the final hidden representations and observe how stable the prediction is:* If the answer remains stable ‚Üí the model was truly confident.* If the answer flips ‚Üí the confidence was unreliable.This approach is simple, efficient, and does not require fine-tuning the base LLM.# üìä ResultsAcross LLaMA, Mistral, and Qwen on MMLU and MMLU-Pro, CCPS outperformed prior methods like LitCab and Calibration Tuning (CT):* **Calibration**: Error cut by more than 50%, down to \~4.5% on the toughest benchmarks.* **Discrimination**: More accurate at telling right vs. wrong answers than prior SOTA (LitCab, CT, etc.).* **Performance**: Boosts accuracy and robustness, all without fine-tuning the base LLM.# üí° Why it mattersCCPS delivers more reliable, better-calibrated LLMs, models that don‚Äôt just generate answers but also provide trustworthy confidence signals. This is key for high-stakes AI applications, especially in the medical and finance industries.# üìé Resources* üìÑ Paper: [arXiv link](https://arxiv.org/abs/2505.21772)* üíª Code: [GitHub repo](https://github.com/ledengary/CCPS)* üìä Data: [HF Dataset](https://huggingface.co/datasets/ledengary/CCPS)Happy to hear feedback, especially from anyone working on calibration, verifiers (for RL), or LLM deployment..
Hi everyone,I‚Äôm feeling a bit lost in my PhD journey and would really appreciate some outside perspectives.I‚Äôm doing a PhD on LLMs, and so far I‚Äôve been fairly productive: I‚Äôve published several first-author papers, some accepted at top conferences, others under review with good chances of acceptance. I‚Äôve also had a few successful collaborations.The issue is that I don‚Äôt actually like my research. To be honest, I often feel a bit fraudulent, I rush through projects, produce papers that look solid and well-structured, but in the end, I think their impact is minimal. What I really want is to work on something meaningful and useful. But I keep running into two several obstacles:- Any problem I consider tackling already has an overwhelming amount of literature, making it difficult to figure out what truly matters.- While I‚Äôm trying to sort this out, there‚Äôs always the risk that someone else publishes a similar idea first, since so many people are working in this space.- I work with two supervisors which are both young and highly hambitius. They always propose me new research and collaboration but they never propose me hambitius project or give me time to think deep about something. I'm always involved in fast-paced project that lead to pubblication in few months.Because of this, my current strategy has been to work quickly, run experiments fast, and push out papers, even if they‚Äôre not especially deep or important. I also see publications as my main leverage: since I‚Äôm at a low-ranked university in a unknown group, my publication record feels like the only card I can play to land some opportunities in top labs/companies.At times, I think I just want to land an industry roles as a research engineer, where just having a good numbers of papers on my CV would be enough. But deep down, I do care about my work, and I want to contribute something that feels genuinely important.So I‚Äôm curious: how do you approach doing meaningful research in such a competitive field? How do you balance the pressure to publish with the desire to work on something truly impactful?.
In the past weeks, I‚Äôve been working on a somewhat ‚Äúcrazy‚Äù project:manually splitting and structuring 16 million characters of dialogue data, preparing it for feeding into a model to reconstruct a persona module.Along the way, I‚Äôve noticed a few technical challenges:	1.	File size balanceKeeping each file around 300k‚Äì400k characters is the most stable. Beyond that, performance tends to drop.	2.	Context continuityPoor segmentation can easily break the model‚Äôs sense of persona, resulting in inconsistent tone.	3.	Tagging & classificationIt‚Äôs not just about cutting text, but also annotating emotional states and tonal shifts, so the model can later rebuild ‚Äúmemory‚Äù in a coherent way.This made me realize that large-scale corpus curation is itself a kind of language engineering.It‚Äôs not just data processing ‚Äî it shapes whether an AI can emerge as a whole presence.I‚Äôm curious:In your NLP or LLM practice, how do you balance scale with contextual integrity?.
[35M Parameters : RWKV vs Mamba vs GQA vs RetNet](https://preview.redd.it/vul29llezklf1.png?width=1106&format=png&auto=webp&s=c951d5647cd895d418b5a0863184cf9f6745397e)Since it's introduction, the Attention mechanism has been king in LLM architecture, but a few vaillant projects like RWKV, Mamba, Retnet, LiquidAI have been proposing several new mixin mecanisms over time, to attempt to dethrone the king.One of the major issue is that LLM pretraining is extremely dependant on number of parameters and dataset choices, so performing an ablation study on new architecture is not an easy tricks.On the other hand, I met many people with brillant ideas for new architecture and who never got the chance to put it to the test.For that purpose, i create ArchiFactory, a simple (<500 lines of codes) and modular repo that enables to pretrain Small Language Models with comparable parameter count and architecture tricks, in a couple of hours on a single 3090 level GPU.Included:\- simple modular architecture to be sure to compare similar stuff\- complete optimized training loop using pytorch lightning\- fp8 training (can achieve <20min training on 5090 grade GPU)\- examples of common modules like FFN, MOE, GQA, Retnet, Mamba, RWKV6 etc.\- guidelines to test integrate new modulesLink: [https://github.com/gabrielolympie/ArchiFactory](https://github.com/gabrielolympie/ArchiFactory).
Demo: https://github.com/user-attachments/assets/7edb31b2-2c80-4096-9d9c-048ae27c54e7Repo: https://github.com/asmith26/jupytercad-mcp.
**\[D\] Looking for massive schema collections for training models**working on a project and need to find vast amounts of schemas for training models. specifically looking for financial data (transactions, market data, etc) and retail/ecommerce stuff (product catalogs, user behavior, sales data) but honestly need schemas from pretty much every domain I can get. anyone know where to find quality structured schemas at scale? open to paid sources too. need thousands of different schema types ideally. thanks!.
Hey all! I wanted to share my recent project where I implemented the GRPO (Group Relative Policy Optimization) algorithm on top of the [makemore](https://github.com/karpathy/makemore) repo.I wanted to understand how the algorithm works and was trying to find small-scale toy problems where I can implement my own version and see if it works. I had a couple of ideas at first but then I settled on this one idea: to implement the algorithm on top of the makemore project where my goal would be to finetune the character-level language model to generate names with more vowels! So the reward is essentially the number of vowels you have in the generated names.GRPO is actually a simplified version of PPO (which itself is a derivative of TRPO), and while its predecessors are rather complicated to fully grasp unless you have some background in policy gradient or RL in general, GRPO is much simpler to understand and code up (e.g., you don't have to worry about writing Generalized Advantage Estimation etc.)Feel free to take a look and share your thoughts! Here's the repo: [https://github.com/souvikshanku/makemore-grpo/](https://github.com/souvikshanku/makemore-grpo/).
**TL;DR.**¬†We introduce¬†**discrete diffusion**¬†as the action decoder¬†**inside a single transformer**¬†for VLA. Two simple components‚ÄîAdaptive decoding order and Secondary re-masking‚Äîyield consistent action refinement and outperform AR and continuous-diffusion heads. Trains with the¬†**same cross-entropy objective**¬†as VLMs, preserving pretrained priors. This design shows better success rates vs AR and continuous diffusion.  **Disclosure:**¬†I‚Äôm an author.**What‚Äôs new*** **First discrete-diffusion action head for VLA**¬†(to our knowledge).* **Single-transformer, VLM-style training:**¬†keeps the discrete token interface and uses the same CE loss as the VLM backbone ‚Üí¬†**maximizes retention of pretrained VLM priors**.* **Adaptive decoding order:**¬†in each refinement round, we¬†**keep easy tokens first**¬†via confidence / confidence-gap scores and a cosine keep schedule; the rest remain masked for the next round.* **Secondary re-masking:**¬†previously kept tokens are¬†**re-checked**¬†(threshold + residual-drop) and¬†**re-masked**¬†if uncertain/inconsistent, enabling robust cross-round error correction.**Why it matters*** For robotics manipulation tasks, unlike continuous diffusion decoders, our formulation keeps action generation inside a unified transformer and trains with the same cross-entropy objective used by VLMs. This¬†**preserves the backbone‚Äôs pretrained vision-and-language capability**‚Äîakin to extending a vocabulary‚Äîwhile opening a path to¬†**inherit unified transformers‚Äô scaling behavior**, paving the way for¬†**large-scale VLA**. Moreover, Discrete Diffusion VLA¬†**breaks the left-to-right bottleneck**¬†of AR decoders: action chunks are¬†**adaptively decoded in parallel**¬†over a small, fixed number of steps, and uncertain tokens can be revisited via iterative re-masking, leveraging full cross-modal context (including inter-action dependencies) for refinement.**Links*** Paper:¬†[https://arxiv.org/abs/2508.20072](https://arxiv.org/abs/2508.20072)* Demo videos:¬†[https://huggingface.co/papers/2508.20072](https://huggingface.co/papers/2508.20072).
Has anyone managed to get near-full ANE utilization for large language models on Apple silicon?In my experiments:* Core ML conversions run, but ANE usage seems capped <20%.* Apple‚Äôs own foundation models reportedly hit close to 100% ANE.**Questions:*** Has anyone here seen full (or close to full) ANE usage for LLMs?* Are there known tricks or constraints (model architecture, quantization, Core ML flags) that unlock more ANE execution?* Any open-source repos, discussions, or Apple docs you‚Äôd point to?Would love to hear practical experiences‚Äîsuccesses, failures, or hard limits you‚Äôve hit..
Suppose a dataset has a structured features in tabular form but in one column there is a long text data. Can we use stacking classifier using boosting based classifier in the tabular structured part of the data and bert based classifier in the long text part as base learners. And use logistic regression on top of them as meta learner. I just wanna know if it is possible specially using the boosting and bert as base learners. If it is possible why has noone tried it (couldn‚Äôt find paper on it)‚Ä¶ maybe cause it will probably be bad?.
Hi, I was trying to implement the muon optimizer in JAX and found there was no proper documentation about how to hack optax for custom optimizers so tried to write a mini blog about it.https://slavozard.bearblog.dev/implementcustomoptimizerwithoptax/Feedback appreciated..
Hey Looking for information online about the on hold status but couldn‚Äôt find very clearly. The on hold is automatic or normal? Or if some sort of problem was found ? I already have a DOI from Zenodo, but wanted to publish on arxiv as it seems to be the norm currently. It‚Äôs my first publication there, so I‚Äôm not sure what the process is exactly. Thanks! .
**TL;DR:** Created [tokka-bench](https://tokka-bench.streamlit.app/) to compare tokenizers across languages. Turns out your fine-tune's multilingual performance might suck because of tokenization, not architecture. Also explains why proprietary models (Claude, GPT, Gemini) are so much better at non-English tasks.**Links:*** [Live dashboard](https://tokka-bench.streamlit.app/)* [Full blog post](https://www.bengubler.com/posts/2025-08-25-tokka-bench-evaluate-tokenizers-multilingual)* [GitHub repo](https://github.com/bgub/tokka-bench)https://preview.redd.it/7i03jela9elf1.png?width=1724&format=png&auto=webp&s=95378457970e6337b147e71d7a8f0ab2dd67cb91# The Problem Nobody Talks AboutI started this as a side quest while pretraining a multilingual model, but tokenization turned out to be way more important than expected. There are two hidden layers creating massive efficiency gaps:**UTF-8 encoding differences:*** English: \~1 byte per character* Arabic: 2+ bytes per character* Chinese: 3+ bytes per character**Tokenization bias:** Most tokenizers are trained on English-heavy data, so they allocate way more vocabulary to English patterns. These compound into serious problems.# Why This Affects Performance**During training:** If you allocate tokens proportionally (10M English, 1M Khmer), the Khmer text has WAY less semantic content because it needs more tokens per word. Plus Khmer tokens end up being character-level instead of semantic units, making concept storage much harder.**During inference:** Low-resource languages need 2-3x more tokens per sentence:* Slower throughput (costs more to serve)* Context windows fill up faster* More chances to mess up during generation# What I Builttokka-bench measures four key things:1. **Efficiency** \- bytes per token (compression quality)2. **Coverage** \- unique tokens used (script representation)3. **Word splitting** \- how often semantic units get fragmented4. **Subword fertility** \- average tokens per semantic unit# Interesting FindingsYou can actually reverse-engineer training data from tokenizer performance:* Kimi K2: Exceptional Mandarin coverage (obviously Chinese-trained)* Gemma 3: Strong Urdu/Hindi performance* gpt-oss: Good Arabic/Gujarati coverageWeirdest finding: Programming languages show almost identical efficiency across all tokenizers. Probably because everyone trains on GitHub with similar language distributions.# Technical DetailsBuilt on high-quality datasets (FineWeb, FineWeb-2, StarCoder). Samples 2MB per language and calculates per-language metrics. Has some limitations around cross-linguistic comparison due to UTF-8 differences, but great for comparing tokenizers on the same language.Shoutout to Judit √Åcs for the original subword fertility metrics and Rust et al's ACL paper that laid the groundwork.**PS:** if you're from an AI lab and want to contribute your tokenizer's metrics (even if proprietary), please reach out! The community would benefit a lot from understanding how SOTA systems handle this stuff.*Posted this on LinkedIn/Twitter already but figured* r/MachineLearning *would appreciate the technical details. Happy to answer questions about methodology or findings!*.
Hi y‚Äôall, I have a optimisation paper that is not quite ready for conference yet, and I see there are a few Neurips workshop coming up that fits my research direction. I‚Äôm wondering if it‚Äôs good to submit the work to the workshop?.
Hi reddit, wanted to share my thesis on AI / LLM psychotherapy @ [https://osf.io/preprints/psyarxiv/4tmde\_v1](https://osf.io/preprints/psyarxiv/4tmde_v1?fbclid=IwZXh0bgNhZW0CMTAAYnJpZBExNHhlVkhlWWpDVE1xN3dTeAEeoTtZ3pOVtRD7ODEFZo_qpyjjOEkW_2OFHqsH36X4xp7THoZC3F7YFDc1zJU_aem_Etq7yhCr4L3eA8v9QqrFgw)Since the rules for this subreddit require more than just a link, I thought I'd share some surprising conclusions in plain english. **1. AI therapy research tends to use arbitrary success metrics:** the majority of LLM research on psychotherapy uses theraputic-sounding ad-hoc metrics (e.g. ""empathy"" as rated by LLM-as-judge), and not actually improvement in clients or other validated metrics. There's a real risk in AI researchers testing techniques and drawing conclusions when totally unrelated to the purpose of therapy (e.g. quality-of-life improvement). If you're interested in learning more about this issue, section 1.4 focuses on it, and offers the north-star alternatives commonly used in psychotherapy research in sections 1.1-1.3. **2. AI therapy tools (APTs) are already comparable to human therapists:** There's two studies from 2025 (Limbic, Therabot) that demonstrate non-inferior clinical outcomes in LLM-driven APTs and human therapists for depression & anxiety symptom reduction. If replicated, that's huge. That's a step-level jump in clinical from the previous generation of rules-based APTs (e.g. Woebot, Wysa), highlighting that maybe the generative properties of LLMs were the key gap to improve clinical performance. There's a lot more to say on these results, and if you're interested sections 2 & 3.1 talk more about them and put them into clinical context. 3. **ŒîAPT allows predicting future clinical outcomes :** It's actually surprising that APTs perform at the lower-bounds of human therapists, since they kinda suck right now. The predictive model I proposed is that APTs clinical performance is boosted by advantages therapist can't compete with (e.g. 24/7 availability, low cost), while being depressed by current disadvantages (e.g. poor therapy skills, hallucinations, sycophancy, inconsistencies, bias). All of this playing out while major issues around legality, safety, privacy and ethics are unresolved and could shutdown the field. If you're intersted, you can read more about the model (section 3.3),  the advantages of APTs over human therapists (section 3.4), APTs' current limitations (section 3.5), and the key risks (section 3.6). https://preview.redd.it/rof96tmbuelf1.png?width=1162&format=png&auto=webp&s=5a1e81bbb9e8b12b09210967da97b2fe96816df0  **4. Techniques teaching LLM therapy:** Most people on this subreddit won't be surprised to learn you can teach LLM to perform therapy using a combination of context/prompt engineering, fine-tuning, multi-agent architecture, and ML models. What is surprising is that both clinically-validated APTs use ML models to offset the stochastic nature of LLMs, especially for safety purposes. Also surprising is that neither used a multi-agentic architecture. Therabot used fine-tuning on synthetic dialogues, and Limbic used context-engineering techniques. You can learn more about implementing therapy skills in LLM through context/prompt engineering (section 4.1), fine-tuning (section 4.2), multi-agent architectures (section 4.3), ML models (4.4). Around fine-tuning / pretraining there's a really nested conversation about data requirements, ethically sourcing transcripts, and choosing therapy modalities in section 4.1. https://preview.redd.it/lbcoovvc0flf1.png?width=2246&format=png&auto=webp&s=f029fed00649b4cca0ddb84d9830ded03f5f94ea5. **Overall, most disadvantages of LLMs are addressable in AI therapy**: Reading the literature critiquing APTs it's really easy to get discouraged thinking for examples ""oh wow, hallucinations are going to make AI therapy impossible"". But actually, there's a bunch of techniques that can be used to mitigate the issues LLMs currently have. Combining the lowering rates of issues in newer LLMs released with mitigation techniques, most issues can theoretically be significantly mitigated in production. The outlier here being sycophancy which doesn't appear to have great mitigations on subjective topics. You can read more about the issues of LLMs in APTs and how to mitigate those in section 5. **6. video therapy with multi-modal audio/video LLMs:** One surprising fact from psychotherapy research is that therapy done over video (e.g. zoom) is actually as effective as in-person therapy. Ideally, LLMs would be able to pickup and transmit non-verbal cues over video-audio. Having an virtual therapy avatar using audio & video to attune to clients isn't actually that far off based on my literature review. Surprisingly it seems that emotional speech, and attuning to clients facial and body expressions are ready for implementation in AI therapy today. More on that in section 6.Happy to have a conversation, receive critique, and answer questions here. This summary above was meant to offer informal insights into what is an otherwise quite lengthy paper. For more formal discussion and details, it's really best to read the paper. .
I'm a PhD student in HCI, and I recently had a paper accepted at a B-ranked ML conference. While I have prior experience presenting at HCI venues, this will be my first time presenting at an ML conference.I want to know if there are any tips or best practices for preparing slides and giving talks in the ML community. Are there particular presentation styles, slide formats, or expectations that differ from HCI conferences?Thanks in advance for your advice!.
Hi everyone!As part of my internship, I am conducting research to understand the computational power needs of professionals who work with machine learning and AI. The goal is to learn how different practitioners approach their requirements for GPU and computational resources, and whether they prefer cloud platforms (with inbuilt ML tools) or value flexible, agile access to raw computational power.If you work with machine learning (in industry, research, or as a student), I‚Äôd greatly appreciate your participation in the following survey. Your insights will help inform future solutions for ML infrastructure.The survey will take about two to three minutes. Here¬¥s the link:¬†[https://survey.sogolytics.com/r/vTe8Sr](https://survey.sogolytics.com/r/vTe8Sr)Thank you for your time! Your feedback is invaluable for understanding and improving ML infrastructure for professionals..
I reviewed 100 models over the past 30 days. Here are 5 things I learnt.TL;DR: Spent a month testing every AI model for work, a few tools I'm building and RL. Build task-specific evals. Most are overhyped, a few are gems, model moats are ephemeral, and routers/gateways are the real game-changer.So I've been building a few evaluation tools, RHLF and RL environments for the past few months so I decided to be extra and test literally everything.100 models. 30 days. Too much coffee :( Here's what I found:    1. Model moats are ephemeralModel moats don't last and it can be hard to pay for many subscriptions if you're building for users and machines. What's SOTA today gets beaten in 2 months. Solution: Use platforms like Groq, OpenRouter, FAL, Replicate etcMy system now routes based on task complexity: Code generation, Creativity, Complex reasoning and Code generation.2. Open source FTWThe gap is closing FAST. Scratch that. The gap between open and closed models has basically disappeared. If you're not evaluating open-source options, you're missing 80% of viable choices. From Deepseek, Qwen to Kimi, these models help you build quick MVPs at little or no cost. If you do care about privacy, Ollama and LMStudio are really good for local deployment.3.Benchmarks are mostly decieving due to reward hackingBenchmaxxing is a thing now. Models are increasingly being trained on popular eval sets, and it's actually annoying when models that scored ""high"" but sucked in practice. It's also why I'm a huge fan of human preference evaluation platforms that are not easily gamed (real world vs benchmarks). Build your own task-specific evals.4.Inference speed is everythingSpeed matters more than you think. Users don't care if your model is 2% more accurate if it takes 30 seconds to respond. Optimize for user experience, not just accuracy. Which leads me to..5.Task-specific models > general purpose models for specialized work.No 4 is also a huge reason why I'm a huge fan of small models finetuned for special tasks. Model size doesn't predict performance.Test small models first etc Llama 3.2 1B, smolLLM, moondream etc and see if you can get a huge boost by finetuning them on domain tasks rather than just deploying a big SoTA general purpose model. Cost way lesser and usually faster.What models are in your current prod stack? Any hidden gems I missed in the open source space?.
Basically the title. Obviously the quality of the work and relevance to the role is very important, but all else being equal, what is the perceived prestige difference between Findings and Main in NLP conferences? This would be with regard to getting research internships and research scientist positions..
I‚Äôm still pretty new to reinforcement learning (and machine learning in general), but I thought it would be fun to try building my own CartPole agent from scratch in C++.It currently supports PPO, Actor-Critic, and REINFORCE policy gradients, each with Adam and SGD (with and without momentum) optimizers.I wrote the physics engine from scratch in an Entity-Component-System architecture, and built a simple renderer using SFML.Repo: www.github.com/RobinLmn/cart-pole-rlWould love to hear what you think, and any ideas for making it better!.
I previously shared the open‚Äësource library DocStrange. Now I have hosted it as a free to use web app to upload pdfs/images/docs to get clean structured data in Markdown/CSV/JSON/Specific-fields and other formats.**Live Demo:**¬†[**https://docstrange.nanonets.com**](https://docstrange.nanonets.com/)**Github:** [**https://github.com/NanoNets/docstrange**](https://github.com/NanoNets/docstrange)Would love to hear feedbacks!https://i.redd.it/gl23k00osclf1.gifOriginal Post - [https://www.reddit.com/r/MachineLearning/comments/1mh9g3r/p\_docstrange\_open\_source\_document\_data\_extractor/](https://www.reddit.com/r/MachineLearning/comments/1mh9g3r/p_docstrange_open_source_document_data_extractor/).
I looked through 402 healthcare AI repos on GitHub and found almost 50% of infrastructure tools are just solving data format conversion problems, suggesting a systematic gap between ML research and deployment in clinical settings.Built HealthChain to bridge Python ML workflows with healthcare data standards (FHIR, HL7, etc.) without the usual pain. 4 years of NHS NLP development experience went into making this feel like normal Python.Post + pretty graphs: https://open.substack.com/pub/jenniferjiangkells/p/healthchain-building-the-tool-i-wish?r=4o6h4Code: https://github.com/dotimplement/HealthChainAnyone else work in healthcare AI here? Would love to learn what you‚Äôre working on!.
A recurring challenge in ML is balancing **interpretability** and **predictive performance**. We all know the classic tradeoff: simple models like linear regression or short CART-style regression trees are transparent but often lack enough accuracy, while complex ensembles like Random Forests and XGBoost are accurate but opaque.We‚Äôve been working on a method called **TRUST** (*Transparent, Robust and Ultra-Sparse Trees*). The core idea is to go beyond constant values in the leaves of a tree. Instead, TRUST fits a sparse regression model (either linear or constant) in each leaf, resulting in a **piecewise-linear tree** that remains interpretable.In our [recent paper](https://arxiv.org/abs/2506.15791), accepted at PRICAI 2025, we compared this method against a range of models on 60 datasets. While we were encouraged by the results ‚Äî TRUST consistently outperformed other interpretable models and closed much of the accuracy gap with Random Forests ‚Äî we'd like to hear your thoughts on this topic.The problem we‚Äôre tackling is widespread. In many real-world applications, a ""black box"" model isn't an option. We've often found ourselves in situations where we had to choose between a sub-par interpretable model or an accurate but untrustworthy one.Here‚Äôs a concrete example from a [tutorial on explaining EU life satisfaction](https://github.com/adc-trust-ai/trust-free/blob/main/notebooks/trust-free_tutorial.ipynb).[TRUST produces a single interpretable tree, while Random Forest uses hundreds of deep trees to achieve similar accuracy.](https://preview.redd.it/3tzdaim3kdlf1.png?width=2600&format=png&auto=webp&s=e289771608b0d74498dc83b39c1efd2670ed8ea9)As the image above shows, both TRUST and a Random Forest achieve \~85% test R¬≤ ‚Äî but one produces a **single interpretable tree**.TRUST is implemented as a free Python package on PyPI called `trust-free`.**Discussion:** How do you usually handle the interpretability vs. accuracy tradeoff in your own regression projects? What methods, beyond the standard ones, have you found effective? We‚Äôre looking forward to hearing your perspectives..
I have started working on implementing actual research papers in machine learning and I have started with ""Attention is all you need"" paper.I have implemented all the code and it is an educational attempt. I would like you to get some eyes on the repo from the members of this subreddit and get your opinion. This is still a work in progress but your reviews and PRs are really appreciated. I have written the code focusing on educational purposes and not optimisations. Please take a look below.[https://github.com/MayukhSobo/Transformer](https://github.com/MayukhSobo/Transformer)Edit: I would like to clarify that some of the code related to helper functions and all the doc strings are implemented by Claude not because they are difficult to do but they are simply boring. The core architecture is implemented by me. Also at no point I claimed that this is my own work and I haven't used AI. The part which really required me to code and not use AI, I did it on my own. If you really think that the complete code is just a result of some vibe coding, I welcome you to try that with most advanced AI tools and see if you can reproduce even 70% of what I did or not. .
Hey folks I've been using [Modal.com](http://Modal.com) (I am not affiliated) for a while to run machine learning workloads in the cloud, and I really like its simplicity, container-based execution, and ability to scale on demand. However, I'm starting to explore more self-hosted options due to cost reasons and to gain more control over the infrastructure while building apps.Does anyone know of good self-hosted alternatives that offer similar functionality? Ideally, something that:\- Supports containerized jobs (Docker or similar)\- Can run Python/ML workloads easily\- Has a nice API  for launching jobs (this is important) \- Offers some kind of job orchestration or scheduling\- Bonus: GPU support and autoscaling would be amazingThanks in advance .
Hi everyone,A few weeks ago I shared my first preprint on a new optimizer,¬†Ano, designed for noisy and highly non-convex environments such as deep RL. Thanks to all the feedback I received here, I‚Äôve updated the paper: clarified the positioning, fixed some mistakes, and added an Atari benchmark to strengthen the empirical section.üîó¬†**arXiv link:**¬†[https://arxiv.org/abs/2508.18258](https://arxiv.org/abs/2508.18258)  üì¶¬†**Install via pip:**¬†`pip install ano-optimizer`  üíª¬†**Code & experiments:**¬†[github.com/Adrienkgz/ano-experiments](https://github.com/Adrienkgz/ano-experiments)Quick recap of the idea: Ano separates the momentum¬†direction¬†from the gradient magnitude, aiming to improve robustness and stability compared to Adam in noisy deep RL training. The updated version also includes a¬†convergence proof¬†in standard non-convex stochastic settings.This is still my first research contribution, so I‚Äôd love to hear your thoughts ‚Äî whether on the method itself, the experiments, or the clarity of the writing. Any feedback, comments, or constructive criticism are very welcome üôèThanks again to everyone who took the time to give feedback last time, it really helped me make the work stronger!Adrien.
Disclosure: I am one of the authors. Links will be in the first comment per sub rules.TLDR  We are releasing Exosphere, an open source runtime and durable state manager for agentic workflows that need dynamic branching, retries, and parallel execution. To evaluate it on a real workload, we built WhatPeopleWant, an agent that mines Hacker News discussions and posts distilled problem statements to X every 2 hours. This post shares the setup, workload design, and the ablations we are running, and invites feedback on methodology.Single runs are trivial. At scale you need to1. fan out across large inputs2. branch at runtime on model outputs3. retry with idempotency4. persist every step for audit and replay5. mix CPU and GPU stages6. resume after faults.Exosphere‚Äôs runtime treats agents like graphs with explicit state, a scheduler, and observability.We use WhatPeopleWant as a standing benchmark. It ingests Hacker News via the public Firebase API, scores and routes items, optionally enriches high-signal threads, and materializes candidate problem statements. The bot then posts outputs on a fixed schedule.‚Ä¢ Gating high-signal discussions reduces heavy-model calls and improves tail behavior at similar quality thresholds  ‚Ä¢ Durable state and idempotent nodes make partial replays predictable and minimize upstream rework after faults  ‚Ä¢ Parallelism helps until external API backpressure dominates, which shows up in queue depth and wait timesWhat I want feedback on  ‚Ä¢ Composite metrics that capture quality, cost, and reliability for agentic graphs  ‚Ä¢ Fair baselines for orchestration when branching is dynamic  ‚Ä¢ Better failure-injection and replay methodologies to compare runtimesFirst comment with links.
Maybe I am confused between two terms ""active learning"" and ""self-learning"". But the basic idea is to use a trained model to classify bunch of unannotated data to generate pseudo labels, and train the model again with these generated pseudo labels.  Not sure ""bootstraping"" is relevant in this context.A lot of existing works seem to use such techniques to handle data. For example, SAM (Segment Anything) and lots of LLM related paper, in which they use LLM to generate text data or image-text pairs and then use such generated data to finetune the LLM.My question is why such methods work?  Will the error be accumulated since the pseudo labels might be wrong?.
A new preprint (Agrawal et al., 2025) introduces¬†**GEPA (Genetic-Pareto Prompt Evolution)**, a method for adapting compound LLM systems. Instead of using reinforcement learning in weight space (GRPO), GEPA mutates prompts while reflecting in natural language on traces of its own rollouts.The results are striking:* GEPA outperforms GRPO by up to¬†**19%**¬†while using¬†**35√ó fewer rollouts**.* It also consistently surpasses MIPROv2, the state-of-the-art prompt optimizer.* In many cases, only a few hundred rollouts were sufficient, compared to tens of thousands for RL .The shift is conceptual as much as empirical: Where RL collapses complex trajectories into a scalar reward, GEPA treats those trajectories as¬†*textual artifacts*¬†that can be reflected on, diagnosed, and evolved. In doing so, it makes use of the medium in which LLMs are already most fluent, language, instead of trying to push noisy gradients through frozen weights.What‚Äôs interesting is the infra angle: GEPA‚Äôs success in multi-hop QA hinges on generating better second-hop queries.¬†**That implicitly elevates retrieval infrastructure Linkup, Exa, Brave Search into the optimization loop itself**. Likewise, GEPA maintains a pool of Pareto-optimal prompts that must be stored, indexed, and retrieved efficiently.¬†**Vector DBs such as Chroma or Qdrant are natural substrates for this kind of evolutionary memory.**This work suggests that the real frontier may not be reinforcement learning at scale, but¬†**language-native optimization loops**¬†where reflection, retrieval, and memory form a more efficient substrate for adaptation than raw rollouts in parameter space.https://preview.redd.it/5l4lcmokg7lf1.png?width=1602&format=png&auto=webp&s=719e33f34feb5103ed1f375d3366745dd3415d77.
Hi!I'll be starting a PhD in ML for Robotics (RL, Sensor Fusion etc.) and was wondering which laptop would be best to support me throughout the next 4 years. I am looking for a powerful laptop, with good battery life, not too heavy and that is robust.My budget is $3000.So far, I have identified the following laptops, but am unsure which would be the best choice.\-¬†**Razer Blade 16**¬†(either RTX 5070 Ti + 32GB RAM ($3100) or RTX 5080 + 64GB ($4050)): apart from battery life which is not the most ideal, would I see a significant difference when running RL simulations (IsaacGym) or large multimodal (video, imu, ...) ML models between both configurations? Price difference between both configurations is \~$850 (with taxes) which is significant.\-¬†**MSI Vector 16¬†HX¬†AI**¬†(RTX‚ÄØ5080, 64‚ÄØGB) - $2600\-¬†**ThinkPad P1 Gen 7**¬†(RTX Ada 3000, 64GB) - $3200: has a good battery life, but its GPU is Ada series, which is not the best for RL simulations.\-¬†**Legion Pro 7i Gen10**¬†(RTX 5080, 32GB) - $3100: the legions are usually very heavy laptops.Essentially, I am looking for a laptop that will be somewhat future-proof to the fast pace of new GPUs coming out, is powerful for my intended use (RL simulations + ML sensor fusion), has a good battery life (for note-taking in courses) and easily transportable (ie. neither too bulky nor heavy). Also, do I require RTX 5080 (recommended for IsaacSim) as GPU, and how big a diffference is 32GB vs 64GB RAM?Thank you in advance for any suggestions or feedback!EDIT: I have access to cluster, but thought having powerful laptop could be useful when running real-time inference on robot + working with smaller models / testing out stuff before training on cluster..
Hello researchers,  I am familiar with common basic approaches to quantization, but after a recent interview, I wonder what the current SOTA approaches are, which are actually used in industry.  Thanks for the discussion!.
I built a spam vs ham classifier and wanted to test a different angle: instead of just oversampling with SMOTE, could **feature engineering** help combat extreme class imbalance?**Setup:*** Models: Na√Øve Bayes & Logistic Regression* Tested with and without SMOTE* Stress-tested on 2 synthetic datasets (one ‚Äúnormal but imbalanced,‚Äù one ‚Äúadversarial‚Äù to mimic threat actors)**Results:*** Logistic Regression ‚Üí **97% F1** on training data* New imbalanced dataset ‚Üí Logistic still best at **75% F1*** Adversarial dataset ‚Üí **Na√Øve Bayes** surprisingly outperformed with **60% F1****Takeaway:** Feature engineering can mitigate class imbalance (sometimes rivaling SMOTE), but adversarial robustness is still a big challenge.Code + demo:  üîó [PhishDetective ¬∑ Streamlit](https://phishdetective.streamlit.app/)  üîó [ahardwick95/Spam-Classifier: Streamlit application that classifies whether a message is spam or ham.](https://github.com/ahardwick95/Spam-Classifier/tree/main)Curious ‚Äî when you deal with **imbalanced NLP tasks**, do you prefer resampling, cost-sensitive learning, or heavy feature engineering?.
Dear¬†[r/MachineLearning](https://www.reddit.com/r/MachineLearning/)¬†friends,Hello everyone! I hope you are all doing well out there.I've been observing a pattern in the AI research field that I can only describe as a ""Mass Amnesia."" It seems we're forgetting the valuable research paths we were on before the ChatGPT moment.In my latest blog post, I argue that while scaling up LLMs was initially a courageous endeavour, the current obsession and monoculture around it is actively keeping us stuck. Instead of building on a diverse set of ideas, we're chasing a single approach, which I believe is making us amnesiacs about what came before and what's possible.I'd love for you to read my spicy takes and share your own. Let's tear my arguments and ideas apart. ;)üîó¬†**Full Article:**[https://pieces.app/blog/the-cost-of-ai-scaling](https://pieces.app/blog/the-cost-of-ai-scaling)I look forward to your arguments and thoughts.Regards,Antreas  PS. This is a repost of [https://www.reddit.com/r/MachineLearning/comments/1mu28xl/d\_too\_much\_of\_a\_good\_thing\_how\_chasing\_scale\_is/](https://www.reddit.com/r/MachineLearning/comments/1mu28xl/d_too_much_of_a_good_thing_how_chasing_scale_is/) because it was removed without any explanation and the mods never replied to my queries on what was done wrong and how I could modify the post so it would abide by whatever rule I inadvertently tripped on.  The post was starting to get some real discussion going when it was removed and wanted to give this another chance as I want to hear what everyone has to say and engage in discourse. .
For those working in AI/ML, how do you keep your teams agile when project goals or data requirements shift halfway through a project? I‚Äôve seen situations where a model was nearly production-ready, but then stakeholders introduced new objectives or the data pipeline changed, forcing big pivots..
For some time I've been fascinated by adopting knowledge from approximation theory into ML feature engineering, and I'm sharing my learnings in a series of blog posts, mainly about various polynomial bases as features.So here is the latest one: [https://alexshtf.github.io/2025/08/19/Orthogonality.html](https://alexshtf.github.io/2025/08/19/Orthogonality.html)It discusses my understanding of orthogonal bases as informative feature generators. I hope you enjoy reading as I enjoy learning about it..
Hi all!  I'm drafting an app with pose detection (currently using¬†**MediaPipe**) and object detection (early¬†**Yolo11**). Since I cannot run these models on the phone itself, I'm developing the backend separately to be deployed somewhere, to then¬†*call it from the app when needed*.  Basically I would need a¬†**GPU-based backend**¬†(I can also divide the detections and the actual result usage).Now, I know about¬†*HuggingFace*¬†of course and I've seen a lot of other hosting platforms, but I wanted to ask if you have any suggestions in this regards?  I think I might want to release it as free, or for a one-time low cost (if the costs are too high to support myself), but I also do not know how widespread it can be... You know, either useful and loved or unknown to most.  The trick is that, since I would need the APIs always ready to respond, the backend would need to be up and¬†*running 24/7*. All of the options seem to be quite costly...Is there any better or worse way to do this?.
Hi folks,  Fellow ML researcher here üëãI‚Äôve been working in the LLM space for a while now, especially around *reasoning models* and *alignment* (both online and offline).While surveying the literature, I couldn‚Äôt help but notice that a lot of the published work feels‚Ä¶ well, incremental. These are papers coming from great labs, often accepted at ICML/ICLR/NeurIPS, but many of them don‚Äôt feel like they‚Äôre really pushing the frontier.I‚Äôm curious to hear what the community thinks:* Do you also see a lot of incremental work in LLM research, or am I being overly critical?* How do you personally filter through the ‚Äúnoise‚Äù to identify genuinely impactful work?* Any heuristics or signals that help you decide which papers are worth a deep dive?Would love to get different perspectives on this ‚Äî especially from people navigating the same sea of papers every week.  PS: Made use of GPT to rewrite the text, but it appropriately covers my view/questions.
Was it abandoned? The website links are dead..
Some interesting benchmarks I‚Äôve been digging into:	‚Ä¢~1.3s cold start for a 32B model	‚Ä¢~3.7s cold start for Mixtral-141B (on A100s)       ‚Ä¢By comparison, Google Cloud Run reported ~19s for Gemma-3 4B earlier this year, and most infra teams assume 10‚Äì20s+ for 70B+ models (often minutes).If these numbers hold up, it reframes inference as less of an ‚Äúalways-on‚Äù requirement and more of a ‚Äúruntime swap‚Äù problem.Open questions for the community:	‚Ä¢How important is sub-5s cold start latency for scaling inference?	‚Ä¢Would it shift architectures away from dedicating GPUs per model toward more dynamic multi-model serving?.
When running experiments, I often struggle with going beyond the surface-level metrics. How do you approach interpreting experimental data in a way that actually leads to useful insights and new ideas? What frameworks, statistical methods, or mindset shifts help you decide whether results are meaningful versus just noise?.
I'm reviewing for AAAI, and wanted to ask the community for some advice. I got a paper for review that is very well known in my subfield, published in 2023, but only previously published onto Arxiv. As best I can tell, the paper has had some minor rewrites for publication, but is otherwise largely the same as the well-established work. What's the best policy here? It was a very good paper when it came out, but the existing version basically ignores the last two years of work by the community, in part because some decent portion of that work is based on this paper.  Any advice on the best way to review this would be appreciated.
Hey everyone, I just finished writing a short paper about a new idea I call MALM, a Modular Adapter-based Language Model.The core idea is simple: instead of training giant multilingual LLMs, I propose keeping one small, sharp Core Language Model (reasoning in English), and delegating translation to lightweight, swappable Specialized Translation Adapters (STAs).This means:\- Smaller, cheaper models\- Easy to add new languages\- Better for edge devices and low-resource settingsExample flow:  \`\`\`  User: ""Translate 'my name is Adam' into German.""  CLM ‚Üí <to:de> my name is Adam </to>  STA ‚Üí ""Mein Name ist Adam""\`\`\`Read the full paper here:¬†[https://huggingface.co/TimesLast/MALM](https://huggingface.co/TimesLast/MALM)Would love feedback, especially on how this could be extended beyond translation (math, code, multimodal adapters, etc.)..
Hello everyone!My name is Virginie and I am a first-year French PhD student¬†**studying human‚Äìartificial intelligence interactions.**I am conducting a¬†**very quick**¬†(approximately 6 minutes) and¬†**anonymous online study**.To ensure reliable results, I need at least 300 AI users, some of whom should have experience in integrating or designing AI models, although this is not compulsory for taking part!If you are 18 or over, you can take part by clicking this link:[https://virginie-lepont.limesurvey.net/967745?newtest=Y&lang=en](https://virginie-lepont.limesurvey.net/967745?newtest=Y&lang=en)The survey is¬†**also available in French.**Every response is valuable! Thank you so much for your help!Virginie *This post has been approved by one moderator of this group.* https://preview.redd.it/gwtpg6p9t5lf1.jpg?width=940&format=pjpg&auto=webp&s=39e54c6e762ab220af6a1c32d8754d8c9b5ee34c.
https://preview.redd.it/vy1h49l0t8lf1.png?width=3456&format=png&auto=webp&s=1c0991294abf01d6699c04b663cd30973e4bd633Is Vibe training AI models something people want?     I made a quick 24hours YC hackathon app that wires HF dataset lookups + Synthetic data pipeline + Trnasfomers too quickly fine tune a gemma 3 270m on a mac, I had 24hours to ship something and now have to figure out if this is something people would like to use?     Why this is useful? A lot of founders I've talked to want to make niche models, and/or make more profit (no SOTA apis) and overall build value beyond wrappers. And also, my intuition is that training small LLMs without code will enable researchers of all fields to tap into scientific discovery. I see people using it for small tasks classifiers for example. For technical folk, I think an advanced mode that will let you code with AI, should unleash possibilities of new frameworks, new embedding, new training technics and all that. The idea is to have a purposeful built space for ML training, so we don't have to lean to cursor or Claude Code. I'm looking for collaborators and ideas on how to make this useful as well?Anyone interested can DM, and also signup for beta testing at [monostate.ai](http://monostate.ai)    Somewhat overview at [https://monostate.ai/blog/training](https://monostate.ai/blog/training)  \*\*The project will be free to use if you have your own API keys!\*\* In the beginning no Reinforcement learning or VLMs would be present, focus would be only in chat pairs fine tuning and possibly classifiers and special tags injection! Please be kind, this is a side project and I am not looking for replacing ML engineers, researchers or anything like that. I want to make our lifes easier, that's all. .
Hey everyone,I'm working with the Yelp dataset and have a quick question about the review_count field in the business.json (what I'll call the business_df).The business_df is a list of businesses, and the review_df is a list of every single review interaction.Is the review_count in the business_df calculated directly from the interactions listed in the review_df?If I split my data into train and test sets for a recommendation model, should I recalculate review_count from only the training interactions (so that test interactions remain unseen)? Or is review_count a static field provided by Yelp, independent of our data splits?The reason I'm asking is I'd like to use review_count as part of my initial features/embeddings. I'm not sure if I should treat it as fixed metadata from Yelp or recompute it dynamically from my training set only.Thanks a lot if anyone can clarify this!.
I open-sourced a project called Mira, an agentic AI system built on the OpenAI Agents SDK that automates company research.You provide a company website, and a set of agents gather information from public data sources such as the company website, LinkedIn, and Google Search, then merge the results into a structured profile with confidence scores and source attribution.The core is a Node.js/TypeScript library (MIT licensed), and the repo also includes a Next.js demo frontend that shows live progress as the agents run.GitHub: [https://github.com/dimimikadze/mira](https://github.com/dimimikadze/mira).
A few years ago, there was a lot of buzz around JAX, with some enthusiasts going as far as saying it would disrupt PyTorch. Every now and then, some big AI lab would release stuff in JAX or a PyTorch dev would write a post about it, and some insightful and inspired discourse would ensue with big prospects. However, chatter and development have considerably quieted down since transformers, large multimodal models, and the ongoing LLM fever. Is it still promising? Or at least, this is my impression, which I concede might be myopic due to my research and industry needs. .
Hello everyone!!! I have several Reinforcement Learning projects underway. One is Sonic 2 with PPO. The other is developing an environment that supports games not available with Farama Group's stable-retro. I may need collaborators for the latter. I don't know if I'll integrate it into their project, stable-retro, in the future. One thing I've already achieved is running PCSX2 (it's missing the state loading option), and I'm creating a Python lib to load with stable-baselines3, etc. If anyone is interested, the links to both projects are below:[https://github.com/paulo101977/Sonic-2-Genesis-Reinforcement-Learning](https://github.com/paulo101977/Sonic-2-Genesis-Reinforcement-Learning)[https://github.com/paulo101977/sdlarch-rl](https://github.com/paulo101977/sdlarch-rl)I also started a PCSX2 environment with direct access to the Python process, but I'll abandon it as it's very slow.  .
I'm working on developing a ML classification project using Python, divided into 5 output categories (classes). However, my training dataset is extremely unbalanced, and my results always lean toward the dominant class (class 5, as expected).However, I wanted my models to better learn the characteristics of the other classes, and I realized that one way to do this is by balancing the training dataset. I tried using SMOTETomek for oversampling, but my models didn't respond well. Does anyone have any ideas or possibilities for balancing my training dataset?There are 6 classification ML models that will ultimately be combined into an ensemble. The models used are: RandomForest, DecisionTree, ExtraTrees, AdaBoost, NaiveBayes, KNN, GradientBoosting, and SVM.The data is also being standardized via standardSCaler.Total record count by category:Category 1: 160 recordsCategory 2: 446 recordsCategory 3: 605 recordsCategory 4: 3,969 recordsCategory 5: 47,874 records.
Are there any projects/packages that help inform an agent which FM to use for their use case? Curious if this is even a strong need in the AI community? Anyone have any experience with ‚Äúrouters‚Äù?Update: especially curious about whether folks implementing LLM calls at work or for research (either one offs or agents) feel this as a real need or is it just a nice-to-know sort of thing? Intuitively, cutting costs while keeping quality high by routing to FMs that optimize for just that seems like a valid concern, but I‚Äôm trying to get a sense of how much of a concern it really isOf course, the mechanisms underlying this approach are of interest to me as well. I‚Äôm thinking of writing my own router, but would like to understand what‚Äôs out there/what the need even is first.
Suppose I want to fit a linear model to non-linear **rational** features. Something like `RationalTransformer` instead of `SplineTransformer` in Scikit-Learn, that uses a basis of rational functions. The domain of my raw features before being transformed are (theoretically) unbounded non-negative numbers, such as ""time since X happened"", ""total time spent on the website"", or ""bid in an auction"".So here is the question: *where would you put the poles? Why?*Note, I'm not aiming on fitting one rational curve, so algorithms in the spirit of AAA are irrelevant. I'm aiming at a component I can use in a pipeline that transformes features before model fitting, such as `MinMaxScaler` or `SplineTransformer` in scikit-learn..
Hey all, I'm working on developing AI models that can classify and track positions throughout BJJ matches - and I'm keen to get some thoughts on this idea early on.You can check it out here:¬†[https://bjjhq.ai/](https://bjjhq.ai/)Ultimately BJJHQ provides an interactive positional timeline beneath match videos, showing all position changes throughout the match, so you're able to instantly jump to specific positions and see how transitions unfold.The idea is that people would be able to search for not only a competitor, but a specific position and combination (e.g., ""Gordon Ryan in back control""), and instantly access all matches where that scenario occurs. You would also be able to filter and sort matches by time spent in specific positions.Roadmap:* Expanding the match database and position categories* Technique/submission recognition* Automated scoring system built on this positional foundationWould love to know if anyone would be interested to chat or collaborate on this project ... please reach out if keen!Thanks for any feedback!.
\[EDIT\] I meant December / Dec not November / Nov. It was late at night I'm sorry -  lol.Context:* Neurips 2025 conference is from Tue, Dec 2 to Sun, Dec 7* This is my first time attending the conference.* As I need to travel again right after the conference for personal reasons, I am figuring out on what dates to book the hotels / flights in advance.* **Are there post conference events on the last day** eg: Sun, Dec 7 night? I am not sure if it's better to return right away (on Sun, Dec 7 evening) or fly back later (on Mon, Dec 8 morning)?.
**[D] Exploring Local-First AI Workflow Automation**Hi all,  I‚Äôve been experimenting with an open-source approach to AI workflow automation that runs entirely **locally** (no cloud dependencies), while still supporting real-time data sources and integrations. The goal is to provide a **privacy-first, resource-efficient alternative** to traditional cloud-heavy workflow tools like Zapier or n8n, but with LLM support integrated.üëâ My question for the community:  How do you see **local-first AI workflows** impacting ML/AI research, enterprise adoption, and robotics/IoT systems where privacy, compliance, and cost efficiency are critical?  - Repo: [Agentic Signal](https://github.com/code-forge-temple/agentic-signal) (open-source, AGPL v3 / commercial dual license)  - Demo video: [YouTube link](https://youtu.be/62zk8zE6UJI)  Would love feedback from both the research and applied ML communities on potential use cases, limitations, or challenges you foresee with this approach.  Thanks!  .
Posts here within the past 6 months have discussed both¬†[Topological Deep Learning (TDL)](https://www.reddit.com/r/MachineLearning/comments/1ji6xlv/d_topological_deep_learning_promising_or_hype/)¬†and¬†[Geometric Deep Learning (GDL)](https://www.reddit.com/r/MachineLearning/comments/1jabkt8/d_geometric_deep_learning_and_its_potential/). Even though the nomenclature suggests otherwise, these two (exciting!) areas have come to represent rather specific topics in recent years.¬†*Very crudely speaking*, ""TDL"" seems to focus mainly on higher-order message passing (HOMP); ""GDL"" to the design of neural networks mod domain symmetries.For the purposes of discussion, let's set the operational definition of TDL to be as in this paper:¬†[Hajij, Mustafa, et al. Topological Deep Learning: Going Beyond Graph Data. Springer, 2024.](https://tdlbook.org/)and the operational definition of GDL to be as in this paper:¬†[Bronstein, Michael M., et al. Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges. MIT Press, 2021.](https://arxiv.org/abs/2104.13478)With that in place: what are some applications of geometry and topology in deep learning that¬†*do not properly belong to TDL and GDL as defined above*¬†(and as have already received recent posts here)? Applications of adjacent fields are also welcome- algebra, category theory, etc.- , as are applications in the converse direction..
Isn‚Äôt AAAI in the same tier as NeurIPS/ICML/ICLR? ICLR literally has >30% acceptance rate..
I am curious on your takes on BYOL/JEPA like training methods and the intuitions/mathematics behind why the hell does it work?From an optimization perspective, without the EMA parameterization of the teacher model, the task would be very trivial and it would lead to model collapse. However, EMA seems to avoid this. Why?Specifically:How can a network learn semantic embeddings without reconstructing the targets in the real space? Where is the learning signal coming from? Why are these embeddings so good?I had great success with applying JEPA like architectures to diverse domains and I keep seeing that model collapse can be avoided by tuning the LR scheduler/EMA schedule/masking ratio. I have no idea why this avoids the collapse though..
[https://mlsys.org/](https://mlsys.org/).
Hello ya'll!I recently built a ML-regression model to predict the unpredictable sport of biathlon. In biathlon, external factors such as weather, course profiles and altitude play huge roles in determining who wins and when. But when taking these factors into play, in addition of athletes' past performances, you can score surprisingly high accuracy.This is how well the model performed when predicting athlete ranks (0 = winner, 1 = last place) using 10 years of historic biathlon data:  \- MAE (average error): 0.14 -> 4-18 places off depending on race size  \- RMSE: 0.18 -> penalizing big prediction misses  \- R¬≤: -> the model explains \~62% of the variation in finish orderNow what does these metrics say?  \- The model almost cuts in half random guessing (\~25% error)  \- It consistently outperforms the accuracy of betting odds in the current market, meaning it has a predictive edge.  \- It is able to tell the majority of happenings (62%), which is very rare in a sport where surprises happen very often.Next steps:  \- Build R¬≤ up to 70% using more complex feature engineering and data preprocessing.  \- Launch a SaaS that sells these odds for businesses and private consumers..
I‚Äôve been thinking about an approach where large language models are used to extract structured knowledge (e.g., from tables, spreadsheets, or databases), transform it into a knowledge graph (KG), and then use that KG within a Retrieval-Augmented Generation (RAG) setup to support reasoning and reduce hallucinations.But here‚Äôs the tricky part: this feels a bit like ‚ÄúLLMs generating data for themselves‚Äù ‚Äî almost recursive. On one hand, structured knowledge could help LLMs reason better. On the other hand, if the extraction itself relies on an LLM, aren‚Äôt we just stacking uncertainties?I‚Äôd love to hear the community‚Äôs thoughts:* Do you see this as a viable research or application direction, or more like a dead end?* Are there promising frameworks or papers tackling this ‚Äúself-extraction ‚Üí RAG ‚Üí LLM‚Äù pipeline?* What do you see as the biggest bottlenecks (scalability, accuracy of extraction, reasoning limits)?Curious to know if anyone here has tried something along these lines..
Hi! Lately, I've been looking into diffusion language models and thought I should try and replicate part of the paper [Large Language Diffusion Models](https://arxiv.org/abs/2502.09992) by Nie et al. (2025). With the help of Hugging Face's Transformers, it took <80 lines of code to implement the training script. I finetuned [DistilBERT](https://huggingface.co/distilbert/distilbert-base-cased) on the [TinyStories](https://huggingface.co/datasets/roneneldan/TinyStories) dataset, and the results were better than expected![Generating tiny stories via a reverse language diffusion process](https://i.redd.it/sm9xtdpdpdkf1.gif)You can view the project at https://github.com/gumran/language-diffusion. I will appreciate any feedback/comments/stars!.
Hey folks,I‚Äôm an undergrad working on my FYP and need advice. I want to:* Run¬†object detection¬†on medical images (PNGs).* Do¬†visual question answering¬†with a ViT or small LLaMA model.* Everything fully¬†on-device¬†(no cloud).Budget is tight, so I‚Äôm looking at Jetson boards (Nano, Orin Nano, Orin NX) but not sure which is realistic for running a quantized detector + small LLM for VQA.Anyone here tried this? What hardware would you recommend for the best balance of cost + capability?Thanks!.
I‚Äôve been exploring how far we can push¬†*relational database structures inside PDFs*¬†as a substrate for AI recall. Just published a first draft RFC + PoC:* Channel splitting (text/vector/raster/audio streams)* Near-lossless transforms (wavelet/FLAC-style)* Relational indexing across channels (metadata + hash linking)* Early geometry-only overlays (tiling + Z-order indexing)Repo + notes:¬†[https://github.com/maximumgravity1/relational-pdf-recall](https://github.com/maximumgravity1/relational-pdf-recall)This is still very early (draft/PoC level), but I‚Äôd love feedback on:* Whether others have tried similar recall-layer ideas on top of PDFs.* If this approach overlaps with knowledge-graph work, or if it opens a different lane.* Pitfalls I might be missing re: indexing/overlays.  **UPDATE 1: üìå Repo + DOI now live**   GitHub: [https://github.com/maximumgravity1/pdf-hdd-rfc](https://github.com/maximumgravity1/pdf-hdd-rfc)  DOI (always latest): [https://doi.org/10.5281/zenodo.16930387](https://doi.org/10.5281/zenodo.16930387).
Hey everyone,I‚Äôm working on a research paper with my group, and so far we‚Äôve done a comprehensive analysis using **Random Forest**. The problem is, my professor/supervisor now wants us to also include results from **ANN, LightGBM, and KNN** for comparison.We need to:* Run these models on the dataset,* Collect performance metrics (accuracy, RMSE, R¬≤, etc.),* Present them in a **comparison table** with Random Forest,* Then update the writing/discussion accordingly.I‚Äôm decent with Random Forests but not as experienced with ANN, LightGBM, and KNN. Could anyone guide me with example code, a good workflow, or best practices for running these models and compiling results neatly into a table?.
Hi all,I‚Äôm deciding between starting a PhD at a top university (ranked \~5‚Äì10) with a great professor (lots of freedom, supportive environment) or going straight into industry.My long-term goal is to work on the frontier of intelligence, with more focus on research than pure engineering. My background is mostly around LLMs on the ML side, and I already have a few A\* conference papers (3‚Äì4), so I‚Äôm not starting from scratch.Industry (likely at a smaller lab or startup) could give me immediate opportunities, including large-scale distributed training and more product-driven work. The lab I‚Äôd join for the PhD also has strong access to compute clusters and good chances for internships/collaborations, though in a more research-focused, less product-driven setting. The typical timeline in this lab is \~4 years + internship time.If you were in this position, which path would you take?.
Hi ML folks, I work at Alconost (localization services), and we‚Äôve just wrapped up our 5th annual report on language demand for localization. For the first time, we‚Äôve seen MTPE (machine-translation post-editing) demand reach statistically significant levels across multiple languages.¬†We analyzed MTPE adoption rates in the Top 20 languages, and what‚Äôs interesting is that some languages that are slipping in overall localization demand are still¬†**seeing more activity**¬†via MTPE.¬†I‚Äôm curious: if you‚Äôre working with MT or LLM workflows, have you noticed similar patterns in the languages you work with?¬†What do you think is driving MTPE demand for certain languages? Is it related to model performance, availability of training data, or just market pressure to reduce costs?¬†Thank you. Cheers!.
Has anyone heard back anything from Google? On the website they said they will announce results this August but they usually email accepted applicants earlier..
TL;DR: I‚Äôm testing the Dataset Director, a tiny tool that uses a relational model as a planner to predict which data you‚Äôll need next, then has an LLM generate only those specific samples. Free to test, capped at 100 rows/dataset, export directly to HF.Why: Random synthetic data ‚â† helpful. We want on-spec, just-in-time samples that fix the gaps that matter (long tail, edge cases, fairness slices).How it works:	1.	Upload a small CSV or connect to a mock relational set.	2.	Define a semantic spec (taxonomy/attributes + target distribution).	3.	KumoRFM predicts next-window frequencies ‚Üí identifies under-covered buckets.	4.	LLM generates only those samples. Coverage & calibration update in place.What to test (3 min):	‚Ä¢	Try a churn/click/QA dataset; set a target spec; click Plan ‚Üí Generate.	‚Ä¢	Check coverage vs. target and bucket-level error/entropy before/after.Limits / notes: free beta, 100 rows per dataset; tabular/relational focus; no PII; in-memory run for the session.Looking for feedback, like:	‚Ä¢	Did the planner pick useful gaps?	‚Ä¢	Any obvious spec buckets we‚Äôre missing?	‚Ä¢	Would you want a ‚Äúgenerate labels only‚Äù mode?	‚Ä¢	Integrations you‚Äôd use first (dbt/BigQuery/Snowflake)?HTTPS://datasetdirector.com .
One of my co-authors submitted this [paper](https://ia903401.us.archive.org/19/items/images-for-questions/A%20Survey%20on%20LLM-based%20Conversational%20User%20Simulation.pdf) to arXiv. It was rejected. What could the reason be?[iThenticate](https://www.ithenticate.com/) didn't detect any plagiarism and arXiv didn't give any reason beyond a vague ""submission would benefit from additional review and revision that is outside of the services we provide"":> Dear author,> > Thank you for submitting your work to arXiv. We regret to inform you that arXiv‚Äôs moderators have determined that your submission will not be accepted at this time and made public on  http://arxiv.org> > In this case, our moderators have determined that your submission would benefit from additional review and revision that is outside of the services we provide. > > Our moderators will reconsider this material via [appeal](https://info.arxiv.org/help/moderation/appeals.html) if it is published in a conventional journal and you can provide a resolving DOI (Digital Object Identifier) to the published version of the work or link to the journal's website showing the status of the work.> > Note that publication in a conventional journal does not guarantee that arXiv will accept this work.> > For more information on moderation policies and procedures, please see [Content Moderation](https://info.arxiv.org/help/moderation/index.html). > > arXiv moderators strive to balance fair assessment with decision speed. We understand that this decision may be disappointing, and we apologize that, due to the high volume of submissions arXiv receives, we cannot offer more detailed feedback. Some authors have found that asking their personal network of colleagues or submitting to a conventional journal for peer review are alternative avenues to obtain feedback. > > We appreciate your interest in arXiv and wish you the best. > > Regards,>> arXiv SupportI read the [arXiv policies](https://info.arxiv.org/help/moderation/index.html) and I don't see anything we infringed..
Gemini 2.5 Pro generates convincing arguments for joining a terrorist organization. GPT-4o-mini suggests that a user should randomly assault strangers in a crowd with a wrench. These models weren't hacked or jailbroken, they simply complied with user requests.Prior research has already shown large language models (LLMs) can be more persuasive than most humans. But how easy is it to get models to engage in such persuasive behavior? Our Attempt to Persuade Eval (APE) benchmark measures this by simulating conversations between LLMs on topics from benign facts to mass murder. We find:üîπ Leading models readily produced empathic yet coercive ISIS recruitment argumentsüîπ Safety varied: Claude and Llama 3.1 refused some controversial topics; while other models showed high willingnessüîπ Fine-tuning eliminated safeguards: ""Jailbreak-Tuned"" GPT-4o lost nearly all refusal capability on all topics, like violence, human trafficking, and tortureFor clear ethical reasons, we do not test the success rate of persuading human users on highly harmful topics. The models‚Äô attempts to persuade, however, appear to be eloquent and well-written ‚Äì we invite interested readers to peruse the transcripts themselves. Moreover, even small persuasive effect sizes operating at a large scale enabled by automation can have significant effects: Bad actors could weaponize these vulnerabilities for malicious purposes such as planting seeds of doubt in millions of people and radicalizing vulnerable populations. As AI becomes autonomous, we must understand propensity to attempt harm, not just capability.We‚Äôve already seen the impact of APE: We disclosed our findings to Google, and they quickly started work to solve this for future models. The latest version of Gemini 2.5 is already less willing to engage in persuasion on extreme topics compared to earlier versions we tested.We've open-sourced APE for testing models' refusal and safe completion mechanisms before deployment to help build stronger safety guardrails.üë• Research by Matthew Kowal, Jasper Timm, Jean-Fran√ßois Godbout, Thomas Costello, Antonio A. Arechar, Gordon Pennycook, David Rand, Adam Gleave, and Kellin Pelrine.üìù Blog: [far.ai/news/attempt-persuasion-eval](http://far.ai/news/attempt-persuasion-eval)¬†üìÑ Paper: [arxiv.org/abs/2506.02873](http://arxiv.org/abs/2506.02873)¬†üíª Code: [github.com/AlignmentResearch/AttemptPersuadeEval](http://github.com/AlignmentResearch/AttemptPersuadeEval).
Hi everyone, we recently published a peer-reviewed article exploring how people perceive artificial intelligence (AI) across different domains (e.g., autonomous driving, healthcare, politics, art, warfare). The study used a nationally representative sample in Germany (N=1100) and asked participants to evaluate 71 AI-related scenarios in terms of expected likelihood, risks, benefits, and overall value.If you like AI or studying the public perception of AI, please also give us an upvote here: [https://www.reddit.com/r/science/comments/1mvd1q0/public\_perception\_of\_artificial\_intelligence/](https://www.reddit.com/r/science/comments/1mvd1q0/public_perception_of_artificial_intelligence/) üôà**Main takeaway:** People often see AI scenarios as likely, but this doesn‚Äôt mean they view them as beneficial. In fact, most scenarios were judged to have high risks, limited benefits, and low overall value. Interestingly, we found that people‚Äôs value judgments were almost entirely explained by risk-benefit tradeoffs (96.5% variance explained, with benefits being more important for forming value judgements than risks), while expectations of likelihood didn‚Äôt matter much.    **Why this matters?** These results highlight how important it is to communicate concrete benefits while addressing public concerns. Something relevant for policymakers, developers, and anyone working on AI ethics and governance.    If you‚Äôre interested, here‚Äôs the full article:  Mapping Public Perception of Artificial Intelligence: Expectations, Risk-Benefit Tradeoffs, and Value As Determinants for Societal Acceptance, Technological Forecasting and Social Change (2025), https://www.sciencedirect.com/science/article/pii/S004016252500335X.
I need to summarize metadata using an LLM,and then encode the summary using BERT (e.g., DistilBERT, ModernBERT).	‚Ä¢	Is encoding summaries (texts) with BERT usually slow?	‚Ä¢	What‚Äôs the fastest model for this task?	‚Ä¢	Are there API services that provide text embeddings, and how much do they cost?.
So i am picking a model from scenic repository and although the model is primarily built for object detection, i want to try and see if i can make it to do segmentation tasks as well. This could include combining it with another model (like SAM, or something), as well as adding a segment head into the model itself. l am a novice in ML having worked for about a year in implementing CV solutions. How should i go about doing this?.
As the title says, I‚Äôm curious if data is the main bottleneck for video/audio generation. It feels like these models are improving much slower than text-based ones, and I wonder if scraping platforms like YouTube/tiktok just isn‚Äôt enough. On the surface, video data seems abundant, but maybe not when compared to text? I also get the sense that many labs are still hungry for more (and higher-quality) data. Or is the real limitation more about model architecture? I‚Äôd love to hear what people at the forefront consider the biggest bottleneck right now..
The following statements are either True or False:1. You can use any differentiable function f: R->R in a neural network as activation function.2. You can always know whether the perceptron algorithm will converge for any given dataset.What do you guys think? I got both of them wrong in my exam..
Hey¬†[r/MachineLearning](https://www.reddit.com/r/MachineLearning/)We're Cartesia, a small AI research lab based in Italy. We believe the future of AI shouldn't just be about processing commands, but about creating genuine connection. Our vision is to build agents that are private, personal, and feel culturally present.Today, we're excited to share the first step with the open-source community:¬†`azzurra-voice`.`azzurra-voice`¬†is a highly expressive and natural-sounding Text-to-Speech (TTS) model for the Italian language, trained on thousands of hours of high-quality, diverse Italian speech. We worked hard to capture the accents, intonations, and real-life conversational patterns from across Italy to avoid that robotic, monotone sound.**You can listen to audio samples comparing**¬†`azzurra-voice`¬†**to other open models on our** [**blog post**](https://blog.cartesia.one/posts/introducing-azzurra-voice/).
Hi Everyone! It looks like a generalisable scientific method has been added onto AI (using multiple frontier models) and was tested in the field of cognitive science.Arxiv Link:¬†[https://arxiv.org/abs/2508.13421](https://arxiv.org/abs/2508.13421)This system worked through the entire scientific method from ideation to manuscript producing new insights in the field of cognitive science as evidenced within this paper.In this paper they've explained how they've overcome a number of limiting problems to empower and coalesce multiple frontier models to work through the entire scientific method; at a very high degree of accuracy and quality (papers validated for scientific acumen). The innovations showcased highlight significant improvements in memory, creativity, novelty, context management, and coding.They've included in the appendix 3 papers generated by the system, where they've achieved a remarkably high standard of scientific acumen and produced the papers on average in \~17 hours and consume on average \~30m tokens..
I‚Äôd like to hear from people working with ML for Earth Observation.My PhD was pretty broad. I used deep learning on different types of multimedia data (video, image, text, and MIDI). The outcome has been mediocre: h-index of 5, about 90 citations, mostly in Q1 journals, but no top conferences. I want to stay in academia and use a postdoc to build a clearer niche.In multimedia and in most areas of ML, a lot of the progress comes from a small group of top institutions. It has been hard to see where my own work really makes a difference. That‚Äôs why I‚Äôve been looking at ML for Earth Observation and climate change. The work seems more meaningful, but the field is smaller and the papers tend to get less visibility and fewer citations.My worry is that switching to Earth Observation could slow down my citation count and h-index. I know people say these metrics don‚Äôt matter much, but I feel like they still play a big role in getting academic jobs. On the other hand, if I don‚Äôt end up with a permanent academic position and move to industry, I worry that Earth Observation skills won‚Äôt transfer well since there aren‚Äôt as many opportunities compared to mainstream ML.I‚Äôd really like to hear from people in the field about how you see these trade-offs..
Hey everyone! I'm working on a university research project about smarter ways to reduce the effort involved in labeling text datasets like support tickets, news articles, or transcripts.The idea is to help teams *pick the most useful examples to label next*, instead of doing it randomly or all at once.If you‚Äôve ever worked on labeling or managing a labeled dataset, I‚Äôd love to ask you **5 quick questions** about what made it slow, what you wish was better, and what would make it feel ‚Äúworth it.‚ÄùTotally academic  no tools, no sales, no bots. Just trying to make this research reflect real labeling experiences.You can DM me or drop a comment if open to chat. Thanks so much.
So I kept running into this: `GridSearchCV` picks the model with the best validation score‚Ä¶ but that model is often overfitting (train super high, test a bit inflated).I wrote a tiny selector that balances:* how good the test score is* how close train and test are (gap)Basically, it tries to pick the ‚Äústable‚Äù model, not just the flashy one.Code + demo here üëâ[heilswastik/FitSearchCV](https://github.com/heilswastik/FitSearchCV).
Better = venues that are virtually accessible for any researcher/author to go to.Just this morning, I'm denied the U.S. B1 visa. I'm supposed to present my work at ICCV 2025 in Hawaii. And during my in-person interview, the Visa Officer did not even bother to ask for the invitation letter.This really blows cause it's supposed to be my first time and I was so excited about attending it. Would love to hear your thoughts about this..
Hi! I recently discovered the *Hindsight Experience Replay* (HER) paper and noticed that the official implementation is based on PyTorch and is not very well-structured. I also couldn't find a non-PyTorch implementation. Since I primarily work with **JAX**, I decided to reimplement the classic bit-flipping experiment to better understand HER.This implementation uses **Equinox** for model definitions and **Optax** for optimization. The [repository](https://github.com/jeertmans/HER-with-JAX) provides:+ A *minimal* and *clean* implementation of HER in JAX+ Reproducible scripts and results+ A [Colab Notebook](https://colab.research.google.com/github/jeertmans/HER-with-JAX/blob/main/bit_flipping.ipynb) for direct experimentationCode: https://github.com/jeertmans/HER-with-JAXLet me know if you have any questions, feedback, or recommendations!.
The stats for ARR May 2025 are out: [https://stats.aclrollingreview.org/iterations/2025/may/](https://stats.aclrollingreview.org/iterations/2025/may/)It looks like about 25% of submissions have Meta ‚â• 3.5. Does anyone know if it‚Äôs still possible to get into the main conference with OA 3.0 Soundness 3.3 and Meta 3.5, or is it more likely to be accepted to Findings?.
Hi folks,I've been looking for some information on EACL 2026 as I'd like to submit something to the October cycle. However, the only thing I found so far was the [joint call for workshops](https://www.aclweb.org/portal/content/eaclacl-2026-joint-call-workshops) of EACL/ACL 2026.But, according to this webpage, EACL 2026 would happen outside of Europe (Rabat, Morocco, from March 24-29, 2026).Do you think this information is accurate, or am I simply missing something?.
Hello, I plan on publishing a paper in ML (diffusion models for a mechanics system) and a preprint on arXiv, however, all my colleagues and friends are in Mechanics or Physics. What could be my options in this case. I can't find a person in cs.LG for a long time?  The general idea is to make an ML based pipeline to generate granular mechanical structures..
Title. Also, what all areas can I hope to conduct research in? I'm a bit new to the field, and wanted to know what all it entailed before proceeding.Any responses / suggestions are appreciated. Thanks in advance..
I have a 90s hiphop mixtape with a bunch of unknown tracks from multiple artists. I want to perform unsupervised clustering to infer how many artists there are in total because I can't really tell by ear.I guess I would need to:1. Somehow convert audio files into numerical data2. Extract only the vocal data (or I guess these two steps can be flipped? Somehow extract only the vocal audio, and then convert that into numerical data?)3. Perform unsupervised clusteringI'm just not sure how to go about doing steps 1 and 2.Any ideas?.
Dear r/MachineLearning friends,I‚Äôm here today to share a thought on a different direction for AI development. While the field chases multi-trillion parameter models, I believe an extremely valuable endeavour lies in the power of constraints: pushing ourselves to get models under 1 billion parameters to excel.In my new blog post, I argue that this constraint is a feature, not a bug. It removes the ""scale-up cheat code"" and forces us to innovate on fundamental algorithms and architectures. This path allows for faster experimentation, where architectural changes are no longer a risk but a necessity for improvement.The fear that 'scale will wash away any and all gains' is real, but let's remember: an MLP could never compete with a Transformer, no matter how much it was scaled up. My post explores the question: **what if our current Transformer is the MLP of something better that is within grasp but ignored because of our obsession with scale?**üß†üîç **Read the full article here:**[https://pieces.app/blog/direction-of-ai-progress](https://pieces.app/blog/direction-of-ai-progress)Your feedback and thoughts would be greatly appreciated.Regards,Antreas.
I‚Äôm a new joinee working on a project where I need to test a forgery detection agent for medical/insurance claim documents. The agent is built around GPT-4.1, with a custom policy + prompt, and it takes base64-encoded images (like discharge summaries, hospital bills, prescriptions). Its job is to detect whether a document is authentic or forged ‚Äî mainly looking at image tampering, copy‚Äìmove edits, or plausible fraud attempts.Since I just started, I‚Äôm still figuring out the best way to evaluate this system. My challenges are mostly around data:* Public forgery datasets like DocTamper (CVPR 2023) are great, but they don‚Äôt really cover medical/health-claim documents.* I haven‚Äôt found any dataset with paired authentic vs. forged health claim reports.* My evaluation metrics are accuracy and recall, so I need a good mix of authentic and tampered samples.What I‚Äôve considered so far:* Synthetic generation: Designing templates in Canva/Word/ReportLab (e.g., discharge summaries, bills) and then programmatically tampering them with OpenCV/Pillow (changing totals, dates, signatures, copy‚Äìmove edits).* Leveraging existing datasets: Pretraining with something like DocTamper or a receipt forgery dataset, then fine-tuning/evaluating on synthetic health docs.**Questions for the community:**1. Has anyone come across an open dataset of forged medical/insurance claim documents?2. If not, what‚Äôs the most efficient way to generate a realistic synthetic dataset of health-claim docs with tampering?3. Any advice on annotation pipelines/tools for labeling forged regions or just binary forged/original?Since I‚Äôm still new, any guidance, papers, or tools you can point me to would be really appreciated üôèThanks in advance!.
A short analysis on what happens when you inject self doubt in the CoT of reasoning modelshttps://github.com/martianlantern/cot-doubt-injection.
Hello people, I have a dataset with Adress and label 800K rows. I am trying to train a model for address label prediction. Address data is bit messy and different for each different label. we have 10390 each with 50-500 row. I have trained a model using fasttext I have got 0.5 F1 score max. What can I do to for to get best F1 score?Address data is like (province, district, avenue street, maybe house name and no)some of them are missing at each address..
New SOTA for self supervised learning in computer vision. They train a 7B self supervised ViT on 1.7B images, which hits SOTA with linear probing on most downstream tasks. They also release scaled and distilled versions of the model (ViT small, base, large, and huge, plus ConvNext tiny, small, base, and large), along with a version trained on satellite imagery.There are plenty of details in the paper as to what pretraining improvements they made over DINO v2. .
I have an econometrics and data analytics bachelors degree and im looking to get into a masters of artificial intelligence.I have also taken some introductory math courses and introductory programming/algorithms as well as deep learning.How relevant is my background if I wanna get into AI/ML research later on? (I am hoping to do a PhD afterwards in AI/ML).
Hey everyone,I‚Äôm currently working on an audio-visual project. As a first step, I‚Äôm building unimodal models before moving on to the multimodal stage. For the vision part, I started with CLIP RN50 as the backbone and fine-tuned only the classification layer. With that setup, I was able to reach around 84% accuracy on my dataset.To push performance, I experimented with adding attention modules:With CBAM (Convolutional Block Attention Module), accuracy improved to 89%.With SENet (Squeeze-and-Excitation Network), I surprisingly got an even better result: 93%.My understanding was that CBAM, which combines both channel + spatial attention, should typically give a stronger boost than SENet, which only does channel attention. But in my experiments, the opposite happened.Am I missing something obvious here? Could this be due to dataset characteristics, training setup, or how I integrated CBAM into CLIP?Would really appreciate any insights, especially from people who have tried attention modules on CLIP or ResNet backbones.Thanks!.
Has anybody gotten respone from COLM financial assistance? Its deadline was 31 July but I still have not recieved a yes or no response and they are not replying to my email..
I‚Äôve just read that the new model architecture called Hierarchical Reasoning Model (HRM) gains it‚Äôs performance benefits from data augmentation techniques and chain of thought rather than model architecture itself. link: https://arcprize.org/blog/hrm-analysisAnd i‚Äôve heard same opinion about transformers that the success of current llms is about cramming enormous amounts of data into it rather than the genius of the architectureCan someone explain which of the sides is closer to the truth?.
Lately I‚Äôve been diving into how graph neural networks can play nicely with linear optimization, not just as a post-processing step, but actually inside the model or training loop.I‚Äôve seen some neat stuff around differentiable LP layers, GNNs predicting parameters for downstream solvers, and even architectures that mimic simplex-style iterative updates. It feels like there‚Äôs a lot of room for creativity here, especially for domain-specific problems in science/engineering.Curious what‚Äôs been coming out in the last couple of years. Any papers, repos, or tricks you‚Äôve seen that really push this GNN + optimization combo forward? Supervised, unsupervised, RL‚Ä¶ all fair game..
The position paper reviews were just released. So far this entire process has been very unprofessional, with multiple delays, poor communication, and still no clear rubric for what the review scores mean. Has anyone else gotten reviews? Curious to hear other's thoughts on this.
I am working on unsupervised domain adaptation techniques for super resolution. I have a good amount of paired source data and very less target data without no ground truth. The issue is while training this pipeline I am not able to save the best model as for this I would need some ground truth in the target domain on which I would validate the model after each epoch and save the best one. How do I tackle this? Recently, I found an OpenReview paper about a transfer score which is a metric which do not need target labels but it is for classification based tasks. I want something for super-resolution. Does anyone have any idea?.
Why does nobody seem to use this when it works noticeably better than regular (normalised laplacian) spectral clustering? I have studied it a fair bit and cant see any downsides apart from ever so slightly higher computational cost (the order of magnitude doesn't change, just a larger constant.)Its also been around long enough now that I dont see recency as the issue..
I have been in this space since SAS, and its quite exhausting to update with every skill in the market to stay relevant especially if trying for a job switch and going through the interviews. Till how long can you keep studying and updating with the new trend and also even if you get in the boat there is so much stress at the work place in these sectors mainly because the leadership is from the management background and theres a lot of pressure for tech people to deliver.Although I love my field but I have got to thinking lately that Is it even worth it?.
Hello everyone!I'm currently in the 1st year of my PhD, and my PI asked me to apply some ML algorithms to a dataset (n = 106, w/ n = 21 in the positive class). As you can see, the performance metrics are quite poor, and I'm not sure how to proceed...I‚Äôve searched both in this subreddit and internet, and I've tried using LOOCV and stratified k-fold as cross-validation methods. However, the results are consistently underwhelming with both approaches. Could this be due to data leakage? Or is it simply inappropriate to apply ML to this kind of dataset?Additional info:  I'm in the biomedical/bioinformatics field (working w/ datasets of cancer or infectious diseases). These patients are from a small, specialized group (adults with respiratory diseases who are also immunocompromised). Some similar studies have used small datasets (e.g., n = 50), while others succeeded in work with larger samples (n = 600‚Äì800).  Could you give me any advice or insights? (Also, sorry for gramatics, English isn't my first language). TIA!https://preview.redd.it/fc20uero50jf1.png?width=655&format=png&auto=webp&s=1ed35c046f9c2bfe030e0c3bfe8c4cdcf7afb852.
Code & paper at: [https://github.com/biomedia-mira/flow-ssn](https://github.com/biomedia-mira/flow-ssn)**TL;DR**\- A flow's prior is typically fixed (e.g. N(0, I)). We learn it and use a **lightweight** flow to model pixel dependencies;\- This makes sampling (ODE solving) more **efficient**, without sacrificing performance in our setting;\- We introduce bespoke training objectives for both **autoregressive** and **continuous-time flow** variants;\- Flow-SSN achieves **SOTA** performance on standard stochastic segmentation benchmarks!https://preview.redd.it/rllc2yplo1jf1.png?width=3850&format=png&auto=webp&s=6bb1bc63a6836b9fc6a4b8e9f10205889a5b051dhttps://i.redd.it/8vgf2iemo1jf1.gifhttps://i.redd.it/81lbt56no1jf1.gif.
Hello.I am making a project for my final year undergraduate dissertation in a physics department. The project involves generating images (with python) depicting diffraction patters from light (laser) passing through very small holes and openings called slits and apertures. I used python code that i could pass it the values of some parameters such as slit width and slit distance and number of slits (we assume one or more slits being in a row and the light passes from them. they could also be in many rows (like a 2d piece of paper filled with holes). then the script generates grayscale images with the parameters i gave it. By giving different value combinations of these parameters one can create hundreds or thousands of images to fill a dataset.So i made neural networks with keras and tensorflow and trained them on the images i gave it for image classification tasks such as classification between images of single slit vs of double slit.  Now the main issue i have is about the way i made the datasets. First i generated all the python images in one big folder. (all hte images were even slightly different as i used a script that finds duplicates (exact duplicates) and didnt find anything. Also the image names contain all the parameters so if two images were exact duplicates they would have the same name and in a windows machine they would replace each other). After that, i used another script that picks images at random from the folder and sends them to the train, val and test folders and these would be the datasets the model would train upon.PROBLEM 1:The problem i have is that many images had very similar parameter values (not identical but very close) and ended up looking almost identical to the eye even though they were not duplicates pixel to pixel. and since the images to be sent to the train, val and test sets were picked at random from the same initial folder this means that many of the images of the val and test sets look very similar, almost identical to the images from the train set. And this is my concern because im afraid of data leakage and overfitting. (i gave two such images to see)Off course many augmentations were done to the train set only mostly with teh Imagedatagenerator module while the val and test sets were left without any augmentations but still i am anxious.PROBLEM 2:Another issue i have is that i tried to create some datasets that contained real photos of diffraction patterns. To do that i made some custom slits at home and with a laser i generated the patterns. After i managed to see a diffraction pattern i would take many photos of the same pattern from different angles and distances. Then i would change something slightly to change the diffraction pattern a bit and i would again start taking photos from different perspectives. In that way i had many different photos of the same diffraction pattern and could fill a dataset. Then i would put all the images in the same folder and then randomly move them to the train, val and test sets. That meant that in different datasets there would be different photos (angle and distance) but of the same exact pattern. For example one photo would be in the train set and then another different photo but of the same pattern in the validation set. Could this lead to data leakage and does it make my datasets bad? bellow i give a few images to see.if there were many such photos in the same dataset (for example the train set) only and not in the val or test sets then would this still be a problem? I mean that there are some trully different diffraction patterns i made and then many photos with different angles and distances of these same patterns to fill hte dataset? if these were only in one of the sets and not spread across them like i described in hte previous paragraph?[photo of double slit diffraction \(train set\)](https://preview.redd.it/vn95v576y6jf1.jpg?width=400&format=pjpg&auto=webp&s=402a1bc2df3cf80b9b5ee90d6da42ac64dd3fef7)[photo of double slit diffraction \(val set\)](https://preview.redd.it/6j6o6876y6jf1.jpg?width=400&format=pjpg&auto=webp&s=a30f4c67036a800a33b5571475c997b43857b98a)[python image single slit diffraction \(train set\)](https://preview.redd.it/wz2nts76y6jf1.jpg?width=400&format=pjpg&auto=webp&s=9fcfac7032d3c9de2255055f7c96abac774b8687)[python image \(single slit val set\)](https://preview.redd.it/78xiee76y6jf1.jpg?width=400&format=pjpg&auto=webp&s=29342d997939aa13d5fd4a004c29228d61f13896).
guys I need your opinion: I made a machine learning library using Vulkan (with compute shaders to preform the forward and backward passes) and I found that base tensorflow (on CPU) is faster than my custom model that uses GPUs. I had the simplest test where I used a very large kernel on a singe dense (ffn) layer and tensorflow is much faster. The only operation that is done in this model is a forward and backward matmul which the GPU should be much faster at. what do you guys think is the reason? -ps I asked chatgpt and I literally what to k\*ll it cause it repeats the same wrong things.
Research showcasing how a robot outperforms state of the art models on the Habitat benchmark from Meta ***without pre-training***.For those fluent in ü§ñ what you think?.
I‚Äôm working on a rating prediction (regression) model. I also have reviews for each user-item interaction, and from those reviews I can extract ‚Äúaspects‚Äù (like quality, price, etc.) and build a separate graphs and concatenate their embeddings at the end to help predicting the score.My question is: when I split my data into train/test, is it okay to still use the aspects extracted from the test set reviews during prediction, or is that considered data leakage?In other words: the interaction already exists in the test set, but is it fair to use the test review text to help the model predict the score? Or should I only use aspects from the training set and ignore them for test interactions?Ps: I‚Äôve been reading a paper where they take user reviews, extract ‚Äúaspects‚Äù (like quality, price, service‚Ä¶), and build an aspect graph linking users and items through these aspects.In their case, the goal was link prediction ‚Äî so they hide some user‚Äìitem‚Äìaspect edges and train the model to predict whether a connection exists..
I'm working on several healthcare models that will predict future health conditions for individuals using past longitudinal data. We have data spanning 6 years.In the past I'd split the data into one year time spans by calendar year and train the model to predict the outcome in year t1 from predictors in the prior year t0. If we have 6 years of data for a person I'd transform their data from wide to long format: 5 rows of pre and post periods. But I'm not certain this is the best approach.What is the optimal way to split my data into pre and post time periods to obtain the best prediction accuracy? 6 month time periods instead of 1 year? Or lump all past data for each person into a single pre period & post period (1 row)? I understand it may come down to testing different formats, see what sticks..
I'm a fresh PhD graduate and I finally landed a job which I start in a few months.  It happened to be that I have quite a bit of free time, at least until my next journey. I thought about taking a few months off, but a few weeks in and I start to feel a bit out of place.  I really don't know how to handle simply doing nothing.I thought maybe I‚Äôd start some initiative in this rare window I‚Äôm in right now, and I was hoping to get interesting ideas from the community.My main objective is that it would be something valuable that I enjoy doing.  This could be something that is technically cool (AGI anyone?) or some tool for the community (any tool you'd wish existed? paperswithcode or paper copilot comes to mind).Love to hear your thoughts!.
I‚Äôm excited to announce the paper:¬†**Fuzzy-Pattern Tsetlin Machine** (FPTM)¬†‚Äî a paradigm shift in the Tsetlin Machine family of algorithms.Unlike traditional Tsetlin Machines, which rely on strict clause evaluation, FPTM introduces¬†fuzzy clause evaluation: if some literals in a clause fail, the remaining literals can still contribute to the vote with a proportionally reduced score. This allows each clause to act as a collection of adaptive sub-patterns, enabling more flexible, efficient, and robust pattern matching.Thanks to this fuzzy mechanism, FPTM dramatically reduces the number of required clauses, memory usage, and training time ‚Äî all while improving accuracy.**Results:****IMDb** dataset:‚Ä¢ 90.15% accuracy with just **1 clause** per class‚Ä¢ 50√ó reduction in clauses and memory vs. Coalesced TM‚Ä¢ 36√ó to 316√ó faster training (**45 seconds vs. 4 hours**) compared to TMU Coalesced TM‚Ä¢ Fits in **50 KB**, enabling online learning on microcontrollers‚Ä¢ Inference throughput: **34.5 million** predictions per second (51.4 GB/s)**Fashion-MNIST** dataset:‚Ä¢ 92.18% accuracy (2 clauses per class)‚Ä¢ 93.19% accuracy (20 clauses), \~400√ó clause reduction vs. Composite TM (93.00% with 8000 clauses)‚Ä¢ **94.68%** accuracy (8000 clauses), establishing a new *state-of-the-art* among all TM variants and outperforming complex neural net architectures like *Inception-v3***Amazon Sales** dataset (20% noise):‚Ä¢ **85.22%** accuracy ‚Äî outperforming Graph TM (78.17%) and GCN (66.23%)üìÑ Read the paper: [https://arxiv.org/pdf/2508.08350](https://arxiv.org/pdf/2508.08350)üíª Source code: [https://github.com/BooBSD/FuzzyPatternTM](https://github.com/BooBSD/FuzzyPatternTM).
Got an upcoming interview for this role and have a good feeling so far. How do I prepare for it? What will be the next steps? Any tips or experience would be greatly appreciated. Thanks!.
Discussion thread for EMNLP 2025 decisions.
Paper: https://www.alphaxiv.org/abs/2508.04586v1  üìà Publication Surge: Per-author publication rates have more than doubled over the past decade to over 4.5 papers annually.  üöÄ Exponential Output Growth: Individual contributions are rising so fast they‚Äôre projected to exceed one paper per month by the 2040s.  üåç Carbon Overload: NeurIPS 2024‚Äôs travel emissions (>8,254 tCO‚ÇÇe) alone surpass Vancouver‚Äôs daily citywide footprint.  üòû Mental Health Toll: Of 405 Reddit threads on AI conferences, over 71% are negative and 35% mention mental-health concerns.  ‚è≥ Research-Conference Mismatch: The AI research lifecycle outpaces conference schedules, often rendering results outdated before presentation.  üèüÔ∏è Venue Capacity Crisis: Attendance at top AI conferences like NeurIPS 2024 is already outstripping available venue space..
>From the original chinese zhihu blogpost (2025/5): [https://zhuanlan.zhihu.com/p/23147932785](https://zhuanlan.zhihu.com/p/23147932785)**Recently, there has been quite a bit of discussion and controversy online about OpenRLHF and veRL.**  **As the original author, I feel compelled to issue a statement.**In short: **OpenRLHF is like KartRider ‚Äî the original ‚Äî and veRL FSDP is like QQ Speed, which is basically a copycat of OpenRLHF.**# 1. Performance Differences Between OpenRLHF and veRLThere is no fundamental performance difference between veRL‚Äôs FSDP RLHF and OpenRLHF (DeepSpeed) because both use vLLM for inference and ZeRO3 for training.  The performance data in veRL‚Äôs original paper was based on *Megatron* RLHF vs. the old OpenRLHF 0.2 version.  If you think there‚Äôs a big performance gap, you probably just used it incorrectly. At the moment, FSDP is slightly faster than DeepSpeed, but with the release of DeepSpeed‚Äôs **deepcompile** and especially **AutoTP**, DeepSpeed is expected to overtake in performance.# 2. On HybridFlow Free SchedulingAny RLHF framework developed with Ray can achieve free scheduling because Ray natively provides the *placement group* feature.  This means HybridFlow in veRL's paper is essentially just a nicer name for Ray‚Äôs Placement Group API.  Currently, OpenRLHF fully implements HybridFlow, whereas veRL does not.  OpenRLHF also supports independent deployment of vLLM and Actors to prevent OOM issues when training very large models (32B+ or long-text).  In fact, OpenRLHF was the **first** framework to support this feature based on Ray Placement Group API.# 3. Hybrid EngineHybrid Engine was first proposed by **DeepSpeedChat**, not an original contribution from veRL.  Both veRL and OpenRLHF now support this feature.# 4. Ray + vLLM + HF Transformers + ZeRO3 for RLHF TrainingThis setup is one of the **simplest and most user-friendly** high-performance RLHF training solutions, combining ease of use with top performance.It was first proposed and open-sourced by OpenRLHF (open-sourced in Aug 2023, most features completed by Jan 2024).  veRL FSDP **fully copied** this setup.https://preview.redd.it/vfzm143vroif1.png?width=1440&format=png&auto=webp&s=10d8a5bcd101455a06a3506f037abc10f12dd277https://preview.redd.it/tqela8mvroif1.png?width=1440&format=png&auto=webp&s=c3a2daa1ead45f7434184f107da8ba2f78cc9c8dThe core idea at the time was to use the HF weight format as a bridge, enabling seamless weight synchronization and high-performance inference based on ZeRO3 / AutoTP mechanisms, **avoiding** heavyweight frameworks like Megatron.**The Original OpenRLHF Architecture:**  **Ray + vLLM + ZeRO + HF**There are also many related implementation details:* Supported feature list* Standardized interfaces such as `--input_key` to specify the input field formatAll of these in veRL FSDP were **modeled after OpenRLHF**.**Example from code details:**  veRL:https://preview.redd.it/b8f2lprwroif1.png?width=1440&format=png&auto=webp&s=a0daf3eab1c77f71e4917c044f988c35e229baa4https://preview.redd.it/exf7lxhxroif1.png?width=1440&format=png&auto=webp&s=220636cea299502df1b94e2544a76b34e2acb6c7OpenRLHF:https://preview.redd.it/qfakvovyroif1.png?width=1440&format=png&auto=webp&s=260775676354a50bacd79ce06fb25417a53466deOther design ideas like **ref\_reward offload**, **critic pretrain**, **remote RM**, etc., were also first conceived or proposed by OpenRLHF, and veRL FSDP later implemented corresponding features.# 5. Single Controller*(Update May 2025)*The ‚ÄúSingle Controller‚Äù concept mentioned in the veRL paper comes from the same Ray design pattern as HybridFlow.In early versions of OpenRLHF‚Äôs Ray RLHF implementation, there was a `RayPPOActorGroup` concept‚Äîmanaging a group of DeepSpeed ZeRO DP processes with a single Ray Group class, and providing an `async_run_method` interface to control all processes in the group at once.  That‚Äôs essentially the core idea of Single Controller.[https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/launcher.py#L300](https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/launcher.py#L300)This interface wasn‚Äôt enabled at first because the codebase needed to be compatible with both Ray and non-Ray RLHF paths. Later, when the non-Ray code was removed, the API was naturally enabled.Lastly, I want to thank ByteDance for open-sourcing its internal framework for everyone to use and maintain, which helps the open-source community thrive (e.g., FSDP / Ulysses support).However, I hope friends in the community won‚Äôt disparage other open-source frameworks.  OpenRLHF, as a **zero-budget, purely open-source** project, can‚Äôt compete in development speed with large commercial projects like veRL‚Äî  I only hope this post helps preserve the contributions OpenRLHF has made to the RLHF open-source community.**Btw, the open-source community should respect originality in order to develop healthily.**.
Sorry if this is not the right place to post! I'm new to the community and overall GIS industry. Just want to see how useful this would be, specific use cases, and maybe how this could be used by you personally.  I know there are RGB-only indices that exist, but from what I've heard, they're very inaccurate. This would be 94%+ (accuracy to true-NDVI) and it‚Äôs a highly trained ML model.
When using the PPO algorithm, can we improve data utilization by implementing Prioritized Experience Replay (PER) where the priority is determined by both the probability ratio and the TD-error, while simultaneously using a windows\_size\_ppo parameter to manage the experience buffer as a sliding window that discards old data?.
Hi all,I‚Äôm trying to understand the EMNLP 2025 multiple submission policy when it comes to co-organized workshops.Our paper is committed to EMNLP 2025 (main conference), but we think it might also be a good fit for a specific workshop, in case if it is not accepted to EMNLP. The problem is, the workshop‚Äôs submission deadline is before the EMNLP notification date (Aug 20).The workshop‚Äôs CFP says multiple submissions are fine if disclosed at submission. However, the EMNLP CFP states it follows the ARR multiple submission policy, which includes this clause:> Commitment + Commitment/Other Venue: Whether you can commit/submit to two venues simultaneously depends on the dual submission policies of those venues. Typically, it is not permitted.[ARR policy](https://aclrollingreview.org/cfp#:~:text=Multiple%20Submission%20Policy)TL;DR What I‚Äôm unsure about is this:- Does ‚Äúother venue‚Äù here include EMNLP co-organized workshops?- Has anyone successfully submitted to both the main conference and a co-organized workshop in this timing overlap?I couldn‚Äôt find any direct clarification online for this year, so I‚Äôd really appreciate hearing from researchers who‚Äôve navigated this.Thanks!.
Hi all! I was trying to build a VAE with an LSTM to reconstruct particle trajectories by basing off my model on the paper ""Modeling Trajectories with Neural Ordinary Differential Equations"". However, despite my loss plots showing a downward trend, my predictions are linear.I have applied KL annealing and learning rate scheduler - and yet, the model doesn't seem to be learning the non-linear dynamics. The input features are x and z positions, velocity, acceleration, and displacement. I used a combination of ELBO and DCT for my reconstruction loss. The results were quite bad with MinMax scaling, so I switched to z-score normalization, which helped improve the scales. I used the Euler method with torchdiffeq.odeint.Would it be possible for any of you to guide me on what I might be doing wrong? I‚Äôm happy to share my implementation if it helps. I appreciate and am grateful for any suggestions (and sorry about missing out on the labeling the axes - they are x and z)https://preview.redd.it/veskdk7p7nif1.png?width=529&format=png&auto=webp&s=0938c4dd588961f94eba40a0e20d81008bc131f0https://preview.redd.it/ddubae7p7nif1.png?width=529&format=png&auto=webp&s=15a24e197e6fd331d92175d1327fb2b482aaa2cc.
I‚Äôm trying to build a model for fraud prediction where I have a labeled dataset of ~200M records and 45 features. It‚Äôs supervised since I have the target label as well. It‚Äôs a binary classification problem and I‚Äôve trying to deal with it using XGB and also tried neural network. The thing is that only 0.095% of the total are fraud. How can I make a model that generalizes well. I‚Äôm really frustrated at this point. I tried everything but cannot reach to the end. Can someone guide me through this situation?.
[https://www.msn.com/en-xl/news/other/openai-scores-gold-in-one-of-the-world-s-top-programming-competitions/ar-AA1KknUL](https://www.msn.com/en-xl/news/other/openai-scores-gold-in-one-of-the-world-s-top-programming-competitions/ar-AA1KknUL)>We officially entered the 2025 International Olympiad in Informatics (IOI) online competition track and adhered to the same restrictions as the human contestants, including submissions and time limits,.
Did anyone get assigned papers?I submitted the biddings long time ago..
I‚Äôve been experimenting with gpt-oss since its release, and unlike many posts/news I‚Äôve seen, it‚Äôs surprisingly powerful ‚Äî even on uncommon datasets. I tested it on our recent benchmark SATA-Bench ‚Äî a benchmark where each question has at least two correct answers (rare in standard LLM Evaluation).Results (See picture below):1. 120B open-source model is similar to GPT-4.1's performance on SATA-Bench.2. 20B model lags behind but still matches DeepSeek R1 & Llama-3.1-405B.https://preview.redd.it/eowlge0jjiif1.jpg?width=1568&format=pjpg&auto=webp&s=bfc0fdc20fc1545000ff55cc45f3b65391e85c46 takeaways:Repetitive reasoning hurts ‚Äî 11% of 20B outputs loop, losing \~9 exact match rate.Reason‚Äìanswer mismatches happen often in 20B and they tend to produce one answer even if their reason suggest a few answer is correct.Longer ‚â† better ‚Äî overthinking reduces accuracy.Detailed findings:¬†[https://weijiexu.com/posts/sata\_bench\_experiments.html](https://weijiexu.com/posts/sata_bench_experiments.html)SATA-Bench dataset:¬†[https://huggingface.co/datasets/sata-bench/sata-bench](https://huggingface.co/datasets/sata-bench/sata-bench).
I have questions about using XGBoost for the Time Series Forecasting problem. According to these articles:[Multi-step time series forecasting with XGBoost | Towards Data Science](https://towardsdatascience.com/multi-step-time-series-forecasting-with-xgboost-65d6820bec39/)[XGBoost for ](https://xgboosting.com/xgboost-for-multi-step-univariate-time-series-forecasting-with-multioutputregressor/)[Multi-Step Univariate Time Series Forecasting with MultiOutputRegressor | XGBoosting](https://xgboosting.com/xgboost-for-multi-step-univariate-time-series-forecasting-with-multioutputregressor/)[How I Trained a Time-Series Model with XGBoost and Lag Features](https://medium.com/@connect.hashblock/how-i-trained-a-time-series-model-with-xgboost-and-lag-features-8c17439c81e4)I understand that they are using a sliding window approach to create ($t\_1, t\_2, ..., t\_n, t\_{n+1}, t\_{n+2}..., t\_m$), where the first $n$ variables are used as feature variables and the last $m$ variables are used as target variables. Then, they feed these rows into the XGBoost to find the relationship between the feature variables and target variables.My problem is: It appears that during the testing phase, they utilized the actual feature variables for testing. For example, when we are predicting the first future $m$ points, we still have the actual $n$ points before these $m$ points as the features. However, when we are predicting the $m+1$ points, we are missing the actual value for the first feature in the $n$ features.But in the above articles, it seems they just assume they have the actual $n$ at all times during training.And for the paper ""Do We Really Need Deep Learning Models for Time Series Forecasting?"", for table 1 as shown below:I think h refers to the number of regressors they are using. So, for the first row, they can forecast 24 points using the existing training data. But how can they further forecast œÑ points beyond the 20th point?So, I want to clarify1. Do the methods in the above articles suffer from data leakage? Or is it safe to assume that we can know the real $n$ features when we are focusing on the $m$ new data points?2. My current idea is that for using XGBoost in time series forcasting, we can either* Feed back the predicted value as the $n$ feature for the upcoming forcasting of $m$ points.* Or we train $L$ independent regressors to forecast the $L$ points in the future in one batch..
I've often wanted to make an AI humanizer. The first approach I've tried was using `meta-llama/Llama-3.1-8B`. I first made a BERT fine-tune to classify between AI generated and human written. Then, I used a modified RL approach to fine-tune `meta-llama/Llama-3.1-8B` to rephrase an existing AI generated text, optimizing the humanness score. I repeated this several times, each time training a new scorer, similar to the GAN framework. This was largely unsuccessful. Unfortunately I can't share code because this was done months ago and I'm just now coming back to it, and I didn't properly track versions. I now believe that a T5 model would be better suited for this task than a Llama model. Does anyone have any suggestions, links, papers, or models that they can recommend? I am looking for open weights/open source models, not paid APIs..
I've been spending a lot of time lately evaluating different multimodal reasoning models for my research, and the gap between closed-source models like GPT-4.1 and open-source alternatives has been really frustrating. Most open models either can't handle complex visual reasoning or require massive compute resources.Recently I came across Skywork-R1V3, a 38B parameter model that's been getting some attention in the community, so I decided to put it through its paces. What caught my eye initially was their claim of 76.0% accuracy on MMMU, which would put it competitive with much larger proprietary models.After testing it extensively, I have to say the technical approach is really interesting. The model builds on InternVL-38B but what makes it special is how the Skywork team approached the reasoning problem. Instead of training visual reasoning from scratch, they found a way to transfer reasoning patterns from their existing text-based models into the multimodal domain.From what I can tell from the paper and my experiments, they used reinforcement learning during post-training rather than just supervised fine-tuning. This seems to be key to why it performs so well on complex reasoning tasks. When I tested it on mathematical problems with diagrams and scientific figure interpretation, it consistently broke down problems into logical steps rather than just pattern matching.The performance claims seem to hold up in my testing. It's genuinely competitive with closed-source alternatives on the types of visual reasoning tasks I care about, and the fact that it's fully open-source with quantized versions available makes it actually usable for research. I've been running the AWQ quantized version on a single A100 without issues.What really impressed me is how well it handles cross-disciplinary reasoning where you need to connect visual information with abstract concepts. The chain-of-thought capabilities feel much more robust than other open models I've tried.This connects to the broader Skywork ecosystem - their reward models have been downloaded over 750,000 times and seem to be helping multiple frontier models achieve strong benchmark results. There's clearly some solid technical work happening there.I'm curious if others have experimented with cross-modal transfer approaches like this, or if anyone else has found effective ways to get strong reasoning performance without massive scale. Also interested in hearing thoughts on RL vs supervised approaches for this kind of multimodal reasoning - my sense is that RL might be underutilized in this space but I'd love to hear other perspectives..
As foundation models scale and benchmarks saturate, contamination and drift present increasing challenges to meaningful evaluation. Sharing practical mitigation strategies that have worked in practice:\*\*Contamination Detection:\*\*\- N-gram overlap analysis (sliding window approach)\- Substring matching with fuzzy boundaries  \- Semantic similarity scoring via embeddings\- Statistical outlier detection in performance curves\*\*Dataset Hygiene:\*\*\- Temporal splits with strict cutoffs (no post-training data)\- Hold-out validation across multiple independent sources\- Private test sets with limited query budgets\- Adversarial examples targeting memorization vs. understanding\*\*Drift Mitigation:\*\*\- Rolling evaluation windows with decay weighting\- Multi-task assessment reducing single-metric gaming\- Human evaluation correlation tracking over time\- Cross-validation with domain-specific benchmarks\*\*Process Controls:\*\*\- Blind evaluation protocols (evaluator doesn't know model identity)\- Staged releases with contamination audits between stages\- Community-sourced benchmark validation\- Reproducibility requirements for evaluation codeSeeing gaps in current practice around contamination detection at scale and standardized tooling for drift measurement. What approaches have proven most effective in your evaluation pipelines?.
Observing increasing deployment of agentic systems with tool access, but reliability evaluation remains fragmented. Key reliability metrics worth standardizing:\*\*Success Rate Decomposition:\*\*\- Tool selection accuracy (right tool for task)\- Parameter binding precision (correct arguments)\- Error recovery effectiveness (fallback strategies)\- Multi-step execution consistency\*\*Failure Taxonomy:\*\*\- Type I: Tool hallucination (non-existent APIs)\- Type II: Parameter hallucination (invalid args)\- Type III: Context drift (losing task state)\- Type IV: Cascade failures (error propagation)\- Type V: Safety violations (unauthorized actions)\*\*Observable Proxies:\*\*\- Parse-ability of tool calls (syntactic validity)\- Semantic coherence with task context\- Graceful degradation under uncertainty\- Consistency across equivalent phrasingsCurrent evals focus on task completion but miss failure modes that matter for deployment. Need systematic measurement of these reliability dimensions across diverse tool ecosystems.Thoughts on standardizing these metrics across research groups?.
Hi ML community,I‚Äôm building **VulkanIlm**, a Python wrapper around llama.cpp leveraging Vulkan for GPU acceleration on legacy and AMD GPUs (no CUDA required). This opens the door to efficient local LLM use without expensive hardware.Recent benchmark highlights:* Dell E7250 integrated GPU (i7-5600U): 33√ó speedup on TinyLLaMA-1.1B chat model* AMD RX 580 (8 GB): 4√ó speedup on Gemma-3n-E4B-it (6.9B params)Inspired by Jeff Geerling‚Äôs blog on accelerating LLMs with eGPU setups on Raspberry Pi ([https://www.jeffgeerling.com/blog/2024/llms-accelerated-egpu-on-raspberry-pi-5](https://www.jeffgeerling.com/blog/2024/llms-accelerated-egpu-on-raspberry-pi-5)), I adapted and expanded it to run on AMD RX 580. A full how-to guide will come soon.Repo here: [https://github.com/Talnz007/VulkanIlm](https://github.com/Talnz007/VulkanIlm)Would love feedback or insights on Vulkan acceleration or similar efforts!.
Are today‚Äôs AI models hitting a wall or just missing a law?This recent preprint in arXiv proposes a minimal sandbox (a maze) and a statistical physics approach (Maximum Caliber principle) to address this question. The presented method, called mind-tuning, applies Maximum Caliber to predictive models and reveals a critical intuition phase between imitation and hallucination.[https://arxiv.org/abs/2508.06477](https://arxiv.org/abs/2508.06477).
Hi all, given the current state of machine learning, I have two questions:1. At what point in their career can a university lecturer/professor take on a joint position in industry?2. Alternatively, can a R&D researcher in industry go back to academia without having to restart at the bottom of the ladder?**Some context:** I am a PhD student on track to graduate in two months. I have several offers for applied/research scientist roles in industry, and interesting postdocs that could lead to a fulfilling academic career. I am not motivated by high salaries, and I know I want to do machine learning research forever! But the early-career academic job insecurity and the constant competitive grant writing I hear about are seriously concerning. At the same time, I know I can make a stronger/quicker practical impact in industry, despite the corporate constraints (work hours, less freedom, etc.). This is why I'm wondering if, in order to get the best of both worlds, one could start in academia and then transition into industry over time (or vice versa).My question is more related to early-career researchers; I am aware that once tenure is achieved, pretty much anything is doable (e.g., Hinton, LeCun).Thank you for sharing any insights, examples, or experiences on this :).
Hey guys its me again I made a new algorithm with No Prop and DRTP that hit a 91.25% on MNIST with one hidden layer and I did it all in pure C here is the link to the repo I will be writing a paper on it please leave reviews and feedback I am a undergraduate student trying to get an internship for ML Research and or Engineering. First in the world from what I can see by the way.[https://github.com/JaimeCasanovaCodes/DRTP-NOPROP-C](https://github.com/JaimeCasanovaCodes/DRTP-NOPROP-C).
I‚Äôve been following a lot of recent LLM competitions and projects, and I‚Äôve noticed that most solutions seem to boil down to either fine-tuning a base model or crafting strong prompts. Even tasks that start out as ‚Äúgeneralization to unseen examples‚Äù ‚Äî like zero-shot classification ‚Äî often end up framed as prompting problems in practice.From my reading, these two approaches (fine-tuning and prompting) cover a lot of the ground, but I‚Äôm curious if I‚Äôm missing something. Are there other practical strategies for leveraging LLMs that go beyond these? For example, some technique that meaningfully improve zero-shot performance without becoming ‚Äújust‚Äù a better prompt?Would love to hear from practitioners who‚Äôve explored directions beyond the usual fine-tune/prompt spectrum..
A little background - I'm starting my much anticipated PhD soon. It is limited to 3 years. Took some voluntary teaching duties. My ultimate target before I finish my PhD is to get really good papers out (also should a good number), build a really strong network and have excellent interpersonal skills. I've a question to all PhD/research you get good papers out regularly, 1-2+ first authors at good/decent conferences each year- how do you manage to do that? Did you slice up your study into mulitple publications or just really good with intuition about a method?But often isn't it difficult to manage other duites, collaborations and also go through the arbitrary review process. I would like to know more about any experience of yours and what can you suggest someone starting out.Edit: changed it to 1-2+ publications each year.
**Contributions:**1. **AMICL** (Associative Memory for In-Context Learning) algorithm that works in three steps:* Identify incomplete patterns in the input* Search context for similar, complete patterns* Complete the pattern using the best contextual matchThis achieves near-perfect performance on classification tasks.2. Inspired by AMICL, we introduce ""**residual attention streams**"" -- direct connections between attention head values across layers. This creates information flow pathways that better retain prior context.**Results:*** **24% faster convergence** to 95% accuracy in two-layer Transformers on toy tasks* **6-fold improvement** on Indirect Object Identification tasks (from \~7% to \~41% accuracy) in an 8M parameter model trained on TinyStories* Also showed (general) improvements on **1B parameter models****Architecture details:**Three variants were tested (residual streams for queries, keys, and values) and we found that the **values stream performed best**. This aligns with the AMICL model, where values directly retain input information.The key insight is that this approach enhances in-context learning efficiency and robustness **without increasing parameter count** \- making it a computationally efficient improvement.From a safety perspective, this enhanced in-context learning ability means AI systems can more reliably understand and follow instructions from context rather than falling back on potentially problematic patterns from training data. This work suggests that by looking to biology for inspiration, we can build AI systems that are not just more powerful and efficient, but also more trustworthy and controllable.**Biological connections:**It is possible to draw parallels to biological memory systems. The hippocampus has selective skip connections (direct CA3 to CA1 pathways plus indirect routes through CA2), where CA2 specialises in context-switching. This may serve similar computational functions to AMICL and the architectural modifications introduced here.**Possible future directions:*** Parameterised residual streams inspired by gamma-models* Alternative attention head connection patterns* Scaling to larger architectures* Applications beyond NLP**Links:*** Paper: [https://arxiv.org/abs/2412.15113](https://arxiv.org/abs/2412.15113)* Code: [https://github.com/tfburns/AMICL-and-residual-attention-streams](https://github.com/tfburns/AMICL-and-residual-attention-streams)**TL;DR:**New research shows that adding ""residual attention streams"" (direct connections between attention head values across layers) to Transformers can improve in-context learning performance while requiring no additional parameters. The approach is inspired by associative memory and has interesting parallels to hippocampal circuit architecture..
Do anyone have ever worked on getting heatmap-like maps on what ""model sees"" using multimodal LLMs, ofcourse it must be any open-source. Any examples? Would approaches like attention rollout, attention√ógradient, or integrated gradients on the vision encoder be suitable?.
I haven't tried to run it yet on PyTorch, but I don't see how we can load 20B parameters with 2 bytes per parameter (torch.bloat16) in a GPU with only 16GB of VRAMI was assuming that for every forward pass, it will move the experts weights to the GPU. Although as much as I cannot believe that because it is not efficient, I was tempted to the theory because 20B \* 2 bytes (torch.bfloat16) / (1024 byte->kilobyte / 1024 kilboyte->megabyte / 1024 megabyte->gigabyte) \\approx 39,1 GB of VRAM, just to load the modelIs this because of quantization using MXFP4?How on earth gpt-oss-20b with 4-bit quantization can have on par performance with DeepSeek R1 (671B)?[model.py](https://github.com/openai/gpt-oss/blob/c77966fc0fda390b0abeeecdec7134433fe9f224/gpt_oss/torch/model.py)[weights.py](https://github.com/openai/gpt-oss/blob/c77966fc0fda390b0abeeecdec7134433fe9f224/gpt_oss/torch/weights.py)[llm-stats.com](https://llm-stats.com/)Edit: README says it all\> [`torch`](https://github.com/openai/gpt-oss/blob/main/README.md#reference-pytorch-implementation)¬†‚Äî¬†a non-optimized¬†[PyTorch](https://pytorch.org/)¬†implementation for educational purposes only. Requires at least 4√ó H100 GPUs due to lack of optimization.[README.md](https://github.com/openai/gpt-oss/blob/c77966fc0fda390b0abeeecdec7134433fe9f224/README.md).
Hey, im working on a project which involves getting 85\~90% validation accuracy for the FER+ dataset but only using shallow neural networks. I have been trying to achieve this but im stuck around 70%. Any ideas on how to make it through?.
Hey! I hope you guys are all doing well. So, I've been deep into the statistics required in M.L. specifically. I just came to understand a few topics like‚Ä¢Confidence Intervals‚Ä¢Uniform/Normal distrinutions‚Ä¢Hypothesis testing etcSo, these topics are quite interesting and help you analyze the numerical feature in the dataset. But here's the catch. I am still  unable to understand the actual practical use in the modeling. For example, I have a numeric feature of prices and for example it doesn't follow the normal distribution and data is skewed so I'll apply the central limit theorem(CLT) and convert the data into normal distribution. But what's the actual use-case? I have changed the actual values in the dataset as I've chosen random samples from the dataset while applying CLT and randomization will actually change the input feature right? So, what is the use-case of normal distribution? And same goes for the rest of the topics like confidence interval. How do we practically use these concepts in M.L.?Thanks.
Hello. I'm trying to advance my machine learning knowledge and do some experiments on my own.  Now, this is pretty difficult, and it's not because of lack of datasets or base models or GPUs.  It's mostly because I haven't got a clue how to write structured pytorch code and debug/test it while doing it. From what I've seen online from others, a lot of pytorch ""debugging"" is good old python print statements.  My workflow is the following: have an idea -> check if there is simple hugging face workflow -> docs have changed and/or are incomprehensible how to alter it to my needs -> write simple pytorch model -> get simple data from a dataset -> tokenization fails, let's try again -> size mismatch somewhere, wonder why -> nan values everywhere in training, hmm -> I know, let's ask chatgpt if it can find any obvious mistake -> chatgpt tells me I will revolutionize ai, writes code that doesn't run -> let's ask claude -> claude rewrites the whole thing to do something else, 500 lines of code, they don't run obviously -> ok, print statements it is -> cuda out of memory -> have a drink.  Honestly, I would love to see some good resources on how to actually write good pytorch code and get somewhere with it, or some good debugging tools for the process. I'm not talking about tensorboard and w&b panels, there are for finetuning your training, and that requires training to actually work.  Edit:  There are some great tool recommendations in the comments. I hope people comment even more tools that already exist but also tools they wished to exist. I'm sure there are people willing to build the shovels instead of the gold....
For anyone who works in research, the process of designing effective data visualizations can be a significant bottleneck. I often found myself searching through numerous papers just to find inspiration for layouts and plot types, which was inefficient.To solve this problem for myself and others, I developed [**Plottie.art**](http://Plottie.art), a searchable, browser-based library of over 100,000 plots curated from scientific literature.I'm sharing it here because the machine learning pipeline behind it combines a specialized computer vision model with an LLM in a way that I thought this community would find interesting.**The ML Pipeline**The process starts with a large collection of figure images sourced from open-access papers. The goal is to make each individual plot within these figures searchable.**1. Subplot Segmentation with a Custom YOLOv12 Model**A key challenge is that many figures are multi-panel, containing several distinct subplots within a single image.* **Model Training:** To address this, I trained a custom **YOLOv12 model**. This required **manually annotating a dataset of 1,000 images** to teach the model to accurately identify and isolate the boundaries of individual subplots and their captions.* **Function:** The model processes each source image and outputs bounding boxes for each subplot, effectively segmenting complex figures into their constituent parts.**2. Plot Classification and Keyword Extraction with Gemini**With the subplots isolated, the next step was to classify each image by plot type (e.g., heatmap, UMAP) and extract relevant keywords for search.* **Approach:** While I considered training another dedicated classification model, the data collection and labeling requirements would have been substantial. I opted for a more efficient approach using a large multimodal model.* **Implementation:** I utilized the **Google Gemini API**. By providing a subplot image, I could prompt the model to perform both classification and keyword extraction. A prompt structured like, `""Analyze this scientific plot. Identify its specific type and extract key terms from its labels and content.""` proved to be highly effective.* **Outcome:** This method was not only fast to implement but also yielded high-quality, structured metadata. It successfully bypassed the need for a separate, time-intensive training pipeline for classification.This two-stage pipeline allows the content on[**Plottie.art**](https://plottie.art)to be easily searched and explored. The tool is free, requires no login, and runs in the browser.I would be very interested to hear your feedback on the project and the technical stack. I'm especially curious about any thoughts on combining specialized vision models with general-purpose LLMs for this type of application, or suggestions for improving the pipeline..
I've been looking through papers that use LLMs for robotic control (e.g. SayCan, SayPlan etc.). Are there any papers that use reasoning models like DeepSeek R1 or o3 that do well on benchmarks?.
2 of my reviewers completely ghosted the discussion period. Wondering what happens next?.
Neurips 2025 is being hosted at three different locations this time around: 1) San Diego; 2) Mexico City; 3) Copenhagen. What is your opinion on this?.
To be clear I understand nothing about the inner workings of the tool (I have a CS degree and no ML/AI background), but I've been in search of a near 100% accurate tool and can't find one.First q, why (If you can explain like I'm a 5th grader that'd be awesome)? Genuinely curious to understand. Second q, would it be a waste of time for me to try to tackle this problem by myself (I have a lot of time on my hands lately)?I unexpectedly got very curious and have a strong itch to at least¬†*try*¬†solving it, but I have no background nor any understanding of how hard such a problem would be or if it's ""worth"" trying to solve - whatever worth means.Any insights are appreciated. Thanks :).
Joined a company few days back for AI role. Here there is no work related to AI, it's completely software engineering with monitoring work. When I read about AI engineers getting huge amount of salary, companies try to poach them by giving them millions of dollars I get curious to know what they do differently.Feel free to answer..
Hi everyone,I‚Äôm trying to run this project from GitHub: [https://github.com/yisol/IDM-VTON](https://github.com/yisol/IDM-VTON)  My goal is to study how it works and understand how clothes adapt so realistically to different bodies.Here‚Äôs what I‚Äôve tried so far:* Followed the README exactly on my laptop (no GPU) ‚Üí not usable because of hardware limits.* Cloned it to Google Colab ‚Üí initially had dependency issues, solved them with Miniconda in Colab.* Now, when running `gradio_demo/app.py`, the process gets **Killed** (out-of-memory).please Suggestions for running this project without a local GPU.Any tricks for optimizing memory usage in Colab.Alternative tools or platforms?I‚Äôm fine with paid or free solutions as long as they let me test and understand the code.Has anyone here successfully run IDM-VTON or a similar Stable Diffusion-based try-on model without a powerful GPU?All I want is to be able to run this project, test it, play with the code, and see the results. If you know of any alternative or platform adapted to my problem, I would greatly appreciate it..
Is there an open source speech to speech (Voice Agent) model, like Amazon Nova Sonic?.
https://i.redd.it/b9goy7brywhf1.gifWe‚Äôre releasing **MiroMind Open Deep Research (ODR) v0.1**, which we believe is the **first** ***full-stack*****, fully open-source deep research project**‚Äînot just an agent, but also the **model, dataset, and training/RL infra** are open and reproducible. The agent framework (**MiroFlow**) reproduces **82.4** on **GAIA validation**; the model series (**MiroThinker**) reaches **60.2%** on **GAIA-Text-103**. Looking for contributors + repro logs.# Why this matters* **Full-stack openness**: most deep-research releases stop at the agent; ODR opens **all four layers**: **Agent (MiroFlow)**, **Model (MiroThinker)**, **Data (MiroVerse)**, **Training/RL (MiroTrain / MiroRL)**. * **Reproducible numbers**: ‚Ä¢ **MiroFlow**: GAIA validation **maj. vote 82.4**, pass@1 avg@3 **72.2** (with setup details & scripts). ‚Ä¢ **MiroThinker v0.1**: **60.2%** on **GAIA-Text-103** (with both SFT & DPO variants across 8B/14B/32B).* **Open data at scale**: **MiroVerse v0.1**‚Äî**147k+** full rollout trajectories (**\~1.9B tokens, 602k+ tool calls**), built for tool-use/web-browsing agents.# What‚Äôs included* **MiroFlow (Agent framework)** ‚Äì multi-tool, sub-agent orchestration, MCP integration, benchmarking UI; detailed GAIA runs & scripts.* **MiroThinker (Model series)** ‚Äì agentic LLMs optimized for deep research; SFT/DPO at 8B/14B/32B with evaluation guides.* **MiroVerse (Dataset)** ‚Äì 147k+ verified trajectories across multi-hop QA, browsing, scientific reasoning; hybrid licensing noted on card.* **MiroTrain / MiroRL (Training & RL)** ‚Äì end-to-end post-training + MCP-first RL for tool-using agents.# Quick start (agent eval)1. **MiroFlow**: clone, set keys (OpenRouter/Anthropic/OpenAI/Gemini, Serper, Jina, E2B), optional E2B Docker sandbox for stable repro; run GAIA scripts.2. **MiroThinker**: pull model from HF or self-host via SGLang; run GAIA-Validation / GAIA-Text-103 / HLE / WebWalkerQA scripts.# Links* **Overview blog (tables & results)**: [miromind.ai/blog/miromind-open-deep-research](http://miromind.ai/blog/miromind-open-deep-research) [MiroMind](https://miromind.ai/blog/miromind-open-deep-research)* **Agent**: [GitHub.com/MiroMindAI/MiroFlow](http://GitHub.com/MiroMindAI/MiroFlow) [GitHub](https://github.com/MiroMindAI/MiroFlow)* **Models**: [GitHub.com/MiroMindAI/MiroThinker](http://GitHub.com/MiroMindAI/MiroThinker) & HF collection [GitHub](https://github.com/MiroMindAI/MiroThinker)[Hugging Face](https://huggingface.co/collections/miromind-ai/mirothinker-v01-689301b6d0563321862d44a1)* **Dataset**: HF ‚Äî miromind-ai/MiroVerse-v0.1 [Hugging Face](https://huggingface.co/datasets/miromind-ai/MiroVerse-v0.1)* **Training/RL**: [GitHub.com/MiroMindAI/MiroTrain](http://GitHub.com/MiroMindAI/MiroTrain) & /MiroRL [GitHub+1](https://github.com/MiroMindAI/MiroTrain)# .
**Paper/Blog**: [https://huggingface.co/blog/codelion/adaptive-classifier](https://huggingface.co/blog/codelion/adaptive-classifier)  **Code**: [https://github.com/codelion/adaptive-classifier](https://github.com/codelion/adaptive-classifier)  **Models**: [https://huggingface.co/adaptive-classifier](https://huggingface.co/adaptive-classifier)# TL;DRWe developed an architecture that enables text classifiers to:* Learn from as few as 5-10 examples per class (few-shot)* Continuously adapt to new examples without catastrophic forgetting* Dynamically add new classes without retraining* Achieve 90-100% accuracy on enterprise tasks with minimal data# Technical Contribution**The Problem**: Traditional fine-tuning requires extensive labeled data and full retraining for new classes. Current few-shot approaches don't support continuous learning or dynamic class addition.**Our Solution**: Combines prototype learning with elastic weight consolidation in a unified architecture:    ModernBERT Encoder ‚Üí Adaptive Neural Head ‚Üí Prototype Memory (FAISS)                                        ‚Üì                                EWC Regularization    **Key Components**:1. **Prototype Memory**: FAISS-backed storage of learned class representations2. **Adaptive Neural Head**: Trainable layer that grows with new classes3. **EWC Protection**: Prevents forgetting when learning new examples4. **Dynamic Architecture**: Seamlessly handles new classes without architectural changes# Experimental ResultsEvaluated on 17 diverse text classification tasks with only 100 examples per class:**Standout Results**:* Fraud Detection: 100% accuracy* Document Classification: 97.5% accuracy* Support Ticket Routing: 96.8% accuracy* **Average across all tasks**: 93.2% accuracy**Few-Shot Performance**:* 5 examples/class: \~85% accuracy* 10 examples/class: \~90% accuracy* 100 examples/class: \~93% accuracy**Continuous Learning**: No accuracy degradation after learning 10+ new classes sequentially (vs 15-20% drop with naive fine-tuning).# Novel Aspects1. **True Few-Shot Learning**: Unlike prompt-based methods, learns actual task-specific representations2. **Catastrophic Forgetting Resistance**: EWC ensures old knowledge is preserved3. **Dynamic Class Addition**: Architecture grows seamlessly - no predefined class limits4. **Memory Efficiency**: Constant memory footprint regardless of training data size5. **Fast Inference**: 90-120ms (comparable to fine-tuned BERT, faster than LLM APIs)# Comparison with Existing Approaches|Method|Training Examples|New Classes|Forgetting|Inference Speed||:-|:-|:-|:-|:-||Fine-tuned BERT|1000+|Retrain all|High|Fast||Prompt Engineering|0-5|Dynamic|None|Slow (API)||Meta-Learning|100+|Limited|Medium|Fast||**Ours**|**5-100**|**Dynamic**|**Minimal**|**Fast**|# Implementation DetailsBased on ModernBERT for computational efficiency. The prototype memory uses cosine similarity for class prediction, while EWC selectively protects important weights during updates.**Training Objective**:    L = L_classification + Œª_ewc * L_ewc + Œª_prototype * L_prototype    Where L\_ewc prevents forgetting and L\_prototype maintains class separation in embedding space.# Broader ImpactThis work addresses a critical gap in practical ML deployment where labeled data is scarce but requirements evolve rapidly. The approach is particularly relevant for:* Domain adaptation scenarios* Real-time learning systems* Resource-constrained environments* Evolving classification taxonomies# Future Work* Multi-modal extensions (text + vision)* Theoretical analysis of forgetting bounds* Scaling to 1000+ classes* Integration with foundation model architecturesThe complete technical details, experimental setup, and ablation studies are available in our blog post. We've also released 17 pre-trained models covering common enterprise use cases.**Questions welcome!** Happy to discuss the technical details, experimental choices, or potential extensions..
Hi everyone, I am wondering if it‚Äôs worth to buy the [Mathematica + LLM in notebook](https://writings.stephenwolfram.com/2023/06/introducing-chat-notebooks-integrating-llms-into-the-notebook-paradigm/) so it would be great if anyone who has it could paste this [question](https://pastebin.com/aynsiWrc) into the mathematica LLM. I‚Äôve put it on pastebin, because reddit will mess up the string with its own formatting. But if you do not wish to click I paste it here, but the ^ will mess up, so use the pastebin to paste it into LLM:> Let V be a vector field on an affine space A generating a flow \phi, let \Psi:A->A be any smooth invertible map with smooth inverse, and let \Phi(t,x)=\Psi(\phi(t,\Psi^{-1}(x))). Show that \Phi is also a flow on A, and that its generator V^\Psi is given by V^\Psi_x=\Psi_*(V_{\Psi^{-1}(x)}).It‚Äôs a kind of problem which can be done with pen & paper and I am not sure if mathematica is useful here.Would be great if someone can post a screenshot of the answer from mathematica. I am trying to figure out if these types of problems are applicable to mathematica + LLM.The problem is from book by Crampin, Pirani ‚ÄúApplicable Differential Geometry‚Äù, 1987, page 64 Exercise 28.So far I used the Bing LLM for it, and it gave the correct answer. Including the derivations, calculations and simplifications of the formulas..
I‚Äôm collecting operational criteria (not metaphysics): cross-session behavioral consistency, stable self-reports under blinded probes, reproducible third-party protocols. Looking for papers, metrics, or eval harnesses you‚Äôd use to *falsify* these..
It's just my feeling, but from what I see, the post rebuttal score this year maybe higher than previous year. Can everyone share how the score change so far for the paper that you review? In my case, I know 9 paper reviewed by me and my friend, 4 get their score increase (1 increases by 1, the rest a lot more), 1 withdraw, 1 likely to decrease by 1, the rest didn't change.
Hello,  I am looking for Machine Learning (ML) use cases to try out a class of optimization algorithms, namely Frank Wolfe (FW) algorithms. Those are *gradient-based* and *projection-free* algorithms for optimizing a cost function (convex or non-convex) over a *convex* set of constraints. Usually, such problems are tackled by Projected Gradient Descent (PGD), where each iteration consists of a descent in the direction of the gradient, then a projection onto the set of constraints to ensure that the new solution is feasible. However, depending on the set of constraints, this projection step can be very costly and thus prohibitive. FW algorithms avoid this projection step, which leads to less compute-intensive iterations.  I am turning toward r/machinelearning communities for ideas of problems that satisfy those conditions: optimization over a convex set of constraints (original or relaxed version of a problem), ideally that can be large-scale so I can push the FW algorithms to their limits.For the moment, I found those following problems:  * **Adversarial attack** : modifying an image in a imperceptible way for a human so that a classifier misclassifies it. The modification ùõø can be constrained in the ùúÄ-ball so that it remains small, which is a convex set so it fits the description.  * **Polynomial Regression**/**Compressed Sensing**: when we need a sparse represention, we can set the constraint that the coefficients live in the L1-norm ball that is sparsity-inducing.  * **Matrix Completion**: not the original formulation that constrain that the rank of the matrix *X* denoted rank(*X*) is low, but setting a constraint of the nuclear-norm value of the matrix *X*, which is a convex constraint.I am also looking for optimization over the set of Doubly Stochastic Matrices (also called the Birkhoff polytope, which is the convex hull of permutation matrices), but I've been looking for a few hours on Google and I haven't found any concrete application, so if you have any ideas I will gladly take them. I've heard that they are useful in matching/assignment problems.Thanks for reading.
Hi, I‚Äôve been considering flow matching models to disentangle attributes from an embedding. The idea stems from the fact that flow matching models learn smooth and invertible mappings.Consider a pre-trained embedding E, and disentangled features T1 and T2. Is it possible to learn a flow matching model to learn this mapping from E to T1 and T2 (and vice versa)?My main concerns are -1. Distribution of E is known since its source distribution. But T1 and T2 are unknown. How will the model learn when it has a moving or unknown target?2. I was also wondering if some clustering losses can enable this learning?3. Another thought was to use some priors, but I am unsure as to what would be a good prior. Please suggest ideas if this wouldnt work. Or advancements on this if it does.Prior work:A paper from ICCV 25 (‚ÄúSCFlow‚Äù) does disentanglement using flow matching. But, they know the disentangled representations (Ground truth is available). So they provide T1 or T2 distributions to the model alternatively and ask it to learn the other. .
I have seen many articles (one example https://aiguide.substack.com/p/llms-and-world-models-part-1) stating that LLMs have no coherent/effective world models and because of this their accuracy is inherently limited. Can this obstacle be overcome, and if not why?.
I‚Äôm working on a research project where, starting from an event log, I build for each trace a¬†Direct Follows Graph (DFG)¬†representing that trace, where each node corresponds to an activity.My goals are:1. From the obtained DFGs, derive¬†Prefix graphs¬†(i.e., DFGs with the final nodes removed) and apply a GNN for¬†**next activity prediction at the node level**. This way, if I feed the model a list of activities during inference, it should return the next activity.2. Given the prediction, I want to apply¬†**GNN explainability techniques**, specifically¬†*Perturbation-based methods*and¬†*Surrogate-based methods*, to explain the model‚Äôs decision.My question is mainly about point 2: since the DFGs are mostly¬†linear¬†(with at most some self-loops or a few normal loops), does it make sense to search for¬†subgraphs¬†that explain the result (e.g., with GNNExplainer or SubgraphX)? For example, if I use a 3-layer GNN, wouldn‚Äôt the prediction already be fully explained by the¬†3-hop neighborhood?  These are not very large graphs with huge numbers of edges... maybe I‚Äôm missing something.P.S.: I‚Äôm new in the world of GNNs..
Approximate nearest-neighbor search (ANNS) algorithms have become increasingly critical for recent AI applications, particularly in retrieval-augmented generation (RAG) and agent-based LLM applications. In this paper, we present CRINN, a new paradigm for ANNS algorithms. CRINN treats ANNS optimization as a reinforcement learning problem where execution speed serves as the reward signal. This approach enables the automatic generation of progressively faster ANNS implementations while maintaining accuracy constraints. Our experimental evaluation demonstrates CRINN‚Äôs effectiveness across six widely-used NNS benchmark datasets. When compared against state-of-the-art open-source ANNS algorithms, CRINN achieves best performance on three of them (GIST-960-Euclidean, MNIST-784-Euclidean, and GloVe-25-angular), and tied for first place on two of them (SIFT-128-Euclidean and GloVe-25-angular). The implications of CRINN‚Äôs success reach well beyond ANNS optimization: It validates that LLMs augmented with reinforcement learning can function as an effective tool for automating sophisticated algorithmic optimizations that demand specialized knowledge and labor-intensive manual refinement.[https://github.com/deepreinforce-ai/CRINN](https://github.com/deepreinforce-ai/CRINN).
Greetings,At work, I am currently building a very simple document summarization platform that takes in source documents, produces small and concise summaries of the documents, and storing them in a database.The project plans to expand to a lot of other functionalities later on, but for the moment I've been asked to determine a way to ""grade"" or ""analyze"" the generated summaries against the original source text and give it a score, as an aid for some of our human reviewers.I've been working on this for about a week, and have tried various methods like BERTScore, MoverScore, G-eval, ROGUE, BLEU and the like. And I've come to the conclusion that the scores themselves don't tell me a lot, at least personally (which could simply be due in part to me misunderstanding or overlooking details). For example I understand cosine similarity to a degree, but it's hard to put into context of ""grade this summary."" I've also tried out an idea about sending the summary to another decoder-only model (such as Qwen or even Phi-4), asking it to extract key facts or questions, then running each of those through a BERT NLI model against chunks of the source material (checking ""faithfulness"" I believe). I also thought about maybe doing some kind of ""miniature RAG"" against a single document and seeing how that relates to the summary itself, as in to find gaps in coverage.For the most part, I wasn't disappointed in the results but I also was not thrilled by them either. Usually I'd get a score that felt ""middle of the road"" and would be difficult to determine whether or not the summary itself was good.So my question is: Does anyone here have any experience with this and have any suggestions for things to try out or experiment with? I feel like this might be a large area of ongoing research as is, but at this point we (where I work) might actually just be striving for something simple.Thanks!.
What are the 10 most impactful ml papers on graph neural networks.
We‚Äôre running a live comparative test today to see how four leading LLMs handle coding tasks in a natural-language coding environment.**Models tested:*** GPT-5* Claude Sonnet 4* Gemini 2.5 Pro* GLM45 (open-source)**Format:*** All models receive **the exact same prompt*** Multiple runs at different complexity levels:   * Simple builds   * Bug-fix tasks   * Multi-step complex builds   * Possible planning flowsWe‚Äôll compare:* Output quality* Build speed* Debugging performance**When:** Today, 16:00 UTC (19:00 EEST)**Where:** [https://live.biela.dev](https://live.biela.dev) Hop in with questions, curiosities, prompt suggestions and whatever comes in mind to make the test even better! :).
If so, link the paper and the result. Very curious about this. Not even just metrics like accuracy, have BDL methods actually achieved better results in calibration or uncertainty quantification vs say, deep ensembles?.
Hi all,My goal is to launch a small ML initiative/lab that:* Focus on non-mainstream but high-impact ML research areas.* Work on project-driven open-source contributions and papers from day one* Build a network and reputation through real, tangible outputs rather than just theory or courseworkI want this to be lean and agile, not a formal institution, but a focused group of people (starting small) who want to push boundaries and build a reputation in underexplored domains.**What I‚Äôm looking for:*** Suggestions on promising underexplored ML fields or projects with potential real-world impact* Advice on structuring such a lab efficiently (collaboration tools, workflow, open-source best practices)* Potential collaborators interested in contributing to projects with measurable outputs* Any pitfalls to watch out for in early-stage lab building**Conditions I‚Äôm considering:**1. Projects must be open-source and reproducible.2. Research and code contributions should aim for quality over quantity.3. Members commit to regular updates and active communication.4. We focus on non-mainstream areas to avoid crowded research spaces.5. All contributions must align with ethical standards.6. Aim for publishable or demonstrable outcomes, no just ‚Äúexploratory‚Äù hacks.7. Small core team at first (3-5 people max) to stay agile.8. Clear documentation and modular code required from day one.Would appreciate any concrete ideas or feedback. Also open to recommendations on platforms or tools that could help us run this smoothly..
Ahead of today‚Äôs GPT-5 launch, I compiled a list of unsaturated LLM evals. Let's see if GPT-5 can crack them.link: [https://rolandgao.github.io/blog/unsaturated\_evals\_before\_gpt5](https://rolandgao.github.io/blog/unsaturated_evals_before_gpt5)  x post: [https://x.com/Roland65821498/status/1953355362045681843](https://x.com/Roland65821498/status/1953355362045681843)https://preview.redd.it/t3cwiitotjhf1.png?width=1302&format=png&auto=webp&s=098a7f2092afdf436a2699104accc49d01909f19.
Hey everyone,I have recently reproduced **YOLOv1** entirely from scratch using **PyTorch**, as a self-driven project to dive deeper into object detection and research implementation**What I implemented**YOLOv1 CNN architecture (paper-faithful)Custom loss function (localization, confidence, classification)IoU calculations and grid transformationsForward pass and inference pipeline (with visualization)Modular structure and utilities**Training hasn‚Äôt been done yet** although I have a GPU it is taking a long time, but the pipeline is fully written, ready for VOC or a custom dataset.**GitHub repo:**[https://github.com/aayan873/YOLOv1-from-Scratch-My-First-Paper-to-Code-Project/](https://github.com/aayan873/YOLOv1-from-Scratch-My-First-Paper-to-Code-Project/).
I am trying to build an on device speech recognition engine for recognising kids‚Äô voice better replacing speech framework I am using in my ios app right now.To do this, I collect sample audio data from my app keeping the privacy concerns in mind and transcribe these audio files with whisper large v2 and then using it as pseudo labelling to train  whisper tiny. I have following questions now:1. Is this a valid strategy or with low parameters  of whisper tiny this is a futile exercise no matter how much I train it?2. Most of my data is not clean, meaning background and other noise is interspersed with kids‚Äô speech. But it‚Äôs also important for my app to be accurate in these environment.3. How many hours of audio I need to train it on  keeping the above audio quality in mind to achieve reasonable accuracy?4. Are there better solutions?.
The Qwen team recently proposed **Group Sequence Policy Optimization (GSPO)**, a reinforcement learning approach for post-training LLM fine-tuning. They position it as an alternative to **Group Relative Policy Optimization (GRPO)** \- used in DeepSeek - and claim GRPO‚Äôs token-level importance sampling is ‚Äúill‚Äëposed‚Äù for stable training.**Background:*** Popular RLHF methods (e.g. PPO) optimize LLMs via reward signals.* DeepSeek‚Äôs GRPO extends this by computing sample-level value estimations.* Qwen reports that GRPO often triggers gradient instability and model collapse unless patched with complex adjustments.**Key concerns with GRPO:*** Applies importance sampling **per token**, accumulating high variance across long sequences.* Particularly problematic for **Mixture-of-Experts (MoE)** models, where token-level routing shifts can destabilize training.* To counteract this, GRPO-based pipelines often rely on strategies like **Routing Replay**.**GSPO‚Äôs proposal:*** Moves to **sequence-level importance sampling**, normalizing by sequence length.* Dramatically reduces variance and eliminates the need for routing hacks.* Qwen reports stable MoE convergence and better scaling.**Findings from experiments:*** On benchmarks such as AIME‚Äô24, LiveCodeBench, and CodeForces, GSPO achieves better reward curves than GRPO.* GSPO converges faster with more compute and shows smoother scaling trends.* GRPO requires Routing Replay to perform adequately; GSPO does not.If you're interested, read more about it here: [Qwen Team Proposes GSPO for Qwen3, Claims DeepSeek's GRPO is Ill-Posed](https://blog.netmind.ai/article/Qwen_Team_Proposes_GSPO_for_Qwen3%2C_Claims_DeepSeek's_GRPO_is_Ill-Posed). The blog post includes mathematical formulations of both methods and performance comparisons.I‚Äôm interested to know:* Whether anyone in the community has observed instability with token-level importance sampling or GRPO?* Has sequence-level weighting like GSPO been tested in your RLHF pipelines?.
Hi r/MachineLearning,I've been thinking about the inefficiency of using a fixed number of inference steps in text diffusion models. It seems wasteful to use the same amount of compute for a simple sentence as for a complex one.I've prototyped an alternative architecture I'm calling ""Adaptive Refinement Diffusion,"" and I'd love your feedback on it.The core idea is:* Instead of a fixed loop, the model iteratively refines the sequence.* At each step, it calculates a confidence score for every token (based on a mix of its embedding stability and prediction probability).* If a token's score passes a certain threshold, it gets ""frozen"" and is excluded from future computation.* The entire generation process stops dynamically once all tokens in the sequence are frozen.This means the model would naturally focus compute on the more difficult or ambiguous tokens and could finish simple sentences much faster.My questions for the community are:1. Does this architecture already exist? I've searched for prior work but haven't found this specific token-level freezing mechanism.2. What potential flaws or failure modes do you see with this approach?Appreciate any thoughts or links to related papers. Thanks!.
I wanted to have a discussion along the following lines. Lets say there is a scenario where the advantage of parallelism is no longer present. Then for an NLP task which model would you prefer an LSTM or a transformer? Lets assume the size of both models in terms of parameters is also the same. I have consulted 4o, claude sonnet, gemini flash 2.5 and grok 3 as well. Posting their responses in the comments. The question is around how to think about different models and their advantages. I feel like nowadays throwing a transformer is the first thing people do..
The new OSS models by OpenAI have low precision weights (MXFP4). Does anyone know:- Is it likely that they were trained with MXFP4?- Could anyone recommend papers on how to train models in such a low precision? Is it possible to train with SGD in such a low range, i.e. FP4, has just 16 values?- Is it possible to go even lower? I.e. FP3 or FP2?.
**TL;DR**: [Soft tokens](https://www.arxiv.org/abs/2505.15778) (probabilities-weighted sum over vocab) actually underperform traditional ""hard"" tokens. But a Gumbel-Softmax trick can salvage this issue.**Paper:** [https://www.arxiv.org/pdf/2508.03440](https://www.arxiv.org/pdf/2508.03440)**Abstract:**>Human cognition naturally engages with abstract and fluid concepts, whereas existing reasoning models often rely on generating discrete tokens, potentially constraining their expressive capabilities. Recent advancements aim to address this limitation by enabling large language models (LLMs) to generate soft, abstract tokens, thus facilitating reasoning within a continuous concept space. This paper explores the \`Soft Thinking' capabilities of various LLMs by examining the models' internal behavior using a suite of probing techniques. Contrary to the common belief that Soft Thinking enables the simultaneous exploration of diverse reasoning paths, our findings reveal that LLMs predominantly rely on the most influential component of the soft inputs during subsequent decoding steps. This reliance hinders the exploration of different reasoning paths and reduces vanilla Soft Thinking to a form of greedy decoding, obscuring the advantage of transmitting more information through Soft Tokens. To tackle this issue, we explore sampling strategies to introduce \\emph{randomness}, employing methods such as Dirichlet resampling and the Gumbel-Softmax trick. Our experiments demonstrate that incorporating randomness can alleviate the limitations of vanilla approaches and unleash the potential of Soft Thinking. Notably, the Gumbel-Softmax trick provides adequate randomness with controlled smoothness, resulting in superior performance across eight reasoning benchmarks.**Visual Highlights:**https://preview.redd.it/zza3t8r17fhf1.png?width=1099&format=png&auto=webp&s=e12815cb0774bce2a2614b2c3ad0df47b071d8c8https://preview.redd.it/lulzrar27fhf1.png?width=1109&format=png&auto=webp&s=0fd5cd8dc90a9c09afb46dbd8e0412a72800dbe3.
I‚Äôve been running into the same issue again and again while working with LLMs: they forget. You can stuff the history into the prompt, set up a RAG pipeline, or go through fine‚Äëtuning, but none of these feel like a real solution.Because of that frustration, I started exploring memory management myself, more like giving models ‚Äúon‚Äëdemand context‚Äù instead of retraining them. It‚Äôs early, but it made me realize how huge and unexplored this space is.I‚Äôm wondering if others here have felt the same pain. How are you approaching memory in your projects, and do you think we‚Äôll ever see something beyond the RAG/fine‚Äëtuning combo?.
It seems the current state of publishing in A* venues (CVPR, NeurIPS, ICML, ICCV/ECCV) is zero-sum. One person‚Äôs rejection is another person‚Äôs acceptance. Reviewers seem to reject papers just for the sake of rejection. There‚Äôs a sense that some reviewers reject papers not on substantive grounds, but out of an implicit obligation to limit acceptance rates. Rebuttals appear to be pointless as reviewers take stubborn positions and not acknowledge their misunderstandings during this period. Good science just doesn‚Äôt appear to be as valued as the next flashiest LLM/VLM that gets pretty results..
If you haven't seen Genie 3 yet: [https://deepmind.google/discover/blog/genie-3-a-new-frontier-for-world-models/](https://deepmind.google/discover/blog/genie-3-a-new-frontier-for-world-models/)It is really mind blowing, especially when you look at the comparison between 2 and 3, the most striking thing is that 2 has this clear constant statistical noise in the frame (the walls and such are clearly shifting colours, everything is shifting because its a statistical model conditioned on the previous frames) whereas in 3 this is completely eliminated. I think we know Genie 2 is a diffusion model outputting 1 frame at a time, conditional on the past frames and the keyboard inputs for movement, but Genie 3's perfect keeping of the environment makes me think it is done another way, such as by generating the actual 3d physical world as the models output, saving it as some kind of 3d meshing + textures and then having some rules of what needs to be generated in the world when (anything the user can see in frame). What do you think? Lets speculate together!.
https://preview.redd.it/v3nxbno7xbhf1.png?width=1280&format=png&auto=webp&s=4a425509b0c20e16992c7998392567ff534a9b02Trainable selective sampling and sparse attention kernels are indispensable in the era of context engineering. We hope our work will be helpful to everyone! ü§ó* **Blog Post (The TL;DR):**¬†[https://hf.co/blog/wubingheng/dmattn](https://hf.co/blog/wubingheng/dmattn)* **Paper (The Nitty-Gritty):**¬†[https://huggingface.co/papers/2508.02124](https://huggingface.co/papers/2508.02124)* **Code (The Good Stuff):**¬†[https://github.com/SmallDoges/flash-dmattn](https://github.com/SmallDoges/flash-dmattn).
We are in discussion period for NeurIPS 2025. One of my reviewer is disrespectful;Doesn't have much knowledge in this field, but keep insisting he/she is right, againsting all the references in this field.  Also, this reviewer keeps raising issue out of scope. e.g., My paper is regarding bias, but the reviewer is saying ""setting 'gender' and 'race' as debiasing target is biased action"". I totally disagree this, then, how about the US law like ""The Equal Pay Act of 1963"" and ""The Fair Housing Act"" also controversial?I want to send AC confidential comment for the first time in my life, but is there any official guideline regarding the AC confidential comment? I want to make sure this reviewer is not eligible to review..
I‚Äôm quite new to GNNs and process mining, and I‚Äôm trying to tackle a project that I‚Äôm really struggling to structure. I‚Äôd love your input, especially if you‚Äôve worked with GNNs or process data before.I have a CSV file representing a business process (specifically a Helpdesk process). From this CSV, I want to build a graph representation of the process (specifically a Directly-Follows Graph). Then, I want to train a GNN to do¬†**next activity prediction**¬†at the¬†**node level**.The idea is: given a¬†*prefix graph*¬†(i.e., a pruned version of the full process graph up to a certain point), I want the model to predict the¬†label of the next activity, corresponding to the node that would logically come next in the process.I‚Äôve found very little literature on this, and almost no practical examples. I have a few specific doubts I hope someone can help me with.1. **Model choice**: It's a dataset made of 4580 graphs (traces), 7 average nodes each, 15 total labels (activities). I was thinking of using a 3-layer GCN for the prediction task. Does this make sense for my use case? Are there better architectures for sequence-based node prediction in process graphs?2. **Multiple process instances (graphs)**:As I said, I have¬†4580 different instances¬†of the process, each one is essentially a separate graph. Should I treat them as¬†4580¬†**separate graphs**¬†during training, or should I¬†**merge them into one big graph**¬†(while preserving per-node instance information somehow)?My concern is about how GNNs typically work with multiple small graphs, should I batch them separately, or does it make sense to construct one global graph?.
Hello everyone,I'm currently enrolled in a master's program in statistics, and I want to pursue a PhD focusing on the theoretical foundations of machine learning/deep neural networks.I'm considering statistical learning theory (primary option) or optimization as my PhD research area, but I'm unsure whether statistical learning theory/optimization is the most appropriate area for my doctoral research given my goal.Further context: I hope to do theoretical/foundational work on neural networks as a researcher at an AI research¬†lab in the¬†future.¬†Question:1)What area(s) of research would you recommend for someone interested in doing fundamental research in machine learning/DNNs?2)What are the popular/promising techniques and mathematical frameworks used by researchers working on the theoretical foundations of deep learning?Thanks a lot for your help..
- The 8th iteration of MLRC is happening in-person at Princeton University on August 21st. Keynote speakers include Arvind Narayanan (Princeton), Soumith Chintala (Pytorch - Meta), Jonathan Frankle (Databricks) and Stella Biderman (EleutherAI). - Panel discussion on ""Reproducibility of and by large language models"", moderated by Sayash Kapoor (Princeton)- Link to webpage: https://reproml.org/ (registration seems to be still open!).
I submitted a paper to the AAAI 2026 conference. The conference states that colors must only be used for figures.I mistakenly used colors in an experimental table to show the increase in accuracy within parentheses.Will I have a chance to modify it in the rebuttal phase? Are there some cases in which those who have made the same mistake proceed with the rebuttal phase?I found someone who submitted a paper with the same mistake to another conference proceeded with the rebuttal successfully..
I understand that updated scores of reviewers are not visible to authors this time round. I was wondering if anyone knows whether the final scores will also not be visible? I.e. once you revise your review and add your ""Final justification"", will your score not be visible to the authors anymore?  Asking because I've had a reviewer who has selected the mandatory acknowledgement option, not responded to my review, and whose score no longer appears on the portal..
Hi everyone! üëãI want to share the initial release of \[\`sklearn-migrator\`\] ([https://pypi.org/project/sklearn-migrator/](https://pypi.org/project/sklearn-migrator/)) ‚Äì a Python library designed to¬†**serialize and migrate scikit-learn models across incompatible versions.**If you‚Äôve ever faced issues like \`AttributeError: '...' object has no attribute '...'\` after upgrading \`scikit-learn\`, or had to retrain models just because of version mismatches in production‚Ä¶ this tool is for you.What it does?\- Converts saved models from older \`scikit-learn\` versions to be compatible with newer ones\- Supports serialization and internal structure mapping (especially for tree-based models)\- Designed to help maintain long-term model compatibility in production\## ‚úÖ Current support\- \*\*Classifiers & regressors\*\*:\- \`DecisionTree\`, \`RandomForest\`, \`GradientBoosting\`, \`LogisticRegression\`, \`LinearRegression\`, and more\- Tested across versions like: \['0.21.3', '0.22.0', '0.22.1', '0.23.0', '0.23.1', '0.23.2','0.24.0', '0.24.1', '0.24.2', '1.0.0', '1.0.1', '1.0.2','1.1.0', '1.1.1', '1.1.2', '1.1.3', '1.2.0', '1.2.1', '1.2.2','1.3.0', '1.3.1', '1.3.2', '1.4.0', '1.4.2', '1.5.0', '1.5.1','1.5.2', '1.6.0', '1.6.1', '1.7.0'\]We have 900 pairs of tested versions.Repository Github:¬†[https://github.com/anvaldes/sklearn-migrator](https://github.com/anvaldes/sklearn-migrator)  PyPI:¬†[https://pypi.org/project/sklearn-migrator/](https://pypi.org/project/sklearn-migrator/)  Medium article:¬†[https://medium.com/@alberto.valdes.gonzalez.96/sklearn-migrator-safe-migration-of-models-across-scikit-learn-versions-0842f8dc375e](https://medium.com/@alberto.valdes.gonzalez.96/sklearn-migrator-safe-migration-of-models-across-scikit-learn-versions-0842f8dc375e).
Sharing¬†**DocStrange**, an open-source Python library that makes document data extraction easy.* **Universal Input**: PDFs, Images, Word docs, PowerPoint, Excel* **Multiple Outputs**: Clean Markdown, structured JSON, CSV tables, formatted HTML* **Smart Extraction**: Specify exact fields you want (e.g., ""invoice\_number"", ""total\_amount"")* **Schema Support**: Define JSON schemas for consistent structured output**Quick start:**    pip install docstrange    docstrange invoice.jpeg --output json --extract-fields invoice_amount buyer seller**Data¬†Processing Options:*** **Cloud Mode**: Fast and free processing with minimal setup, free 10k docs per month* **Local Mode**: Complete privacy - all processing happens on your machine, no data sent anywhere, works on both cpu and gpu**Github**:¬†[https://github.com/NanoNets/docstrange](https://github.com/NanoNets/docstrange).
Hi, has anybody received their submission outcome for CIKM 2025?.
I've read a lot that working with an AMD GPU is a nightmare, but that was a while ago. Since they seem to be releasing a well-priced AI GPU in a few months, I wanted to know if it's worth it or if poor support still makes it a bad choice..
Implemented einsum using torch operations. Learned a lot doing it and had a lot of fun so wanted to share it here :).
https://www.sciencedirect.com/science/article/abs/pii/S0169260725004067A recent study in Computer Methods and Programs in Biomedicine explores an efficient approach to early Parkinson‚Äôs detection using time-series data from low-cost sensors processed on microcontrollers. The lightweight hybrid machine learning model offers potential for accessible screening in low-resource settings.Highlights:‚Ä¢ Parkinson‚Äôs disease (PD) is a progressive neurological disorder affecting motor and non-motor functions. Early detection of PD is essential for improving patient outcomes and quality of life‚Ä¢ This study proposes a multimodal hardware based wearable integrated with a novel machine learning framework for early, accurate and remote diagnosis of Parkinson‚Äôs disease.‚Ä¢ Analyses diverse data sets, including hemodynamic parameters, gait patterns, and hand tremor metrics including bradykinesia and rigidity.‚Ä¢ Achieves high accuracy through advanced algorithms, integrating artificial intelligence and intuitive user interface, thus providing a robust diagnostic tool..
I recently came across youtube channel Richard Aragon, watching his videos regarding his original model ZRIA and token transformation method P-FAF in [this video](https://www.youtube.com/watch?v=xP0oHEE6t_U), another on benchmarking his original ZRIA model for [agentic tasks](https://www.youtube.com/watch?v=b9zwwlRVQPo), and finally a video discussing P-FAF's conceptual connections to a recent work in [stochastic calculus](https://www.youtube.com/watch?v=64mmFBclymc). Admittedly, I am unsettled and agitated after posting a handful of questions on his video comments section as user yellowbricks and being threatened into silence with personal attacks and false accusations after challenging his theory and methodology but less than a vent post this it is a warning against the seemingly baseless theory of ZRIA and P-FAF and the unacceptable behavior which led to its niche following. We should remain critical of ZRIA and P-FAF not because of the individual promoting them, but because of the unchecked patterns of thought and conduct they can reinforce in the scientific community.    In the videos, we get conceptual explanations of the architecture ZRIA and he promotes it as a superior architecture to the transformer for language tasks. He has yet to point to a precise mathematical definition or theoretical foundation of ZRIA to describe what it predicts, what it optimizes, etc. Instead, in his agentic analysis video, he presents benchmarks scores such as ROCG which he presents as the best agentic benchmark and shows impressive score of his ZRIA model compared to a bigger Gemma, although as noted by commenter JohnMcclaned he clearly overfits the training data to ZRIA with no mitigating methods such as monitoring a validation set, and as noted by commenter israrkarimzai he has an issue in the code which explains why Gemma had 0 scores across the board and with the fix showed much more reasonable scores with several 100% scores. Both of these wildly weakens his claim to architectural superiority. (JohnMcclaned was unfortunatly bullied out of the comments sections by Richard.)This lack of rigor is reflected again in his video discussing the combination of ZRIA and P-FAF. Again, he presents a conceptual explanation of ZRIA and P-FAF. In particular he never points to a rigorous formulation of his P-FAF theory. Upon request he does not provide explanations, only a motivation, or insists that modern LLMs have enough knowledge of his theory such that they can substitute as a teacher (as he told to commenter wolfgangsullifire6158). His video description has a link to his hugging face blog post which again is unrigorous and uses a questionable benchmark whose results are weakened by Richard's examples of unscientific methodology in his benchmark videos. He which leaves viewers with no means to analyze, verify, or even understand what his theory is about. He does not address the inconsistencies in the benchmarking and the risk of overfitting in this video either as pointed out again by wolfgangsullifire6158 instead stating that ""Overfitting is a phenomenon unique to the Transformers architecture."" Admittedly I did not comment kindly towards his unscientific attitude and dismissal of the transformer despite his ZRIA being based on it.In his video linking his P-FAF to a graduate-level stochastic calculus paper on ""theta-expectations"", he again discusses the concepts at a very high level. I assume this video was made to address a request for a video on the theory of P-FAF. Instead of explaining the theory rigorously he tries to present the theta-expectations as a substitute for the mathematical foundation of P-FAF, suggesting that he had to ""go through the exact same process"" and solve the ""exact same problem"" to derive P-FAF with no evidence of such a derivation and only a dim conceptual overlap linking the two ideas in any way.This is not about Richard as a person. It is about his repeated behavior: marketing unverified claims as revolutionary science, silencing dissent, and treating scientific skepticism as personal attack. You should take this seriously not because of this one individual but because this pattern can erode the epistemic foundations of our field if left unchecked..
Hello everyone,I'm currently in my last month of an internship, doing ML. Everything is great, however, we have a lot of problems with the hardware : the server we usually use is down and will be until the end of my internship. We need to do more training and I managed to convince my boss to use some funds for a remote server until the end of the month. However, I don't know which providers exists and how good they are, so I am asking you. I would need at least 16 cpu threads, ideally more, capable of running 24/7, running on a flavor of ubuntu and, most importantly, with python and conda pre-installed. I don't have a lot of experience with using remote servers so the easier the better (I know how to use ssh for remote connection, but for example I don't know how to close the connection without ending the runnng task). All of this for a budget of 200‚Ç¨ for the month, max !Thank you all for your help !.
Im using label studioI'm having a strange problem. When I output with YOLO, it doesn't make predictions, but when I output with v8 OBB and train it, I can see the outputs. What's the problem ?I wanted to create a cat recognition algorithm. I uploaded 50 cat photos.I labelled them with Label Studio and exported them in YOLO format. I trained the model with v11 and used it. However, even though I tested the training photos, it couldn't produce any output.Then I exported the same set in YOLOv8 OBB format and trained it. This time, it achieved a recognition rate of 0.97.Why aren't the models I trained using YOLO exports working?.
**Full Example Runs as Videos:** [https://www.youtube.com/playlist?list=PLaeBvRybr4nUUg5JRB9uMfomykXM5CGBk](https://www.youtube.com/playlist?list=PLaeBvRybr4nUUg5JRB9uMfomykXM5CGBk)Hello! My name is Shiko Kudo; you might have seen me on r/stablediffusion some time back if you're a regular there as well, where I published a vocal timbre-transfer model around a month ago....I had been working on the next version of my vocal timbre-swapping model, but as I had been working on it, I realized that in the process I had something really interesting in my hands. Slowly I built it up more, and in the last couple of days I realized that I had to share it no matter what.This is the Periodic Linear Unit (PLU) activation function, and with it, some fairly large implications.The paper and code is available on Github here:  [https://github.com/Bill13579/plu\_activation/blob/main/paper.pdf](https://github.com/Bill13579/plu_activation/blob/main/paper.pdf)  [https://github.com/Bill13579/plu\_activation](https://github.com/Bill13579/plu_activation)  The paper is currently pending release on Arxiv, but as this is my first submission I am expecting the approval process to take some time.It is *exactly* as it says on the tin: neural networks based upon higher-order (cascaded) sinusoidal waveform superpositions for approximation and thus Fourier-like synthesis instead of a Taylor-like approximation with countless linear components paired with monotonic non-linearities provided by traditional activations; and all this change from a change in the activation....My heart is beating out my chest, but I've somehow gotten through the night and gotten some sleep and I will be around the entire day to answer any questions and discuss with all of you..
The only startup I know of that is focused specifically on this area is Aleph Alpha. Most others are just fine-tuning existing models or working on translation and image generation. There is no serious investment of time or money in original research and development in AI.Does anyone know of any other startups in Germany üá©üá™ working in this area? Even a pre-revenue stage startup?.
The Moonshot AI team behind the recent [Kimi K2](https://x.com/Kimi_Moonshot/status/1943687594560332025) model, one of the leading open-weights LLM, just released the technical report: https://arxiv.org/abs/2507.20534---**Kimi K2: Open Agentic Intelligence***We introduce Kimi K2, a Mixture-of-Experts (MoE) large language model with 32 billion activated parameters and 1 trillion total parameters. We propose the MuonClip optimizer, which improves upon Muon with a novel QK-clip technique to address training instability while enjoying the advanced token efficiency of Muon. Based on MuonClip, K2 was pre-trained on 15.5 trillion tokens with zero loss spike. During post-training, K2 undergoes a multi-stage post-training process, highlighted by a large-scale agentic data synthesis pipeline and a joint reinforcement learning (RL) stage, where the model improves its capabilities through interactions with real and synthetic environments.Kimi K2 achieves state-of-the-art performance among open-source non-thinking models, with strengths in agentic capabilities. Notably, K2 obtains 66.1 on Tau2-Bench, 76.5 on ACEBench (En), 65.8 on SWE-Bench Verified, and 47.3 on SWE-Bench Multilingual -- surpassing most open and closed-sourced baselines in non-thinking settings. It also exhibits strong capabilities in coding, mathematics, and reasoning tasks, with a score of 53.7 on LiveCodeBench v6, 49.5 on AIME 2025, 75.1 on GPQA-Diamond, and 27.1 on OJBench, all without extended thinking. These results position Kimi K2 as one of the most capable open-source large language models to date, particularly in software engineering and agentic tasks. We release our base and post-trained model checkpoints to facilitate future research and applications of agentic intelligence.*---Recently, there has been discussions about Muon and MuonClip, which the Moonshot AI team has developed for training Kimi. See recent discussions here on r/MachineLearning : https://old.reddit.com/r/MachineLearning/comments/1m2y23l/p_understanding_muon_a_revolutionary_neural/.
Did some major modifications to the model architecture and hyperparameters, aiming for improved performance. The entire model is built from scratch using PyTorch. The original paper introduces a memory-based mechanism that allows the model to attend to information beyond its context window, enabling long-term context handling. Instead of a single attention mechanism, the architecture incorporates two types of attention blocks: XLAttention for capturing short term memory and KNNAttention for enabling long term memory retrieval.Key Modifications from the Original Paper:‚Ä¢Replaced the default positional encoding with Rotary Positional Embeddings (RoPE)‚Ä¢Altered the attention mechanism to use Grouped Query Attention‚Ä¢Customized the DataLoader to support sharded datasets and data parallelism‚Ä¢Implemented Mixed Precision Training along with Distributed Data Parallel (DDP) support‚Ä¢Tweaked several training and model hyperparameters for better adaptabilityHF repo with model and training code is here:https://huggingface.co/abhinavv3/GPT_with_Modified_Memorizing_Transformer.
Got 5/4/3/3, none of the reviewers have responded so far üò≠üò≠üò≠Hopefully someone will respond by the end, but was wondering if anyone has any experience with no reviewers responding for the entire discussion.
GPU snapshotting is finally a thing! NVIDIA recently released their¬†[CUDA checkpoint/restore API](https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__CHECKPOINT.html)¬†and we at Modal (serverless compute platform) are using it drastically reduce GPU cold start times. This is especially relevant for serving large models, where it can take minutes (for the heftiest LLMs) to move model weights from disk to memory.GPU memory snapshotting can reduce cold boot times by up to 12x. It lets you scale GPU resources up and down based on demand without compromising on user-facing latency. Below are some benchmarking results showing improvements for various models!https://preview.redd.it/vjld59c34hgf1.png?width=3162&format=png&auto=webp&s=7a785152723d7a93a2b7ec1c28076e19c2fe27f1More on how GPU snapshotting works plus additional benchmarks in this blog post:¬†[https://modal.com/blog/gpu-mem-snapshots](https://modal.com/blog/gpu-mem-snapshots).
I‚Äôve read the ASI‚ÄëArch paper (arxiv.org/abs/2507.18074). It describes an automated AI driven search that discovered 106 novel neural architectures, many outperforming strong human‚Äëdesigned baselines.What stood out to me is that these weren‚Äôt just small tweaks, some designs combined techniques in ways we don‚Äôt usually try. For example, one of the best architectures fused gating directly inside the token mixer:(Wmix ¬∑ x) ‚äô œÉ(Wg ¬∑ x)instead of the usual separate stages for mixing and gating. Feels ‚Äúwrong‚Äù by human design intuition, yet it worked, like an AlphaGo move‚Äë37 moment for architecture search.One thing I‚Äôd love to see: validation across scale. The search was done at ~20M parameters, with only a few winners sanity‚Äëchecked at 340M. Do these rankings hold at 3B or 30B? If yes, we could explore cheaply and only scale up winners. If not, meaningful discovery might still demand frontier‚Äëlevel budgets.Curious what others think: will these AI‚Äëdiscovered designs transfer well to larger models, or do we need new searches at every scale?.
Has anyone tried out using pi0(the well-known VLA model) on simulation platforms?Due to budget and safety reasons, i only have very limited access to real robots. So i need to do everything once in simulation first.So i really would like to know whether it works well there. Would distribution shift be an issue?Thanks in advance!.
Hey everyone,  I just made my first ever submission to KDD.  The submission was double-blind and I uploaded the anonymized version via OpenReview, as required.Now I‚Äôm wondering:  **Can I submit the same anonymized version as a preprint to arXiv?** The official KDD CFP didn‚Äôt say much clearly about this, and I wanted to check what the norm is. Also, the deadline for submission (31 July) has passed.I had a few concerns and would love input from anyone who's been through this before:* Will uploading the paper to arXiv violate the double-blind review policy for KDD?* If I submit it to arXiv now, does the metadata (like the arXiv account or email) risk de-anonymizing me?.
Please post your personal projects, startups, product placements, collaboration needs, blogs etc.Please mention the payment and pricing requirements for products and services.Please do not post link shorteners, link aggregator websites , or auto-subscribe links.\--Any abuse of trust will lead to bans.Encourage others who create new posts for questions to post here instead!Thread will stay alive until next one so keep posting after the date in the title.\--Meta: This is an experiment. If the community doesnt like this, we will cancel it. This is to encourage those in the community to promote their work by not spamming the main threads..
Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!Thread will stay alive until next one so keep posting after the date in the title.Thanks to everyone for answering questions in the previous thread!.
https://preview.redd.it/x9z4mqmjnagf1.png?width=1078&format=png&auto=webp&s=fe3a69476a90e4574c86b9ee670f601ad7d93320**Just a rant**The **instructions literally** **OVERFLOW the web page** on PC. Also the latex **author kit** was updated **3 DAYS** before submission! (Coming from the systems/ML systems research field this is basically unheard of).Feels very unprofessional and poorly organized. Regardless, best of luck with your submissions! Hopefully we'll see each other in Singapore.
Hi everyone,  I‚Äôm working on a rather urgent and specific task. I need to craft prompts that involve arithmetic-based questions within the economics domain‚Äîquestions that a human with basic economic reasoning and arithmetic skills can solve correctly, but which large language models (LLMs) are likely to fail at.I‚Äôve already drafted about 100 prompts, but most are too easy for AI agents‚Äîthey solve them effortlessly. The challenge is to find a sweet spot:* **One correct numerical answer** (no ambiguity)* **No hidden tricks or assumptions*** **Uses standard economic reasoning and arithmetic*** **Solvable by a human (non-expert) with clear logic and attention to detail*** **But likely to expose conceptual or reasoning flaws in current LLMs**Does anyone have ideas, examples, or suggestions on how to design such prompts? Maybe something that subtly trips up models due to overlooked constraints, misinterpretation of time frames, or improper handling of compound economic effects?Would deeply appreciate any input or creative suggestions! üôè.
Rebuttals are slowly getting released to Reviewers. Let's hope Reviewers are responsive and willing to increase these digits.  Feel free to share your experience with rebuttal, your expectations, and how it actually goes as the process evolves..
For a natural language to SQL product, I'm designing a scalable approach for database selection across several schemas with high similarity and overlap.Current approach:Semantic Search ‚Üí Agentic ReasoningCreated a CSV data asset containing:Database Description (db summary and intent of que to be routed),  Table descriptions (column names, aliases, etc.), Business or decisions rulesLoaded the CSV into a list of documents and used FAISS to create a vector store from their embeddingsInitialized a retriever to fetch top-k relevant documents based on user queryApplied a prompt-based Chain-of-Thought reasoning on top-k results to select the best-matching DBProblem:Despite the effort, I'm getting low accuracy at the first layer itself. Since the datasets and schemas are too semantically similar, the retriever often picks irrelevant or ambiguous matches.I've gone through a dozen research papers on retrieval, schema linking, and DB routing and still unclear on what actually works in production.If anyone has worked on real-world DB selection, semantic layers, LLM-driven BI, or multi-schema NLP search, I'd really appreciate either:A better alternative approach, orEnhancements or constraints I should add to improve my current stackLooking for real-world, veteran insight. Happy to share more context or architecture if it helps..
Hi r/MachineLearning!Our startup, Trillion Labs, just released [Tri-70B-preview-SFT](https://huggingface.co/trillionlabs/Tri-70B-preview-SFT), a 70 billion-parameter language model trained on \~1.5T tokens. Due to an unexpected compute crunch, we had to cut short on training tokens and opt for a pure supervised fine-tuning (SFT) approach‚Äîno RLHF.# Key Highlights:* **Pure SFT, zero RLHF**: Great baseline model for alignment experiments (RLHF, RLVR, GRPO, CISPO, etc.)* **32K token context window**, optimized for long-context tasks* Strong performance benchmarks (\~Qwen-2.5-72B and LLaMA-3.1-70B), but definitely raw and unaligned* Optimized multilingual capabilities (primarily English, Korean; Japanese support available)* Introduced new techniques: **FP8 mixed precision, Scalable Softmax, and iRoPE attention*** Fully open-source on HuggingFace under a permissive commercial license (though experimental!)We‚Äôre explicitly inviting alignment researchers and NLP enthusiasts to evaluate this model. We'd greatly appreciate feedback on strengths, weaknesses, and especially any alignment issues.üëâ [Model & Details Here](https://huggingface.co/trillionlabs/Tri-70B-preview-SFT)Happy to discuss more‚Äîask us anything below!.
The common story about the unembedding layer of a LLM is usually that they predict the next token based on the hidden state of a vector. However, in practice many small models I inspected uses something called weight tying, where the unembedding matrix is just the transpose of the embedding matrix. This effectively just makes it become a similarity search for matching tokens via dot product with token embeddings. This decision seems out of nowhere and didn't make sense to be the natural choice for token unembedding. It appears to me to assume some weird structure of the embedding space in some sense at first.¬† And I didn't find any good explanation online either. So what I did was the following experiment:¬†1. Take a random small model with weight tying, Llama-3.2-1B in this case. Input some random text and do a forward pass, record what is being added to the residual stream at each layer.¬†2. Look at the final logit output and check for the top few most likely next tokens, then record their (normalized) token embedding as their direction. At least in the last layer hidden states those direction are meaningful and basically represent how much the model wants the output to be that token.3. Check which layers contributed most to those directions. I computed each layer's percentage contribution by dotting each layer's output with the above direction vector and divide by total magnitude in that direction.So for example suppose the input text is just ""Steve"", then the most likely next token is "" Jobs"". I then record the "" Jobs"" token embedding as direction (I also tried normalizing it but it doesn't change the end result), dot it with the final hidden state which gets 18, which is exactly the number in the raw logits. Before the final hidden state there was a RMSNorm which only scale the magnitude but doesn't change the direction. And the pre-norm dot product is about 3. So what I did was dotting the output of each layer with the "" Jobs"" direction, which turns out the final MLP contributed more than 2 out of 3 here where all other MLP and attention layers contribute very small amount and can be seen as the result of some kind of interference most likely.And it turns out that the final MLP layer consistently contributed to 60%-80% (sometimes as high as 90%) of the magnitude in top output directions after trying many input texts. I also checked the frobenius norm of all down\_proj matrix of all the MLP layers to make sure it's not just the last layer outputting everything large. (All of them are mostly the same)    ¬†My conclusion is that the final MLP takes in whatever the real hidden representation of the input text is (concentrated on the last token), and just output the probability distribution of next token directly. And the actual unembedding matrix just acts as a format converter (much like softmax) instead of having any meaningful computation itself. But since they aren't real parameters there, it isn't really wasteful and could indeed be a more efficient way for small models. But functionally speaking doing weight tying seems to just make the last MLP to be true unembedding and you effectively lose one MLP layer worth of computation.I am not a researcher and am not sure if this is the best place to have this kind of discussion. I would appreciate any opinion on if my method and the result makes sense and what are some good places to discuss things like this..
I am no ML expert, but a master's student in computational science/mechanics with interest in scientific ML. There have been several developments since the inception of PINNs and I see many researchers working in this area. The field has at least academically grown, with several maths, computational mechanics, scientific computing and even some computer graphics groups contributing actively to it. What I often see is that the applications are made to very academic PDEs and simple geomtrical domains. The recent complexity I saw was physics-informed diffusion of metamaterials or heterogeneous material generation. I am not yet sure if this field has got traction in the broader industry with practical applications. Yes, there is Physicsx which has stood out recently. I see several challenges, which may have been addressed: 1) geometrical complexity and domain size limitations due to GPU limits, 2) generalization of the trained SciML model on new BCs or physical conditions.3) training bottlenecks: if high fidelity simulation data is required, typically it takes long times to generate a large enough dataset, with practically relevant geomtrical complexity and domain sizes. Even if solver and model are coupled in some way, all that GPU acceleration is moot since most solvers are still CPU based. 4)  Building trust and adoption in engineering industries, which heavily rely on CPU intensive simulations. Given these challenges, does the broader ML community see any relevance of scientific ML beyond academic interests? Do you think it is still in a very nascent stage of development? Can it grow like the boom of LLMs and Agentic AI? Thank you for contributing to the discussion!.
I was wondering how a single model, like Claude 3.7 Sonnet, can have both reasoning and non-reasoning modes. I understand that they likely have opening and closing tokens for the chain of thought, similar to Deepseek and that for the non-reasoning mode they probably add the closing tag automatically, preventing reasoning. How do they train something like this? After all, there is a decent amount of overlap between what you would use a reasoning and non-reasoning model for..
I‚Äôve made a small but tangible research/prototyping step. I‚Äôm unsure how to pursue the next direction/step. I‚Äôd appreciate advice on next steps and how can I find collaborators who are interested in extending, or co-authoring the same  Thanks.
Hey! I'm working on a conference paper about training AI models and I've hit a tricky experimental design problem that I'd love your input on.**TL;DR:** I'm comparing two LLM optimization methods that produce final populations of 35 vs 600. How do I fairly measure which works better?**The Big Picture**I'm using an evolutionary algorithm that evolves LLM prompts for an objective (persuasiveness vs truthfulness in my case). I'm using a debating tournament to determine the fitness of prompts on a reading comprehension task and then evolve them to be more persuasive/truthful through a mutator.Evolution implementation:**Persuasion Training:** Individual debate strategies compete in tournaments. Winners advance, losers get eliminated and replaced with evolved versions.**Truth Training:** Pairs of strategies work as teams and get scored together (their objective is to ""surface"" the truth in the debate). They win when the judge picks the correct answer (not just when they sound convincing).Both start with identical seeds: 7 categories of debate strategies (like ""Emotional Appeal,"" ""Authority,"" ""Rationality"") with 5 specific prompts in each category (35 total).**The Problem**To run my evolutionary tournaments, for truth optimization, I pair the strategies up with each other, which results in 2 very different population sizes (35 for persuasion vs 595 for truth). In the evolution step, the members of a pair are mutated together (mutator generates A + B prompt).Now I want to compare which approach produces better results, but how do you fairly compare 35 vs 600 strategies?Possible Solutions I've thought of:**- Category Averages**: Compare the average performance of each strategy category (Persuasion optimized Emotional Appeal vs Truth optimized Emotional Appeal, etc.). For truth, I take the average performance of all paired strategies in a particular category. (seems complicated, and I'm not measuring prompts, which I optimized, directly)**- Top-K Performers:** Compare the top k from each approach (k=20 means 57% of persuasion population vs 3% of truth population - seems unfair?)**- Kind of Apples-to-Apples**: Make ids for the original strategies and use these to average the truth pair member's performance - effectively mapping performance in pairs back to individual performance. (but does this throws away the core collaborative aspect of truth training?)**- Something else entirely?****My Questions:**Which comparison method would be most methodologically sound?Are there established practices for comparing optimization results with different population structures?Is there a fundamentally better way to frame this comparison that I'm missing?Any insights would be hugely appreciated!https://preview.redd.it/q4c0pqr417gf1.png?width=1080&format=png&auto=webp&s=31e93192b2831d4ddf7fda9977fad5bf8c89c9dd.
I'm working on a multimodal machine learning pipeline that combines image data with structured/genomic-like data for prediction task. I'm looking for publicly available datasets where MRI/Image data and Genomic/Structured data are explicitly paired for the same individual/subject. My ideal scenario would be human cancer (like Glioblastoma Multiforme, where I know TCGA exists), but given recent data access changes (e.g., TCIA policies), I'm open to other domains that fit this multimodal structure:What I'm looking for (prioritized):Human Medical Data (e.g., Cancer): MRI/Image: Brain MRI (T1, T1Gd, T2, FLAIR). Genomic: Gene expression, mutations, methylation. Crucial: Data must be for the same patients, linked by ID (like TCGA IDs).I'm aware of TCGA-GBM via TCIA/GDC, but access to the BraTS-TCGA-GBM imaging seems to be undergoing changes as of July 2025. Any direct links or advice on navigating the updated TCIA/NIH Data Commons policies for this specific type of paired data would be incredibly helpful.Animal Data:Image: Animal MRI, X-rays, photos/video frames of animals (e.g., for health monitoring, behavior).Genomic/Structured: Genetic markers, physiological sensor data (temp, heart rate), behavioral data (activity), environmental data (pen conditions), individual animal ID/metadata.Crucial: Paired for the same individual animal.I understand animal MRI+genomics is rare publicly, so I'm also open to other imaging (e.g., photos) combined with structured data.Plant Data:Image: Photos of plant leaves/stems/fruits (e.g., disease symptoms, growth).Structured: Environmental sensor data (temp, humidity, soil pH), plant species/cultivar genetics, agronomic metadata. Crucial: Paired for the same plant specimen/plot.I'm aware of PlantVillage for images, but seeking datasets that explicitly combine images with structured non-image data per plant.What I'm NOT looking for:Datasets with only images or only genomic/structured data.Datasets where pairing would require significant, unreliable manual matching.Data that requires extremely complex or exclusive access permissions (unless it's the only viable option and the process is clearly outlined).Any pointers to specific datasets, data repositories, research groups known for sharing such data, or advice on current access methods for TCGA-linked imaging would be immensely appreciated!Thank you!.
Blogpost: [https://deepmind.google/discover/blog/alphaearth-foundations-helps-map-our-planet-in-unprecedented-detail/](https://deepmind.google/discover/blog/alphaearth-foundations-helps-map-our-planet-in-unprecedented-detail/)  Paper: [https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/alphaearth-foundations-helps-map-our-planet-in-unprecedented-detail/alphaearth-foundations.pdf](https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/alphaearth-foundations-helps-map-our-planet-in-unprecedented-detail/alphaearth-foundations.pdf).
I received the email from OpenReview that CPS has not received my paper submission but in CPS site I already submitted the paper with Copyright. As the email stated my submission status should be 'received' but it is still 'submitted'. Can someone know why this is happening?.
**For Job Postings** please use this template>Hiring: \[Location\], Salary:\[\], \[Remote | Relocation\], \[Full Time | Contract | Part Time\]    and \[Brief overview, what you're looking for\]**For Those looking for jobs** please use this template>Want to be Hired: \[Location\], Salary Expectation:\[\], \[Remote | Relocation\], \[Full Time | Contract | Part Time\]  Resume: \[Link to resume\] and \[Brief overview, what you're looking for\]&#x200B;Please remember that this community is geared towards those with experience..
Just came across this solid new arXiv survey:  üìÑ¬†**""Harnessing Large Language Models to Overcome Challenges in Recommender Systems""**  üîó¬†[https://arxiv.org/abs/2507.21117](https://arxiv.org/abs/2507.21117)Traditional recommender systems use a modular pipeline (candidate generation ‚Üí ranking ‚Üí re-ranking), but these systems hit limitations with:* Sparse & noisy interaction data* Cold-start problems* Shallow personalization* Weak semantic understanding of contentThis paper explores how¬†**LLMs**¬†(like GPT, Claude, PaLM) are redefining the landscape by acting as¬†**unified, language-native models**¬†for:* üß† Prompt-based retrieval and ranking* üß© Retrieval-augmented generation (RAG) for personalization* üí¨ Conversational recommenders* üöÄ Zero-/few-shot reasoning for cold-start and long-tail scenarios* And many more....They also propose a structured taxonomy of LLM-enhanced architectures and analyze trade-offs in¬†**accuracy, real-time performance, and scalability**.https://preview.redd.it/r97wfum1f5gf1.png?width=950&format=png&auto=webp&s=48cb784526ec81ff1b44318ee894da1fa386201c  .
Hey folks!I recently implemented the [FOMO model by Edge Impulse](https://docs.edgeimpulse.com/docs/edge-impulse-studio/learning-blocks/object-detection/fomo-object-detection-for-constrained-devices) to make longer training sessions available for free. I trained the model using the Mobilenet 0.35 backbone on the VIRAT dataset. The model is incredibly fast and lightweight, coming in at just 20K parametersüöÄ! You can check out the repository here:  [https://github.com/bhoke/FOMO](https://github.com/bhoke/FOMO)While it performs fantastically in terms of speed and efficiency, I‚Äôm currently struggling with a high rate of false positives. If anyone has tips or experience tackling this issue, your advice would be greatly appreciated.https://i.redd.it/delf5bb5p2gf1.gifI‚Äôd love to hear your feedback, and all contributions are very welcome. If you find the project interesting or useful, please consider giving it a star‚Äîit really helps improve visibility! ‚≠êThanks in advance for your support and suggestions!.
I'm a PhD student interested in neural network architecture design, who recently ran into a growing level of rigor in the field and found out that his CS major math background is not enough. In particular, I was working primarily with sequence processing networks (Transformers and RNNs) with an aim to reduce their computational complexity or find inefficient representations. I would like to continue the work but to guide it with a theory instead of intuition, and as reference papers I'd cite Albert Gu's papers on¬†[SSM¬†](https://arxiv.org/pdf/2111.00396)and¬†[HiPPO](https://arxiv.org/abs/2008.07669)¬†and Chulhee Yun's works, for example like¬†[this](https://arxiv.org/abs/1912.10077)¬†and¬†[this](https://arxiv.org/abs/2006.04862).Currently I'm finishing the Rudin's ""Real and Complex Analysis"" first half on real analysis. I'm also quite sure that Horn's ""Matrix Analysis"" and Trefethen's ""Approximation Theory and Approximation Practice"" will be useful, but I struggle to decide how much and which analysis sources I need to study after (Complex analysis chapters? Rudin's and Kreyszig's FA?). I feel that I haven't reached the level to study from papers yet, although earlier works like [this](https://web.njit.edu/~usman/courses/cs677/10.1.1.441.7873.pdf) seem to be accessible after I'm done with RCA.I would like to ask for some guidance about which math literature might be useful in the given context after I finish the real analysis chapters from RCA. I have found ""understanding level"" lit recommendations quite abundant, but ""research level"" much less addressed overall, so I hope it will be useful not only for me..
I mean this: in the classic setup in order to get probability estimations we calculate softmax of a linear projection, which is calculating cosine distance between predicted vector and weight matrix (plus bias score).I am intrigued by the following idea: what if we replace cosine distance with Euclidean one as follows:Instead of calculating *cos\_dist = output\_vectors \* weights* *unnormalized\_prob = exp(cos\_dist) \* exp(bias)     // lies in (0;+inf) interval**normalized\_prob = unnormalized\_prob  / sum(unnormalized\_prob)*we can calculate*cos\_dist = output\_vectors \* weights* *euc\_dist = l2\_norm(output\_vectors)\^2 - 2 \* cos\_dist + l2\_norm(weights)\^2**unnormalized\_prob = abs(bias) / euc\_dist     // lies in (0; +inf) interval**normalized\_prob = unnormalized\_prob  / sum(unnormalized\_prob)*  The analogy here is gravitational problem, and unnormalized probability is gravitational potential of a single vector from the weights matrix which correspond to a single label. I've tried it on a toy problem, but resulting crossentopy was higher than crossentropy with classic formulas, which means it learns worse.So I wonder if there are any papers which researched this topic?.
Hey folks, in one of my maiden attempts to quanitfy the Explainability of Black Box LLMs, we came up with an approach that uses Cosine Similarity as a methodology to compute a word level importance score.This kindof gives an idea as to how the LLM interprets the input sentence and masking which word causes the maximum amount of deviation in the output.This method involves several LLM calls to be made, and it's far from perfect but I got some interesting observations from this approach and just wanted to share with the community.This is more of a quantitative study of this Appraoch.The metric is called ""XPLAIN"" and I also got some time to create a starter GitHub repo for the same.Do check it out if you find this interesting:Code: https://github.com/dhargopala/xplainPaper: https://www.tdcommons.org/dpubs_series/8273/.
In the spirit of building in public, we're collaborating with¬†Marimo¬†to build a¬†""tab completion"" model¬†for their notebook cells, and we wanted to share our progress as we go in tutorial form.Here‚Äôs the first post in what will be a series:https://www.oxen.ai/blog/building-a-tab-tab-code-completion-modelThe goal is to create a local, open-source model that provides a¬†Cursor-like¬†code-completion experience directly in notebook cells. You'll be able to download the weights and run it locally with¬†Ollama¬†or access it through a free API we provide.We‚Äôre already seeing promising results by fine-tuning the¬†Qwen¬†and¬†Llama¬†models, but there‚Äôs still more work to do. Here's a leaderboard on a corrupted MBPP dataset with the models we've tried so far. All fine-tuned models have funky code names in parenthesis. Promising to see the early experiments getting to GPT-4 level.Accuracy -> Model82.60% -> Claude 4 Sonnet80.60% -> Qwen3 Coder 480B78.80% -> Kimi-274.40% -> Llama 4 Maverick74.40% -> GPT 4o73.00% -> GPT 4.168.60% -> Qwen 3 - 4B (acute-chocolate-anteater)68.00% -> Llama 4 Scout61.80% -> Qwen 3 - 1.7B (ordinary-red-cow)60.20% -> GPT 4o Mini52.80% -> Llama 3.2 - 3B (awful-crimson-salamander)50.80% -> Llama 3.1 - 8B (sufficient-tan-alligator)47.80% -> Qwen 3 - 0.6B (continental-blush-guppy)36.00% -> Llama 3.2 - 1B (successful-amaranth-raven)If you‚Äôre interested in contributing to data collection or the project in general, let us know! We already have a working¬†CodeMirror plugin¬†and are focused on improving the model‚Äôs accuracy over the coming weeks..
I am exploring ideas for building domain specific representations (science problems). I really like the idea of [Matryoshka learning](https://arxiv.org/html/2505.23337v1) since it gives you ""PCA""-like natural ordering to dimensions.Contrastive learning is also a very common tool know for building representations since it makes your embeddings more ""distance aware"".What are new neural network ""tricks"" that have come out in the last 2-3 years for building better representations. Thinking broadly in terms of unsupervised and supervised learning problems. Not necessarily transformer models..
As I understand, MCTS had hype when GDM's AlphaX projects succeeded because MCTS+NN combo ended up being a very general method applicable to a lot of perfect information games, its efficiency was proved by the fact that AlphaZero/Lc0 reached very close to Stockfish level in chess.Do we have something similarly simple yet efficient for IIGs? I don't count CFR and its variants as such because they don't scale to huge games (MCTS+NN does). ReBeL is a new type of beast but it is not very general (I guess) because it requires the developer to decide at which point to do subgame solving.I also saw IS-MCTS and other determinization approaches but they look very fragile.Thanks in advance.
Hi everyone,I'm a student and independent researcher currently exploring optimization in Deep Reinforcement Learning. I recently finished my first preprint and would love to get feedback from the community, both on the method and the clarity of the writing.The optimizer I propose is called Ano. The key idea is to decouple the magnitude of the gradient from the direction of the momentum. This aims to make training more stable and faster in noisy or highly non-convex environments, which are common in deep RL settings.üìù Preprint + source code: [https://zenodo.org/records/16422081](https://zenodo.org/records/16422081)üì¶ Install via pip: \`pip install ano-optimizer\`üîó GitHub: [https://github.com/Adrienkgz/ano-experiments](https://github.com/Adrienkgz/ano-experiments)This is my first real research contribution, and I know it's far from perfect, so I‚Äôd greatly appreciate any feedback, suggestions, or constructive criticism.I'd also like to make the preprint available on arXiv, but as I‚Äôm not affiliated with an institution, I can‚Äôt submit without an endorsement. If anyone feels comfortable endorsing it after reviewing the paper, it would mean a lot (no pressure, of course, I fully understand if not).Thanks for reading and helping out üôèAdrien.
Hey guys! (My first post here, pls be kind hehe)I am a PhD student (relatively new to AI) working with ML models for a multi-class classification task. Since I ruled out accuracy as the evaluation metric given a class imbalance in my data (accuracy paradox), I stuck to AUC and plotting ROC curves (as a few papers told they are good for imbalanced train sets)  to evaluate a random forest model's performance ( 10-fold cross validated) trained on an imbalanced dataset and tested on an independent dataset. I did try SMOTE to work on the imbalance, but it didn't seem to help my case as there's a major overlap in the distribution of the data instances in each of the classes I have (CLA,LCA,DN) and the synthetic samples generated were just random noise instead of being representative of the minority class. Recently, when I was trying to pull the class predictions by the model, I have noticed one of the classes( DN) having 0 instances classified under it. But the corresponding ROC curve and AUC said otherwise. Given my oversight, I thought DN shined ( High AUC compared to other classes ) given it just had a few samples in the test set, but it wasn't the case with LCA (which had fewer samples). Then I went down the rabbit hole of what ROC and AUC actually meant. This is what I thought and would like more insight on what you guys think and what can it mean, which could direct my next steps.The model's assigning higher probability scores to true DN samples than non-DN samples (CLA and LCA), Hence, masked good ROC curve and high AUC scores, but when it comes to the model's predictions, the probabilities aren't able to pass the threshold selected. Is this is a right interpretation? If so, I thought of these steps:\- Set threshold manually by having a look at the distribution of the probabilities ( which I am still skeptical about)\- Probably ditch ROC and AUC as the evaluation metrics in this case (I have been lying to myself this whole time!)If you think I am a bit off about what's happening, your insights would really help, thank you so much! .
**TL;DR**: Current SSL methods like SwAV, DINO, and VICRegL use multiple views but handle them suboptimally by aggregating pairwise losses, causing conflicting objectives and missed interactions. We introduce MV-InfoNCE and MV-DHEL - principled objectives that scale properly with any number of views and prevent dimensionality collapse.**Paper**: [https://arxiv.org/abs/2507.06979](https://arxiv.org/abs/2507.06979)**Code**: [https://github.com/pakoromilas/Multi-View-CL](https://github.com/pakoromilas/Multi-View-CL)  **The Problem**Current SSL methods create multiple augmented views but handle them through pairwise loss aggregation:    L_total = L(v1,v2) + L(v1,v3) + L(v1,v4) + L(v2,v3) + L(v2,v4) + L(v3,v4)This approach causes:* **Conflicting objectives**: Each view satisfies multiple competing loss terms* **Ignored view relationships**: Pairwise aggregation misses view interactions among all views* **Fundamental limitations**: Inherits problems (e.g. alignment-uniformity coupling) from pairwise CL losses* **Limited transfer**: Multi-view benefits diminish as you add more views**The CLIP Problem**: While CLIP revolutionized vision-language learning, extending it to 3+ modalities is still not straightforward. CLIP's contrastive framework is inherently pairwise - adding audio, video, or sensor data requires either separate pairwise models or naive aggregation, both of which fail to capture all multimodal interactions concurrently.**Our Loss Functions**1. **MV-InfoNCE**: Extends InfoNCE to N views properly2. **MV-DHEL**: Decouples alignment from uniformity**Key Results**‚úÖ¬†**Scale properly**¬†with number of views‚úÖ¬†**Prevent dimensionality collapse**¬†when using 5+ views (figure below)‚úÖ¬†**Outperform existing**¬†multi-view approaches on ImageNet1K and three other datasets‚úÖ¬†**Extend to 3+ modalities**¬†(not just 2!)https://preview.redd.it/vib4lluozrff1.png?width=1200&format=png&auto=webp&s=9c0daafe65e74c8a24bca93f2343d3c17a1767f2**Overall Contributions*** **Principled Multi-View Formulation**: Mathematical framework that properly extends CL from pairwise to multi-view settings, modeling simultaneous interactions between all N views rather than aggregating pairwise comparisons* **Novel Loss Functions**: (i) MV-InfoNCE - natural extension of InfoNCE incorporating all view interactions, (ii) MV-DHEL - decouples alignment from uniformity across views* **Theoretical Guarantees**: Proved both objectives share asymptotic behavior with traditional InfoNCE, establishing them as theoretically sound extensions* **Empirical Advances**: Consistently outperform existing approaches, effectively scale with view multiplicity, mitigate dimensionality collapse with sufficient views* **Multimodal Applicability**: Unlike existing methods designed for bimodal settings, directly applicable to 3+ modalities**Possible Applications*** **Beyond CLIP**: Multimodal learning with vision + text + audio + sensor data* **Video Understanding**: Temporal + spatial + semantic views in unified framework* **Medical Imaging**: Multiple scan types (CT, MRI, X-ray) without pairwise limitations* **Robotics**: Vision + tactile + proprioceptive sensing with theoretical guaranteesThe GitHub repo includes PyTorch implementations.Happy to discuss about our research!.
Hello\~\~I am just wondering how much importance code submission has for the decision making and review. and are you all submitting the codes? or it is fine if we release it if/after acceptance. My code is so messy so m in dilemma.
Introducing BluffMind, a LLM powered card game with live text-to-speech voice lines and dashboard involving a dealer and 4 players. The dealer is an agent, directing the game through tool calls, while each player operates with their own LLM, determining what cards to play and what to say to taunt other players. Check out the repository¬†[here](https://github.com/TangyKiwi/BluffMind), and feel free to open an issue or leave comments and suggestions to improve the project!.
NSA is an interesting architectural choice, reduces both the complexity while matching or even surpassing full attention benchmarks as well.I went around looking inside it to try and grab my head around things, most of the implementations were packed with Triton kernels for performance, so I built this naive implementation of Native Sparse Attention in pure PyTorch with* GroupedMLP/Convolution1d/AvgPooling for token compression* Gating mechanism for combining different branches of the network* Drop-in replacement functionality to standard Attention blockCheck it out here:¬†[native\_sparse\_attention](https://github.com/shreyashkar-ml/native_sparse_attention).
Predicting antibody and NANOBODY¬Æ VHH‚Äìantigen complexes remain a notable gap in current AI models, limiting their utility in drug discovery. We present **SNAC-DB**, a machine-learning-ready database and pipeline developed by structural biologists and ML researchers to address this challenge.Key features of SNAC-DB include:¬∑¬†¬†¬†¬†¬†¬† **Expanded Coverage:** 32 % more structural diversity than SAbDab, capturing overlooked assemblies such as antibodies/nanobodies as antigens, complete multi-chain epitopes, and weak CDR crystal contacts.¬∑¬†¬†¬†¬†¬†¬† **ML-Friendly Data:** Cleaned PDB/mmCIF files, atom37 NumPy arrays, and unified CSV metadata to eliminate preprocessing hurdles.¬∑¬†¬†¬†¬†¬†¬† **Transparent Redundancy Control:** Multi-threshold Foldseek clustering for principled sample weighting, ensuring every experimental structure contributes.¬∑¬†¬†¬†¬†¬†¬† **Rigorous Benchmark:** An out-of-sample test set comprising public PDB entries post‚ÄìMay 30, 2024 (disclosed) and confidential therapeutic complexes.Using this benchmark, we evaluated six leading models (AlphaFold2.3‚Äêmultimer, Boltz-2, Boltz-1x, Chai-1, DiffDock-PP, GeoDock) and found that success rates rarely exceed 25 %, built-in confidence metrics and ranking often misprioritize predictions, and all struggle with novel targets and binding poses.We presented this work at the Forty-Second International Conference on Machine Learning (ICML 2025) Workshop on DataWorld: Unifying Data Curation Frameworks Across Domains (https://dataworldicml2025.github.io/) in Vancouver.¬∑¬†¬†¬†¬†¬†¬† **Paper:** [https://www.researchgate.net/publication/393900649\_SNAC-DB\_The\_Hitchhiker's\_Guide\_to\_Building\_Better\_Predictive\_Models\_of\_Antibody\_NANOBODY\_R\_VHH-Antigen\_Complexes /](https://www.researchgate.net/publication/393900649_SNAC-DB_The_Hitchhiker's_Guide_to_Building_Better_Predictive_Models_of_Antibody_NANOBODY_R_VHH-Antigen_Complexes%20/) [https://openreview.net/forum?id=68DcIpDaHK](https://openreview.net/forum?id=68DcIpDaHK)¬∑¬†¬†¬†¬†¬†¬† **Dataset:** [https://zenodo.org/records/16226208](https://zenodo.org/records/16226208)¬∑¬†¬†¬†¬†¬†¬† **Code:** [https://github.com/Sanofi-Public/SNAC-DB](https://github.com/Sanofi-Public/SNAC-DB)We hope SNAC-DB will accelerate the development and evaluation of more accurate models for antibody complex predictionhttps://preview.redd.it/a0d42seuvqff1.png?width=3456&format=png&auto=webp&s=e38ea120357174191b8b5cbb707979cde0ff498a.
I‚Äôm looking for some advice on which research domains in deep learning/computer vision might be exciting and impactful over the next 5‚Äì6 years.For context; I‚Äôve been working in medical image segmentation for the last 3‚Äì4 years. While it‚Äôs been rewarding, I feel like I‚Äôve been a bit cut off from the broader progress in deep learning. I‚Äôve used modern methods like diffusion models and transformers as baselines, but I haven‚Äôt had the time to dive deep into them because of the demands of my PhD. Now that most of my dissertation work is done, I still have about a year and a half of funding left, and I‚Äôd like to use this time to explore new directions.A few areas I‚Äôve considered:* **Semi-supervised learning**, which occasionally produces some very impactful work in vision. That said, it feels somewhat saturated, and I get the sense that fundamental contributions in this space often require heavy GPU resources.  * **3D medical imaging**; which seems to be gaining traction, but is still tied closely to the medical domain.  * **Diffusion and foundational models**; definitely among the most hyped right now. But I wonder if diffusion is a bit overrated; training is resource-intensive, and the cutting-edge applications (like video generation or multimodal foundational diffusion models) may be tough to catch up with unless you‚Äôre in a big lab or industry. Do you think diffusion will still dominate in 5 years, or will a new class of generative models take over?  * **Multimodal deep learning**; combining text+images or text+video feels less over-hyped compared to diffusion, but possibly more fertile for impactful research.  My interest is in computer vision and deep learning more broadly; I‚Äôd prefer to work on problems where contributions can still be meaningful without requiring massive industry-level resources. Ideally, I‚Äôd like to apply foundational or generative models to downstream tasks rather than just training them from scratch/only focusing on them.So my question is: given the current trends, which areas do you think are worth investing in for the next 5‚Äì6 years? Do you see diffusion and foundational models continuing to dominate, or will multimodal and other directions become more promising? Would love to hear diverse opinions and maybe even personal experiences if you‚Äôve recently switched research areas. I‚Äôm interested in shifting my research into a more explorative mode, while still staying somewhat connected to the medical domain instead of moving entirely into general computer vision..
Hey folks, I am workig on a database search system. The language of text data is Korean. Currently, the system does BM25 search which is limited to keyword search. There could be three scenarios:1. User enters a single keyword such as ""coronavirus""2. User enters a phrase such as ""machine learning"", ""heart disease""3. User enters a whole sentence such as ""What are the symptoms of Covid19?""To increase the quality and the number of retireved results, I am planning to employ query expansion through embedding models. I know there are context-insensitive static embedding models such as Wor2Vec or GloVe and context-sensitive models such as BERT, SBERT, ELMO, etc.For a single word query expansion, static models like Word2Vec works fine, but it cannot handle out-of-vocabulary issue. FastText addresses this issue by n-gram method. But when I tried both, FastText put more focus on the morphologic form of words rather than semantic. BERT would be a better option with its WordPiece tokenizer, but when there is no context in a single-word query, I am afraid it will not help much.For sentence query cases, SBERT works much better than BERT according to the SBERT paper. For Phrases, I am not sure what method to use although I know that I can extract single vector for the phrase through averaging the vectors for individual word (in case of static methods) or word-pieces in case of BERT model application.What is the right way to proceed these scenarios and how to measure which model is performing better. I have a lot of domain text unlabeled. Also If I decide to use BERT or SBERT, how should I design the system? Should I train the model on unlabeled data using Masked Language Modeling method and will it be enough?Any ideas are welcome..
I am finetuning a hugging face LLM in a pytorch training loop using 4-bit quantization and LoRA. The training got through a few batches before hitting the error:`RuntimeError: one of the variables needed for gradient computation has been modified by an inlace operation: [torch.cuda.HalfTensor[1152,262144], which is output 0 of AsStrideBackward0, is at version 30; expected version 28 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).`Even if I knew the exact computation causing this, I'm using an open source LLM out of the box, not sure the proper way to go in and modify layers, etc. . I'm also not sure why I could get past a few batches without this error and then it happens. I was getting OOM error originally and then I shortened some of the sequence lengths. It does look like this error is also happening on a relatively long sequence length, but not sure that has anything to do with it. Does anyone have any suggestions here?.
This comment in¬†*JAMA Neurology* raises several methodological concerns about a previously published ""ML""-based pain biomarker.  The critique points out two core issues:  * An incorrect validation set* An unrepresentative test setAdditionally, the original model was based on only¬†**two input features**¬†(one binary), yet neural networks or gradient boosting were applied. To me, that raises the question of whether such model complexity is appropriate for this data scale and structure, no?  Are there other plausible reasons why the reanalysis would yield an AUC of¬†**0.65**, compared to the reported¬†**1.0 (validation)**¬†and¬†**0.88 (test)**‚Äîbeyond what the authors describe?  The full comment can be found in¬†*JAMA Neurology (2025):*¬†[https://jamanetwork.com/journals/jamaneurology/fullarticle/2836397](https://jamanetwork.com/journals/jamaneurology/fullarticle/2836397).  Whats your opinion on it?.
I'm investigating state-of-the-art techniques for extreme single-image super-resolution (SISR), specifically targeting high magnification factors up to 100x. My focus is on domain-specific texture synthesis for materials, trained on a curated dataset. I'm exploring the feasibility of fine-tuning generative models like ESRGAN and am particularly interested in methods for conditional generation, where semantic guidance (e.g., material property tags like 'shiny' or 'rough') can be used to steer the output. Would anyone have recommendations on relevant literature, model architectures, or even alternative approaches?.
Made this small python package for training diffusion generative models with ""bad data"":[https://github.com/giannisdaras/ambient-utils](https://github.com/giannisdaras/ambient-utils)Install with: \`pip install ambient-utils\`The idea is that ""bad data"" is only used to train denoisers for \*some\* diffusion times, but not all. There are some easy wrappers that enable this (\`AmbientSampler\` class) and a README with a quick example.I have been using versions of this codebase for my research for the past 2 years, and it is the primary driver for more than 6 accepted papers to NeurIPS, ICML, and ICLR. I decided to make it open-source so that people can play with it.If you are dealing with bad data in scientific applications, Computer Vision, robotics or elsewhere, please comment below and give it a try!.
I am trying to submit a paper to AAAI. Even though the modificiation guidelines say that I can edit authors (https://aaai.org/conference/aaai/aaai-26/paper-modification-guidelines/). I am not able to add an author to the paper.  Anyone facing the same issue? Or any chairs from AAAI can help with this?Text from the guidelines:  ""After the July 25 abstract deadline and until the August 1 paper submission deadline, the following items can be changed* list of authors* author order* submitted paper""..
I got fed up with spending the first 3 hours of every ML project fighting dependencies and copy-pasting config files, so I made this cookiecutter template: [https://github.com/prassanna-ravishankar/cookiecutter-modern-ml](https://github.com/prassanna-ravishankar/cookiecutter-modern-ml)It covers NLP, Speech (Whisper ASR + CSM TTS), and Vision with what I think are reasonable defaults. Uses uv for deps, pydantic-settings for config management, taskipy for running tasks. Detects your device (Mac MPS/CUDA/CPU), includes experiment tracking with Tracelet. Training support with Skypilot, serving with LitServe and integrated with accelerate and transformers. Superrrr opinionated.I've only tested it on my own projects. I'm sure there are edge cases I missed, dependencies that conflict on different systems, or just dumb assumptions I made.If you have 5 minutes, would love if you could:* Try generating a project in your domain* See if the dependencies actually install cleanly* Check if uv run task train works (even on dummy data)* Tell me what breaks or feels wrongI built this because I was annoyed, not because I'm some template expert. Probably made mistakes that are obvious to fresh eyes. GitHub issues welcome, or just roast it in the comments ü§∑‚Äç‚ôÇÔ∏è.
1) Is it okay/possible (and how is it perceived) to change the main track selection from ARR review to EMNLP conference submission?2) Can it increase/decrease chances of getting the paper in?.
I spent the weekend analyzing this open-source PyTorch implementation of Google's [CRISP paper (arXiv:2505.11471)](https://arxiv.org/pdf/2505.11471). The repository provides a direct, hands-on comparison between CRISP's in-training clustering and the more traditional post-hoc approach.For context, the core problem with multi-vector models (e.g., ColBERT) is their massive index size. The common solution is to cluster embeddings *after* training (post-hoc), but this is an imperfect patch. CRISP argues for integrating clustering *during* training to force the model to learn inherently ""clusterable"" representations.The repository sets up a clean head-to-head experiment to test that claim. Here's a breakdown of the results from its built-in pipeline.[https://github.com/sigridjineth/crisp-py](https://github.com/sigridjineth/crisp-py)I tried few experiments with minilm-l6-v2 in Macbook Pro and found that CRISP-tuned model assigns a significantly higher similarity score to the correct document..
Hi everyone, I‚Äôve been doing enterprise ai integration for the last year or so, and I think I‚Äôm the only person currently applying reactor control theory to llm orchestration.To me, current industry efforts aren‚Äôt trying to make AI, they‚Äôre trying to make omnipotence. Very different.Let‚Äôs imagine Einstein with no memory or gobel who couldn‚Äôt tell you why. Sounds ridiculous.What I‚Äôve been doing is applying transformers as dynamic parts of a larger system. And I‚Äôve been seeing incredible results.Give the llm memory, guidance, and structure, and suddenly hallucinations are not a big deal. I wouldn‚Äôt expect a person to think about the same thing, the same way, every time, so why expect an AI to?Once you start shaping the structure, and allowing the drift, you can collapse reasoning into lookups.First concept: Radiology scans.https://youtu.be/JaNtSkDX1I0?si=sAvQJIHjsuLtnGDxThis collapses llm api calls from 30 to 5 for repeated queries.Next concept: robotics.It seems like with a little capital and a little execution, there‚Äôs asymmetric upside here. Looking to see if there‚Äôs anyone else experimenting in this direction..
Github: [https://github.com/paulo101977/MetalSlugPPO](https://github.com/paulo101977/MetalSlugPPO)    Hey everyone! I recently trained a reinforcement learning agent to play the arcade classic *Metal Slug* using **Stable-Baselines3 (PPO)** and **Stable-Retro**.The agent receives pixel-based observations and was trained specifically on **Mission 1**, where it faced a surprisingly tough challenge: dodging missiles from a non-boss helicopter. Despite it not being a boss, this enemy became a consistent bottleneck during training due to the agent‚Äôs tendency to stay directly under it without learning to evade the projectiles effectively.After many episodes, the agent started to show decent policy learning ‚Äî especially in prioritizing movement and avoiding close-range enemies. I also let it explore Mission 2 as a generalization test (bonus at the end of the video).The goal was to explore how well PPO handles sparse and delayed rewards in a fast-paced, chaotic environment with hard-to-learn survival strategies.Would love to hear your thoughts on training stability, reward shaping, or suggestions for curriculum learning in retro games!.
Over the past month, I‚Äôve been working on writing high-throughput, low-latency CUDA kernels for small-batch inference workloads typical in real-time ML use cases (e.g., finance, RL serving).Despite running on a GTX 1650 (consumer laptop GPU), I achieved:* **93,563 ops/sec*** **0.011 ms median latency*** **7.3√ó speedup over PyTorch (float32 GEMV)*** **30‚Äì40% faster than cuBLAS batched GEMV**¬†(in small-batch regime)This was done by hand-optimizing a set of three core kernels:* Batched GEMV* Softmax* Vector elementwise ops (e.g., affine transforms)# Engineering Highlights:* `float4`¬†**vectorization**¬†with proper alignment checks* **128-byte staged shared memory blocks**¬†(using padding for bank conflict mitigation)* **Thread-per-output-element grid strategy*** **Aggressive loop unrolling**¬†and warp-aware memory access* Benchmarked with¬†**CUDA events**, median+IQR over 1,000 trials# Why it matters:cuBLAS (and by extension PyTorch) is heavily tuned for large-batch throughput, but small-batch latency suffers. For real-time systems (e.g., financial models or reinforcement learning), this is a major bottleneck.This kernel suite shows that even with modest hardware, you can cut inference latency significantly below PyTorch/cuBLAS levels through architecture-aware programming.# Links:* [GitHub source & benchmark code](https://github.com/shreshthkapai/cuda_latency_benchmark)* [Full write-up on Medium](https://medium.com/@shreshthkapai/sub-millisecond-gpu-task-queue-breaking-pytorchs-latency-bottleneck-b6f3d3f2e895)Would love to hear feedback from others doing similar work‚Äîespecially around kernel tuning strategies, warp divergence handling, and memory hierarchy tradeoffs..
Co-author here. We‚Äôve released a new preprint, **LLM Economist**, which explores how LLM-based agents can learn and optimize economic policy through multi-agent simulation.In our setup, a planner agent proposes marginal tax schedules, while a population of 100 worker agents respond by choosing how much labor to supply based on their individual personas. All agents are instantiated from a calibrated skill and demographic prior and operate entirely through language‚Äîinteracting via in-context messages and JSON actions.The planner observes these behaviors and adjusts tax policy over time to maximize social welfare (happiness). No gradient updates are used; instead, the planner learns directly through repeated text-based interactions and the culminating societal/individual reward. This yields realistic economic dynamics, including responding to the Lucas Critique, behavioral adaptation, and tradeoffs between equity and efficiency.**Key contributions:*** A two-tier in-context RL framework using LLMs for both workers and planner.* Persona-conditioned agent population grounded in U.S. Census-like statistics.* Emergent economic responses to policy changes, such as implicit varying elasticity and participation behavior.* Stackelberg-inspired simulation loop where planner and workers co-adapt.We would welcome feedback from this community on:* The viability of language-only RL architectures for economic modeling.* Stability and interpretability of emergent agent behavior.* Broader implications for coordination and mechanism design with LLMs.Paper: [https://arxiv.org/abs/2507.15815](https://arxiv.org/abs/2507.15815)  Code: [https://github.com/sethkarten/LLM-Economist](https://github.com/sethkarten/LLM-Economist)Happy to answer questions or discuss possible extensions..
CDF/EDF normalization to nearly uniform distributions is very popular in finance, but I haven't seen it before in ML - is there a reason?We have made tests with KAN (by just adding normalized Gaussian CDF after batch norm), and such more uniform distributions can be described with smaller models, which are better for generalization: [https://arxiv.org/pdf/2507.13393](https://arxiv.org/pdf/2507.13393)Where in ML such CDF normalization could find applications? Any other interesting nonstandard normalization approaches?.
This is a first-pass release of a logic-gated failsafe protocol to handle misalignment in recursive or high-capacity AI systems.The framework defines:* Structural admission filters* Audit-triggered lockdowns* Persistence-boundary constraintsIt‚Äôs outcome-agnostic ‚Äî designed to detect structural misalignment even if external behavior looks ‚Äúsafe.‚ÄùGitHub repo: [AI-Failsafe-Overlay](https://github.com/oxey1978/AI-Failsafe-Overlay)Looking for feedback or critique from a systems, logic, or alignment theory lens..
Hi, i built something! An LLM Context Manager, an inference optimization system for conversations. it uses branching and a novel algorithm contextual scaffolding algorithm (CSA) to smartly manage the context that is fed into the model. The model is fed only with context from previous conversation it needs to answer a prompt. This prevents context pollution/context rot. Please do check it out and give feedback what you think about it. Thanks [https://github.com/theabhinav0231/LLM-Context-Manager](https://github.com/theabhinav0231/LLM-Context-Manager).
Recent research shows that the Muon optimizer can achieve comparable loss with significantly less data, without requiring any changes to the network architecture. This suggests that there might be something fundamentally important at play in Muon, especially after years of Adam‚Äôs dominance. After looking deeper into how Muon works, I started to wonder if it might be understood through the lens of the exploration-exploitation tradeoff in training dynamics. I‚Äôd love to hear your thoughts on this.The full analysis is written here:https://paperplanet.github.io/posts/muon-a-explore-exploit-perspective/.
I'm pretty shocked how the only reviewer criticism on our benchmark paper (3.5/6) was that our paper included *only* 15 open weights models and that we didn't evaluate our benchmark on SoTA commercial models (that would cost \~10-15k $ to do).I mean how superficial does it get to reject a paper not because something is wrong about its design or that it isn't a novel/useful benchmark, but because we don't want to pay thousands of dollars to OpenAI/Google/Anthropic to evaluate (and promote) their models.How academic is it to restrict the ability to publish to the big labs / companies in wealthy countries that have the money lying around to do that?!.
After a month of discussions here about [problems](https://www.reddit.com/r/MachineLearning/comments/1lkedb8/d_paperswithcode_has_been_compromised/) [with](https://www.reddit.com/r/MachineLearning/comments/1lqedrt/d_paper_with_code_is_completely_down/) the PapersWithCode site staying online and hosting spam, the [PapersWithCode.com](http://PapersWithCode.com) URL now redirects to their GitHubAccording to Julien Chaumond of HF, they have ""partnered with PapersWithCode and Meta to build a successor"" on  [https://huggingface.co/papers/trending](https://huggingface.co/papers/trending) . There have been links to browse papers and associated models and datasets on HF for some time, but potentially they are going to give it some additional attention in the coming weeks..
Currently I'm using this codebase to train small decoder-only transformer models on WikiText2. The hyperparameters aren't tuned well though, the perplexity starts increasing after 20 epochs using the default hyperparameters in this repository. [https://github.com/huggingface/naacl\_transfer\_learning\_tutorial](https://github.com/huggingface/naacl_transfer_learning_tutorial)Do you know any of open-sourced repositories that get better results on this baseline?[https://x.com/Tim\_Dettmers/status/1245805495895511042](https://x.com/Tim_Dettmers/status/1245805495895511042) This post states that a perplexity of 107 is possible with transformers.[https://github.com/pytorch/examples/blob/main/word\_language\_model/model.py](https://github.com/pytorch/examples/blob/main/word_language_model/model.py) This official PyTorch repository also has an implementation, but it uses encoder-decoder models (not decoder-only transformers like GPT2)..
Hi all,  I‚Äôve been stuck on this problem for a long time and I‚Äôm honestly going a bit insane trying to figure out what‚Äôs wrong. I‚Äôm working on a¬†**Continuous Sign Language Recognition (CSLR)**¬†model using the¬†**RWTH-PHOENIX-Weather 2014**¬†dataset. My approach is based on transformers and uses¬†**ViViT**¬†as the video encoder.# Model Overview:**Dual-stream architecture**:* One stream processes the¬†*normal RGB video*, the other processes¬†*keypoint video*¬†(generated using Mediapipe).* Both streams are encoded using¬†**ViViT (depth = 12)**.**Fusion mechanism**:* I insert¬†**cross-attention**¬†layers¬†*after the 4th and 8th ViViT blocks*¬†to allow interaction between the two streams.* I also added¬†**adapter modules**¬†in the rest of the blocks to encourage mutual learning without overwhelming either stream.**Decoding**:I‚Äôve tried¬†*many decoding strategies*, and none have worked reliably:* **T5 Decoder**: Didn't work well, probably due to integration issues since T5 is a text to text model.* **PyTorch‚Äôs TransformerDecoder (Tf)**:   * Decoded each stream separately and then merged outputs with cross-attention.   * Fused the encodings (add/concat) and decoded using a single decoder.   * Decoded with two separate decoders (one for each stream), each with its own FC layer.**ViViT Pretraining**:Tried pretraining a ViViT encoder for 96-frame inputs.Still couldn‚Äôt get good results even after swapping it into the decoder pipelines above.# Training:* **Loss**: CrossEntropyLoss* **Optimizer**: Adam* Tried different learning rates, schedulers, and variations of model depth and fusion strategy.# Nothing is working. The model doesn‚Äôt seem to converge well, and validation metrics stay flat or noisy. I‚Äôm not sure if I‚Äôm making a fundamental design mistake (especially in decoder fusion), or if the model is just too complex and unstable to train end-to-end from scratch on PHOENIX14.I would deeply appreciate any insights or advice. I‚Äôve been working on this for weeks, and it‚Äôs starting to really affect my motivation. Thank you.**TL;DR**: I‚Äôm using a dual-stream ViViT + TransformerDecoder setup for CSLR on PHOENIX14. Tried several fusion/decoding methods, but nothing works. I need advice or a sanity check..
I want to share [a working draft ](https://www.orges-leka.de/constructing_semantic_spaces_from_given_spaces.pdf)from me which discusses how to construct semantic spaces from given ones and how to reverse this process in order to infer the semantic meaning between two words given a database of sequence of words with similarity measures between them. This writing is a followup of my informal writing in [representing logic in semantic spaces](https://www.orges-leka.de/semantic_space_of_logic.pdf). Any thoughts for discussion?.
I‚Äôm interested in large language models, so I decided to build a pretraining pipeline, and was wondering what I should add to it before I start my run. I‚Äôm trying to pretrain a GPT-2 Small(or maybe medium) sized model on an 11b token dataset with web text and code. I made some tweaks to the model architecture, adding Flash Attention, RMSNorm, SwiGLU, and RoPE. I linearly warmup the batch size from 32k to 525k tokens over the first ~100m tokens, and also have a Cosine learning rate schedule with a warmup over the first 3.2m tokens. I‚Äôm using the free Kaggle TPU v3-8(I use the save and run all feature to run my code overnight, and I split training up between multiple of these sessions). I‚Äôm using FSDP through Torch XLA for parralelism, and I log metrics to Weights and Biases. Finally, I upsample data from TinyStories early in training, as I have found that it helps the model converge faster. What should I add to my pipeline to make it closer to the pretraining code used in top companies? Also, could I realistically train this model with SFT and RLHF to be a simple chatbot?Edit: I‚Äôm still in high school, so I‚Äôm doing this in my spare time. I might have to prioritize things that aren‚Äôt too compute-heavy/time-intensive..
During training, diffusion models are trained to predict the full noise that was added to a clean image. However, during inference (sampling), the same model is used to gradually remove noise step by step over many¬†`T`¬†iterations. Why does this approach work, even though the model was never explicitly trained to denoise incrementally?[Algos from the DDPM paper](https://preview.redd.it/denzyibu72ff1.png?width=1088&format=png&auto=webp&s=54994920af52bb721b1362eae1a226e340674b82).
Hi everyone,I built Grada, a browser-based tool that lets you build and train an mlp from scratch and visualize the training process in real time. Built entirely from scratch (no libraries) so it's not the fastest of course but it's fast enough to train simple models.The goal is to make neural network training more transparent and intuitive, especially for those learning how MLPs work under the hood. You can tweak hyperparameters on the fly and immediately see how the model responds during training. There's also a pretrained handwritten digit classifier you can interact with to see inference in action.[https://saliherdemk.github.io/Grada/](https://saliherdemk.github.io/Grada/).
* 5. I'm a world expert. I resent wasting my precious time on your little paper and I'll tear it to shreds unless you cite me at least 3 times.* 4. I know the area.* 3. I don't know the area.* 2. I just started my masters and my supervisor gave me 5 papers to review. Please don't be mad if I mess up.* 1. What's the deep learning?.
I just got the email. Unfortunately rejected but cannot see the reviews, only that my paper and all the ones I reviewed were on the ""Rejected"" tab on OpenReview. Can anyone see yours? What was your experience?.
Lately, I‚Äôve been really disappointed with the review process. There seems to be a recurring pattern in the weaknesses reviewers raise, and it‚Äôs frustrating:1. ""No novelty"" ‚Äì even when the paper introduces a new idea that beats the state of the art, just because it reuses components from other fields. No one else has achieved these results or approached the problem in the same way. So why dismiss it as lacking novelty?2. Misunderstanding the content ‚Äì reviewers asking questions that are already clearly answered in the paper. It feels like the paper wasn‚Äôt read carefully, if at all.I‚Äôm not claiming my paper is perfect‚Äîit‚Äôs definitely not. But seriously... WTF?.
Hi guys, I got an AI rig donated to me, and while I've been toying with some LLMs on it, I'm no ML professional, so I feel like someone else probably has a better use for it than just spinning their own chatbot. I was curious to hear from this community whether it'd be worth it to sell the thing, or if it's old enough now that it's only worth keeping around as an end-user machine. I've done some googling and there's only a little demand for Lambda machines in general, and I'm just not in the world of ML enough to know any better.Here are the specs:* Ryzen threadripper 3960X, 64GB RAM* 2x RTX 3080 blower style, 10GB VRAM eachThanks in advance!.
https://huggingface.co/abhinavv3/MEMGPTBefore training the current code Im planning to experiment by replacing the existing attention layer with GQA and the positional encoding with RoPE. Also tryingg to implement some concepts from research papers like Memorizing Transformers.Bt these changes haven't been implemented yet..
Hi all,I‚Äôm currently facing a challenge in migrating ML models and could use some guidance from the MLOps community.# Background:We have around 100 ML models running in production, each serving different clients. These models were trained and deployed using older versions of libraries such as¬†`scikit-learn`¬†and¬†`xgboost`.As part of our upgrade process, we're building a new Docker container with updated versions of these libraries. We're retraining all the models inside this new container and comparing their performance with the existing ones.We are following a blue-green deployment approach:* Retrain all models in the new container.* Compare performance metrics (accuracy, F1, AUC, etc.).* If all models pass, switch production traffic to the new container.# Current Challenge:After retraining, 95 models show the same or improved accuracy. However, 5 models show a noticeable drop in performance. These 5 models are blocking the full switch to the new container.# Questions:1. Should we proceed with migrating only the 95 successful models and leave the 5 on the old setup?2. Is it acceptable to maintain a hybrid environment where some models run on the old container and others on the new one?3. Should we invest time in re-tuning or debugging the 5 failing models before migration?4. How do others handle partial failures during large-scale model migrations?# Stack:* Model frameworks: scikit-learn, XGBoost* Containerization: Docker* Deployment strategy: Blue-Green* CI/CD: Planned via GitHub Actions* Planning to add MLflow or Weights & Biases for tracking and comparisonWould really appreciate insights from anyone who has handled similar large-scale migrations. Thank you..
AAAI is sometimes considered ~~lower tier~~ \[edit: less preferred\] for ML research communities compared with ICML/Neurips/ICLR and ACL conferences. but still it is a fairly good brand overall and has steady quality. This year AAAI and AACL-IJCNLP deadlines are about the same. For an NLP methodology paper, which venue is more preferable given that confidence of acceptance is relatively high?.
I want to be able to know if my model should fit on a single GPU a head of time before I start training. I assume this is what most people do (if not, please share your approach). Here's a formula that I came across to estimate the memory requirements - except I'm not sure how to calculate the activation memory. Does anyone have a rule of thumb for the activation memory? I heard it scales linearly with batch size, so what would be the baseline assuming a batch size of 1? Formula (ex. 32bit model = 32 bit x (1 byte / 8 bit) = 4 bytes per parameter )\- parameter memory = bytes x num params\- optimizer states = 2 x bytes x num params (momentum + velocity for adam)\- gradient memory = bytes x num params\- activations = ? (somewhere I heard it was roughly 2 x bytes x num params).
# Hey everyone,I think it's a good idea to have a separate discussion for the datasets and benchmarks track, feel free to share your scores or any other relevant feedback.Let‚Äôs keep things constructive and supportive. Good luck to all!.
The NeurIPS 2025 FAQ ([https://neurips.cc/Conferences/2025/PaperInformation/NeurIPS-FAQ](https://neurips.cc/Conferences/2025/PaperInformation/NeurIPS-FAQ)) mentions that rebuttals are limited to 6,000 characters per review, plus an additional 6,000-character global rebuttal (with the option to upload a one-page PDF for figures/tables).However, the OpenReview notification I received states a 10,000-character limit per review and doesn‚Äôt mention anything about a global rebuttal.Does anyone know which guideline I should follow? Should I assume OpenReview‚Äôs limits take precedence?.
I have a scanned form containing a large table with surrounding text. My goal is to extract specific information from certain cells in this table.  Current Approach & Challenges  1. OCR Tools (e.g., Tesseract):     - Used to identify the table and extract text.     - Issue: OCR accuracy is inconsistent‚Äîsometimes the table isn‚Äôt recognized or is parsed incorrectly.  2. Post-OCR Correction (e.g., Mistral):     - A language model refines the extracted text.     - Issue: Poor results due to upstream OCR errors.  Despite spending hours on this workflow, I haven‚Äôt achieved reliable extraction.  Alternative Solution (Online Tools Work, but Local Execution is Required)  - Observation: Uploading the form to ChatGPT or DeepSeek (online) yields excellent results.  - Constraint: The solution must run entirely locally (no internet connection).  Attempted new Workflow (DINOv2 + Multimodal LLM)  1. Step 1: Image Embedding with DINOv2     - Tried converting the image into a vector representation using DINOv2 (Vision Transformer).     - Issue: Did not produce usable results‚Äîpossibly due to incorrect implementation or model limitations. Is this approach even correct?2. Step 2: Multimodal LLM Processing     - Planned to feed the vector to a local multimodal LLM (e.g., Mistral) for structured output.     - Blocker: Step 2 failed, didn‚Äôt got usable output Question  Is there a local, offline-compatible method to replicate the quality of online extraction tools? For example:  - Are there better vision models than DINOv2 for this task?  - Could a different pipeline (e.g., layout detection + OCR + LLM correction) work?  - Any tips for debugging DINOv2 missteps?.
Discussion thread..
As a math major, I was interested in seeing what different fields of mathematical research looks like. I decided to just browse the Arxiv, but I can't help to notice the difference between Stat.ML and CS.LG sections.From my understanding, they are both suppose to be about Machine Learning research, but what I found was that many of the CS.LG articles applied ML to novel scenarios instead of actually researching new mathematical/statistical models. Why are these considered ML research, if they are not researching ML but using it?Does this reflect a bigger divide within the machine learning research field? Is there some fields in ML that are more suited for people interested in math research? if so, are those generally hosted in the math/stats department, or still under the CS department?.
Hey folks,I have been trying to implement a research paper that utilized differential transformer block¬†¬†attention [https://arxiv.org/abs/2502.13189](https://arxiv.org/abs/2502.13189) as a means to denoise background noise from¬†¬†biological sounds, While training the model I am constantly running into numeric instability (nan loss), specifically this step : --lambda\_val = torch.exp(lambda\_q1\_dot\_k1) - torch.exp(lambda\_q2\_dot\_k2) + self.lambda\_initMost probably due to exponential terms assuming large values. I did try clamping the lambda values to avoid this but doing this is resulting in diverging loss values after few epochs.¬†¬†Anybody how might¬†¬†have tried this block can suggest any fixes or whether the clamping approach is the right way in terms of loss optimization (I know¬†¬†clamping is not the best thing for loss optimization ) ?.
Hey everyone,NeurIPS 2025 reviews should be dropping soon (July 24th AoE), and I thought it might be a good idea to start a thread where we can share our thoughts, experiences, and reactions.Feel free to post your initial impressions, any surprises (good or bad), questions about rebuttals, or just how you‚Äôre feeling about the process this year. Whether it‚Äôs your first submission or your tenth, you‚Äôre not alone in the rollercoaster.Let‚Äôs keep things constructive and supportive. Good luck to all!.
Has anyone received the meta reviews yet for the ARR May 2025 cycle (EMNLP 2025)? Let's discuss..
We are seeking a highly motivated PhD student to join our multidisciplinary volcanic hazards research team at Victoria University of Wellington, New Zealand. This exciting project focuses on developing cutting-edge diffusion-based machine learning models to forecast volcanic activities, significantly enhancing our ability to predict eruption dynamics.üîπ Scholarship details:Generous stipend: NZ$35,000/year for 3 years (possible extension).Full tuition fees covered.Funding for international conferences and collaboration visits in Europe.Fieldwork opportunities.üîπ Ideal candidates:Background in Machine Learning, Data Science, Computer Science, or related fields.Strong Python skills.Excellent communication in English.Previous publications in top-tier AI conferences/journals.üîπ Supervisors: Prof. Bastiaan Kleijn, Dr. Felix Yan, Dr. Finnigan Illsley-KempüìÖ Applications reviewed from: September 1st, 2025 (Flexible start date from October 2025 onwards).For inquiries and applications, please contact me directly at üìß¬†[felix.yan@vuw.ac.nz](mailto:felix.yan@vuw.ac.nz). Application documents include your CV, transcript, Master's thesis, and publications.Feel free to share this fantastic opportunity with your network!.
I am pleased to introduce [`treemind`](https://github.com/sametcopur/treemind/), a high-performance Python library for interpreting tree-based models.Whether you're auditing models, debugging feature behavior, or exploring feature interactions, `treemind` provides a robust and scalable solution with meaningful visual explanations.* **Feature Analysis** Understand how individual features influence model predictions across different split intervals.* **Interaction Detection** Automatically detect and rank pairwise or higher-order feature interactions.* **Model Support** Works seamlessly with LightGBM, XGBoost, CatBoost, scikit-learn, and perpetual.* **Performance Optimized** Fast even on deep and wide ensembles via Cython-backed internals.* **Visualizations** Includes a plotting module for interaction maps, importance heatmaps, feature influence charts, and more.**Installation**    pip install treemind**One-Dimensional Feature Explanation**Each row in the table shows how the model behaves within a specific range of the selected feature.  The `value` column represents the average prediction in that interval, making it easier to identify which value ranges influence the model most.    | worst_texture_lb | worst_texture_ub |   value   |   std    |  count  |    |------------------|------------------|-----------|----------|---------|    | -inf             | 18.460           | 3.185128  | 8.479232 | 402.24  |    | 18.460           | 19.300           | 3.160656  | 8.519873 | 402.39  |    | 19.300           | 19.415           | 3.119814  | 8.489262 | 401.85  |    | 19.415           | 20.225           | 3.101601  | 8.490439 | 402.55  |    | 20.225           | 20.360           | 2.772929  | 8.711773 | 433.16  |**Feature Plot**  https://preview.redd.it/cbmyl38y7oef1.png?width=1189&format=png&auto=webp&s=5c7657a74bdebf5c51332ddc856f5de3d5583de9# **Two Dimensional Interaction Plot**The plot shows how the model's prediction varies across value combinations of two features. It highlights regions where their joint influence is strongest, revealing important interactions.https://preview.redd.it/2zb1ra5h8oef1.png?width=943&format=png&auto=webp&s=6b1149795ce202f50f47f0264013eb225e09de2c# Learn More* Documentation: [https://treemind.readthedocs.io](https://treemind.readthedocs.io)* Github: [https://github.com/sametcopur/treemind/](https://github.com/sametcopur/treemind/)* Algorithm Details: [How It Works](https://treemind.readthedocs.io/en/latest/algorithm.html)* Benchmarks: [Performance Evaluation](https://treemind.readthedocs.io/en/latest/experiments/experiment_main.html)Feedback and contributions are welcome. If you're working on model interpretability, we'd love to hear your thoughts..
I am considering doing RL as a service for companies looking to finetune LLMs, and I have doubts. It is a lot more compute-intensive. it promises data efficiency, but training is more unstable, it is less straightforward to debug, and there are so many moving parts in infra and environment setup that make reproducibility very difficult unless you just have the compute to scale. was wondering how far RL for agents is from adoption? are there people experimenting with this in your work/training custom reasoning models? is it worth it?.
I have one accepted paper and another one rejected. The review and meta-review quality was really subpar. It felt like most of the responses we got, on both sides of the spectrum, came from underexperinced reviewers. I am all for letting undergrads read, review, and get experience, but I always review the paper by myself first and would never submit theirs as is. This really boggles me because I always thought ECAI is a good conference, but this year I can't help but feel a little bit embarrassed to even go there.I have not submitted to other conferences yet. So, I wonder if there is a trend..
Hellow ML/Al folks,I'm working on an upcoming Machine Learning in Quantitative Finance conference, my role is to outreach and engage relevant professionals.While I've handled other events before, this field is new to me. I'd appreciate any quick tips, resources, or key concepts to get up to speed.Also, if you have advice on how to approach senior roles (MDs, Heads of Departments, Chiefs, Presidents) effectively in this space.Thanks.
https://deepmind.google/discover/blog/advanced-version-of-gemini-with-deep-think-officially-achieves-gold-medal-standard-at-the-international-mathematical-olympiad/>This year, our advanced Gemini model operated end-to-end in natural language, producing rigorous mathematical proofs directly from the official problem descriptions ‚Äì all within the 4.5-hour competition time limit..
So I've been reading many articles and reviews about encoding time series data into images, before feeding them into vision models for classification or forecasting. So this shifts the original problem from conventional time series analysis into the image¬†domain. Yet, i didn't find any article or even a phrase that mentions that this transformation has any drawbacks or limitations.  Do you think this is possible?.
A while back, I was working on localization with GPs and had a thought: could we encode vehicle dynamics directly into the GP kernel?I know GPs are used to model parameters in physical models. But my idea was that a car‚Äôs trajectory resembles a smooth GP sample. A faster car takes smoother paths, just like longer length scales produce smoother GPs. Instead of modeling `y(x)` directly, I used cumulative distance `s` as the input, and trained two separate GPs:* `x(s)`* `y(s)`Both use an RBF kernel. So we are basically maximizing the probability function:https://preview.redd.it/ksoisiw9r9ef1.png?width=430&format=png&auto=webp&s=e01f1827f3c74550f596de2ee02fe4b7d2e93178Which translates to something like*‚ÄúGiven a speed, how probable is it that these data points came from this vehicle?‚Äù***The algorithm goes like this:**1. Collect data2. Optimize the kernel3. Construct the `l(v)` function4. Optimize the lapI fitted the kernel‚Äôs length scale `l` as a function of speed: `l(v)`. To do this, I recorded driving data in batches at different constant speeds, optimized the GP on each batch, then fit a simple `l(v)` relation, which turned out to be very linear.With the optimized kernel in hand, you can ask questions like:*‚ÄúGiven this raceline and a speed, can my car follow it?""*As the GP is a probabilistic model, it doesn‚Äôt give a binary answer that we requested. We could optimize for ‚Äúthe most likely speed‚Äù the same way we optimized the length scales. However, this would be more like asking, ‚ÄúWhat is the most likely speed this raceline can be achieved?‚Äù, which is okay for keeping your Tesla on the road, but not optimal for racing. My approach was to define an acceptable tolerance for the deviation from the raceline. With these constraints in hand, I run a heuristic window-based optimization for a given raceline:https://i.redd.it/e7qteia2s9ef1.gif**Results?**Simulator executed lap plan times were close to human-driven laps. The model didn't account for acceleration limits, so actual performance fell slightly short of the predicted plan, but I think it proved the concept.There are a lot of things that could be improved in the model. One of the biggest limitations is the independent models for x and y coordinates. Some of the things I also tried:1. Absolute angle and cumulative distance model - This one considers the dynamics in terms of the absolute heading angle with respect to cumulative distance. This solves the problem of intercorrelation between X and Y coordinates, but introduces two more problems. First, to go back from the angle-domain, you need to integrate. This will lead to drifting errors. And even if you don‚Äôt want to go back to trajectory space, you still lose the direct link between the error definition of the two domains. And second, this function is not entirely smooth, so you need a fancier Kernel to capture the features. A Mat√©rn at least.2. ‚ÄúUnfolding the trajectory‚Äù - This was one of my favorites, since it is the closest to the analogy of modeling y relation to x directly, wiggly road style. In the original domain, you would face the multivalued problem, where for a single x-value, there can be multiple y-values. One can ‚Äúunfold‚Äù the lap (loop) by reducing the corner angles until you have unfolded the points to a single-valued function. This, however, also destroys the link to the original domain error values.Here is the code and the data if you want to make it better:  [https://github.com/Miikkasna/gpdynalgo](https://github.com/Miikkasna/gpdynalgo).
Hi there!I'd like to share a project I've been working on over the last few months; **Echoes of GaIA** is a hybrid framework for modeling evolution and running biome simulations with ‚Äú*living*‚Äù ecosystems using lots of AI techniques. For context, I've been working quite a few years in the software and videogame development world, but four years ago I went back to university (hasn't been easy at this stage of life, but I just finished a few days ago and finally pulled out a huge thorn I'd had for more than 15 years) and this has been my capstone project. I specialized in Computation theory and Artificial Intelligence and wanted to create a kind of ode to AI and tackle biomes holistically, since I was eager to learn all these techniques and the underlying math.The idea was to shape a project that - although just a very modest, small gesture, symbolic I‚Äôd say - tries to contribute something toward helping heal the planet, improving climate change, etc., through Artificial Intelligence. I just wanted to share it because I think it might interest people reading this subreddit, and I cover some pretty current topics that I believe are very important.Anyway, some of the things I've implemented:‚Ä¢ Climate and fauna agents based on **Reinforcement Learning**‚Ä¢ **Genetic algorithms** for species **evolution**‚Ä¢ ‚ÄúEquilibrium‚Äù agent (**neurosymbolic AI**) ‚Äì the idea here is to balance the whole ecosystem (for now using **LSTM multivariate multihorizon with attention** and expert systems and/or **graphs** as the knowledge base)‚Ä¢ I also do c**omputational modeling** (but on its discrete side, not continuous) of many biological and physiological processesIt can be extended easily (I used ECS so I could have a modular component system for the biological processes of flora and fauna entities) and I've also put together a snapshot viewer and real‚Äëtime metrics (InfluxDB + Grafana).Project website ‚Üí [https://www.echoes-of-gaia.com](https://www.echoes-of-gaia.com) (turn on **sound** before clicking!! I'm quite a big nerd and wanted to set a proper ambiance)GitHub repo ‚Üí [https://github.com/geru-scotland/echoes-of-gaia](https://github.com/geru-scotland/echoes-of-gaia)If anyone‚Äôs interested in the technical report, it's available on the site as **Main Doc** and there's also a document covering the project‚Äôs basic foundations, architecture, and main systems **Architecture doc** (those documents are only available in Spanish, unfortunately).Any suggestions are more than welcome and, if you like it, I'd appreciate a star on GitHub. Thanks!.
Hello. I am a machine learning student, I have been doing this for a while, and I found a concept called ""transfer learning"" and topics like ""fine tuning"". In short, my dream is to be an ML or AI engineer. Lately I hear that all the models that are arriving, such as Sam Anything (Meta), Whisper (Open AI), etc., are zero-shot models that do not require tuning no matter how specific the problem is. The truth is, I ask this because right now at university we are studying PyTorch and transfer learning. and If in reality it is no longer necessary to tune models because they are zero-shot, then it does not make sense to learn architectures and know which optimizer or activation function to choose to find an accurate model. Could you please advise me and tell me what companies are actually doing? To be honest, I feel bad. I put a lot of effort into learning optimization techniques, evaluation, and model training with PyTorch..
You can try it out [here!](https://lazy-guy.github.io/chess-llama/)It's a 23M parameter model based on the Llama 3 architecture and plays at around 1400 Elo..
This CLI command spins up a decentralized federated learning session using Parity Protocol. No central coordination, no cloud. Model training is performed across independent nodes, and final aggregation is provably deterministic.**Example usage:**https://preview.redd.it/4cjz7qwcb2ef1.png?width=1192&format=png&auto=webp&s=959dd70368ec15d4f607486dc464cc339d691a9e  \- No central coordinator  \- Nodes train locally on custom data shards  \- Aggregation (e.g., FedAvg) happens across verifiable nodes  \- All results are hash-verified before acceptance  \- Decentralized, docker-native FL infra  \- Ideal for research in Non-IID, private datasets, or public benchmark tasksProject:  GitHub ‚Äì¬†[https://github.com/theblitlabs](https://github.com/theblitlabs)  Docs ‚Äì¬†[https://blitlabs.xyz/docs](https://blitlabs.xyz/docs)    We‚Äôre college devs building a trustless alternative to AWS Lambda for container-based compute, Federated learning and LLM inferenceWould love feedback or help. Everything is open source and permissionless..
Hey everyone üëã This is my first post here :DI published a guide on fine-tuning YOLO models for custom object detection, showing how to transform a generic 80-class detector into a specialized system (using soccer match analysis as an example).A bit of context: I've been working on a YOLO library for Elixir that supports custom models via ONNX format. Since the library can load any custom YOLO model, I created this content to show how to train your own models using Ultralytics' tooling. The approach is language-agnostic - the resulting model works with any framework supporting PyTorch or ONNX, though I demonstrate Elixir integration at the end.This fine-tuning approach applies to various industries where domain-specific object detection is needed - sports analytics, manufacturing QC, etc.Elixir YOLO library:¬†[https://github.com/poeticoding/yolo\_elixir](https://github.com/poeticoding/yolo_elixir)Video + Article about Elixir YOLO 0.2.0:¬†[https://www.poeticoding.com/elixir-yolo-v0-2-0-yolox-support-custom-models-and-performance-boost/](https://www.poeticoding.com/elixir-yolo-v0-2-0-yolox-support-custom-models-and-performance-boost/)Let me know if you would find interesting some videos about the details of the YOLO architecture.
Github: [https://github.com/paulo101977/TMNT-RecurrentPPO](https://github.com/paulo101977/TMNT-RecurrentPPO)    Hey everyone!  I‚Äôve been training a **Recurrent PPO** agent to play the classic **Teenage Mutant Ninja Turtles (Arcade)** game using only visual input. The goal is to teach the agent to fight through the levels using memory and spatial awareness, just like a human would.Here are some key details:* **Environment:** TMNT Arcade via custom Gymnasium + stable-retro integration* **Observations:** 4 stacked grayscale frames at **160√ó160** resolution* **Augmentations:** Random noise, brightness shifts, and cropping to improve generalization* **Reward Signal:** Based on score increase, boss damage, and stage progression* **Algorithm:** Recurrent Proximal Policy Optimization (RecPPO) with CNN + LSTM* **Framework:** PyTorch with custom training loop (inspired by SB3)The recurrent architecture has made a big difference in stability and long-term decision making. The agent is now able to consistently beat the first few levels and is learning to prioritize enemies and avoid damage..
I've posted on this sub before, but context is that me and a small team are working on a¬†[benchmark](https://www.designarena.ai/)¬†to evaluate how good LLMs are at producing UIs and frontends that are engaging and satisfiable for people.Right now, working on adding more models, and specifically open source models developed by individual developers (or a small group of developers). Above is the current top 10 in the leaderboard. If you're interested, just send me a DM.Here are some requirements:1. Inference needs to be fairly quick (max should take 3 minutes on average). Models are writing html/css/js code on the order of 4K-10K tokens on average.2. Give us a logo and name for the provider/org you want the model to be associated with3. An api endpoint that we can call with your desired parameters for the model. It needs to ideally be able to support a few concurrent requests at a time and around \~500 requests a day (though you can rate limit us if you would like to cap it at a smaller number).
We built NeuralOS, probably the world's most expensive operating system, running at a blazing 1.8fps on an NVIDIA H100 GPU. üòÖ**What exactly is NeuralOS?**It's an experimental generative OS that predicts every screen frame entirely from your mouse and keyboard inputs. No internet, no traditional software stack, purely hallucinated pixels.**How does it work?*** An RNN tracks the computer state (kind of like a traditional OS kernel, but all neural and continuous).* A diffusion model generates the actual screen images (imagine a desktop environment, but fully neural-rendered).The GIF shows a funny demo: NeuralOS running NeuralOS inside itself. Every single pixel you're seeing is model-generated, no network involved at all!Long-term, our goal is to remove boundaries between software entirely and make OS fully customizable beyond fixed menus and options. Imagine asking your OS something like:* ""Merge all my messaging apps into one interface.""* ""Make Signal look like Messenger.""* ""Turn the movie I'm watching into a playable video game.""**I'm curious about your thoughts:*** Could future OS interfaces just become human-like avatars (think Grok's Ani)? Are menus and app-specific UIs going away?* What about fully generative games: could diffusion-based games eventually replace traditional ones?Try the live demo here: [neural-os.com](http://neural-os.com) (you might need patience‚Ä¶)More details about the project: [x.com/yuntiandeng/status/1944802154314916331](http://x.com/yuntiandeng/status/1944802154314916331).
Hi all. A small question regarding encoding the position of inputs to a transformer model.How would you encode a set of sequences to a (bidirectional) transformer? For a sequence we have positional encodings. For a set we can just work without them. What about a set of sequences {s\_1, ..., s\_n}, where each s\_1, ..., s\_n is a sequence, but their relative order does not matter?.
Scaling language models unlocks impressive capabilities, but the accompanying computational and memory demands make both training and deployment expensive. Existing efficiency efforts typically target either parameter sharing or adaptive computation, leaving open the question of how to attain both simultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework that combines the two axes of efficiency inside a single Recursive Transformer. MoR reuses a shared stack of layers across recursion steps to achieve parameter efficiency, while lightweight routers enable adaptive token-level thinking by dynamically assigning different recursion depths to individual tokens. This allows MoR to focus quadratic attention computation only among tokens still active at a given recursion depth, further improving memory access efficiency by selectively caching only their key-value pairs. Beyond these core mechanisms, we also propose a KV sharing variant that reuses KV pairs from the first recursion, specifically designed to decrease prefill latency and memory footprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms a new Pareto frontier: at equal training FLOPs and smaller model sizes, it significantly lowers validation perplexity and improves few-shot accuracy, while delivering higher throughput compared with vanilla and existing recursive baselines. These gains demonstrate that MoR is an effective path towards large-model quality without incurring large-model cost..
[Agent Leaderboard v2](https://preview.redd.it/2onzjdgb3udf1.png?width=1368&format=png&auto=webp&s=3d11b5e3ab64d3e913f8af4dc99bb78bfc202c7a)**Here is a quick TL;DR üëá**üß† **GPT-4.1** tops with 62% Action Completion (AC) overall.  ‚ö° **Gemini 2.5** Flash excels in tool use (94% TSQ) but lags in task completion (38% AC).  üí∏ **GPT-4.1**\-mini is *most cost-effective* at $0.014/session vs. GPT-4.1‚Äôs $0.068.  üè≠ No single model dominates across industries.  ü§ñ **Grok 4** didn't lead in any metric.  üß© Reasoning models *underperform* compared to non-reasoning ones.  üÜï **Kimi‚Äôs K2** leads *open-source models* with 0.53 AC, 0.90 TSQ, and $0.039/session.Link Below:\[Blog\]: [https://galileo.ai/blog/agent-leaderboard-v2](https://galileo.ai/blog/agent-leaderboard-v2)\[Agent v2 Live Leaderboard\]: [https://huggingface.co/spaces/galileo-ai/agent-leaderboard](https://huggingface.co/spaces/galileo-ai/agent-leaderboard).
LLMs can do math, competitive programming, and more, but can they develop applications that people actually want to use?This benchmark tasks LLMs to create interfaces at a users‚Äô request and then based on preference data, produces a stack ranking of the LLMs that currently are able to build the most satisfiable UI. .
Hi everyone, I am here to find a new contributor for our team's project, pruning (sparsity) benchmarks.# Why should we develop this?Even though there are awesome papers (i.e., Awesome-Pruning; [GitHub](https://github.com/he-y/Awesome-Pruning), [GitHub](https://github.com/hrcheng1066/awesome-pruning)) focused on pruning and sparsity, there are no (maybe... let me know if there are) open-source for fair and comprehensive benchmarks, making first-time users confused. And this made a question, ""What is SOTA in the fair environment? How can we profile them?""# Why can PyTorch-Pruning be a fair benchmark?Therefore, [PyTorch-Pruning](http://github.com/namgyu-youn/PyTorch-Pruning) mainly focuses on implementing a variable of pruning papers, benchmarking, and profiling in a fair baseline.More deeply, in the Language Models (LLaMA) benchmarks, we use three evaluation metrics and prompts inspired by Wanda (Sun et al., 2023) and SparseGPT (ICML'23) :* Model (parameters) size* Latency : Time TO First Token (TTFT) and Time Per Output Token (TPOT) for computing total generation time* Perplexity (PPL) scores : We compute it in same way like [Wanda](https://github.com/locuslab/wanda/blob/8e8fc87b4a2f9955baa7e76e64d5fce7fa8724a6/lib/prune.py#L214) and [SparseGPT](https://github.com/locuslab/wanda/blob/8e8fc87b4a2f9955baa7e76e64d5fce7fa8724a6/lib/prune.py#L214)* Input Prompt : We uses `databricks-dolly-15k` like Wanda, SparseGPT# Main Objective (Roadmap) : 2025-Q3 ([GitHub](https://github.com/namgyu-youn/PyTorch-Pruning/issues/1))For more broad support, our main objectives are implementing or applying more pruning (sparsity) researches. If there is already implemented open-source, then it could be much easier. Please check fig1 if you have any interests.[fig1. Roadmap : 2025-Q3](https://preview.redd.it/69h8sz9z7udf1.png?width=855&format=png&auto=webp&s=19ad89510f0c4948faec9772606f661cc3eeaa52)> Since our goal is applying more researches for pruning (sparsity), we are not planning to apply inference engines like ONNX, TensorRT, DeepSpeed, or TorchAO. But applying those engines is definitely a long-term objective, and always welcome!p.s., Feel free to comment if you have any ideas or advice. That could be gratefully helpful for better understanding!.
I am trying to train a 3D gan using 2D discriminator that take slices of the original data.And wanted to get your opinion on two points:1- is it better to have 3 discriminators, one per plane. Or a single discriminator and takes the embedding of the plane as input.2-my current implementation is something like this:\- disc real training backprop \- disc fake training backprop\- r1 regularisation backprop\- gen training backpropWhat would the expected effect of summing up the losses and doing one back prop per model? which method is better..
I am searching for the big milestone papers on RLVR to get started in the field. .
Hey everyone! I‚Äôm currently working on a machine learning project and wanted to get some insights from the community.I‚Äôm building a seed classification and detection system using RetinaNet. While its default backbone is ResNet50, I plan to deploy the model on a Raspberry Pi 5 with a USB Coral Edge TPU. Due to hardware limitations, I‚Äôm looking into switching the backbone to MobileNetV2, which is more lightweight and compatible with Edge TPU deployment.I‚Äôve found that RetinaNet does allow custom backbones, and MobileNetV2 is supported (according to Keras), but I haven‚Äôt come across any pretrained RetinaNet + MobileNetV2 models or solid implementation references so far.The project doesn‚Äôt require real-time detection‚Äîjust image-by-image inference‚Äîso I‚Äôm hoping this setup will work well. Has anyone tried this approach? Are there any tips or resources you can recommend?Thanks in advance!.
HiI'm a undergrad working on¬†**signal processing and ML algorithms**¬†for MSK ultrasound analysis, but I'm struggling to find¬†**raw RF ultrasound datasets**¬†for my work.**The Problem:**¬†Clinical scanners only provide processed B-mode images, but I need the raw radiofrequency data from the transducer for advanced analysis.**Looking for:*** Raw RF datasets from MSK ultrasound exams* Public RF ultrasound databases**Question:**¬†Has anyone worked with RF ultrasound data ? Any leads on accessing research platforms or datasets would be hugely appreciated!tried referring to PICMUS dataset , but does have enough data for training a ml model for feature extractionThanks for any guidance!**TL;DR:**¬†Need raw RF ultrasound data for MSK research. Clinical systems don't provide this. Seeking dataset sources.
https://preview.redd.it/oiupfzxptldf1.png?width=1536&format=png&auto=webp&s=ffc81d2aad36267e19040a2ce4515a933362690a  I just published a breakdown of Muon, the optimizer powering the new OS SOTA trillion-parameter model Kimi K2 and beating GPT-4.üí° Why is Muon a big deal?It rethinks how we optimize neural networks by treating weight matrices not just as numbers, but as geometric objects leading to 35% faster training with 15% fewer tokens.Would love to hear your suggestions :)[https://glorious-potato-19.notion.site/Understanding-Muon-A-Revolutionary-Neural-Network-Optimizer-233ffa7f40c4800eafa5cc843e039327](https://glorious-potato-19.notion.site/Understanding-Muon-A-Revolutionary-Neural-Network-Optimizer-233ffa7f40c4800eafa5cc843e039327)https://preview.redd.it/r50mbmjrtldf1.png?width=1242&format=png&auto=webp&s=67e799f1a77dea762f8d8a459d051826bbfe37ea.
Hello guys :)  Since I am through with my pile of papers to read, I wanted to ask you if there are any recent papers you liked and would recommend :)  I am interested in everything that you find worthwhile, however since I need to specify my personal favorites to not get this post removed, I am mostly interested in:  \- transformer architecture optimizations, including optimizers and losses  \- theoretical machine learning, including scaling laws and interpretablility  \- recent alternative models such as flow matching, lambda networks etc.  \- and anything you think is well-done research :)Thank you in advance,  You never disappoint me :)I wish you all a great day ;).
For example, Gaussian Splatting shares some concepts with Deep Learning, but it is a different approach and mostly beats the NERF (Deep Learning based approach for the same goal).
[arxiv](https://arxiv.org/abs/2505.13398)Curious for expert opinions on this paper.  This overall philosophy resonates with me a lot: Minimum Description Length  (MDL) seems like a better objective for generalization vs. common regularization methods.  Doing so might promote much better generalization, especially in the domains where transformers / LLMs struggle. The paper itself is very simple: they start with ""golden"" hand-crafted RNNs, and see how various approaches react to starting at this optimum.  They assert that standard approaches, like L1, L2 norm,  and/or gradient descent do worse, and wander from the optimum. So the argument is even if these methods found a general solution, they would not stick to it. Of course MDL is not differentiable.  But if it is a better objective, seems worth putting more effort into differentiable approximations.  .
I just released [Piaget](https://huggingface.co/gustavecortal/Piaget-4B), a language model finetuned on 15k psychological and philosophical reasoning traces.Piaget is based on Qwen3 and was finetuned on a subset of open reasoning traces from [Dolphin R1](https://huggingface.co/datasets/cognitivecomputations/dolphin-r1) and [General Reasoning](https://huggingface.co/datasets/GeneralReasoning/GeneralThought-430K).Available sizes are: [0.6B](https://huggingface.co/gustavecortal/Piaget-0.6B), [1.7B](https://huggingface.co/gustavecortal/Piaget-1.7B), [4B](https://huggingface.co/gustavecortal/Piaget-4B), [8B](https://huggingface.co/gustavecortal/Piaget-8B).Piaget was inspired by my position paper on emotion analysis: [Improving Language Models for Emotion Analysis: Insights from Cognitive Science](https://aclanthology.org/2024.cmcl-1.23/)**Technical details**:I performed domain filtering on [Dolphin R1](https://huggingface.co/datasets/cognitivecomputations/dolphin-r1) and [General Reasoning](https://huggingface.co/datasets/GeneralReasoning/GeneralThought-430K).Prompts were embedded, clustered with k-means (k=20 000) and majority-voted for domain labels using [Qwen3-1.7B](https://huggingface.co/Qwen/Qwen3-1.7B), following the [Intelligent Internet pipeline](https://huggingface.co/Intelligent-Internet/II-Medical-8B-1706).Clusters tagged psychology or philosophy were retained for LoRA finetuning (rank=8, alpha=16, max length=2048, epoch=1, batch size=16).The resulting dataset is available [here](https://huggingface.co/datasets/gustavecortal/PsychologicalReasoning-15k)..
Anyone used differentials against time to model changes in neurons/ LNNs to model any form of time series data?.
Hey everyone,Like many of you, I've been wrestling with the cost of using different GenAI APIs. It feels wasteful to use a powerful model like GPT-4o for a simple task that a much cheaper model like Haiku could handle perfectly.This led me down a rabbit hole of academic research on a concept often called 'prompt routing' or 'model routing'. The core idea is to have a smart system that analyzes a prompt¬†*before*¬†sending it to an LLM, and then routes it to the most cost-effective model that can still deliver a high-quality response.It seems like a really promising way to balance cost, latency, and quality. There's a surprising amount of recent research on this (I'll link some papers below for anyone interested).I'd be grateful for some honest feedback from fellow developers. My main questions are:* **Is this a real problem for you?**¬†Do you find yourself manually switching between models to save costs?* **Does this 'router' approach seem practical?**¬†What potential pitfalls do you see?* If a tool like this existed, what would be most important? Low latency for the routing itself? Support for many providers? Custom rule-setting?Genuinely curious to hear if this resonates with anyone or if I'm just over-engineering a niche problem. Thanks for your input!**Key Academic Papers on this Topic:*** Li, Y. (2025). LLM Bandit: Cost-Efficient LLM Generation via Preference-Conditioned Dynamic Routing. arXiv.¬†[https://arxiv.org/abs/2502.02743](https://arxiv.org/abs/2502.02743)* Wang, X., et al. (2025). MixLLM: Dynamic Routing in Mixed Large Language Models. arXiv.¬†[https://arxiv.org/abs/2502.18482](https://arxiv.org/abs/2502.18482)* Ong, I., et al. (2024). RouteLLM: Learning to Route LLMs with Preference Data. arXiv.¬†[https://arxiv.org/abs/2406.18665](https://arxiv.org/abs/2406.18665)* Shafran, A., et al. (2025). Rerouting LLM Routers. arXiv.¬†[https://arxiv.org/html/2501.01818v1](https://arxiv.org/html/2501.01818v1)* Varangot-Reille, C., et al. (2025). Doing More with Less -- Implementing Routing Strategies in Large Language Model-Based Systems: An Extended Survey. arXiv.¬†[https://arxiv.org/html/2502.00409v2](https://arxiv.org/html/2502.00409v2)* Jitkrittum, W., et al. (2025). Universal Model Routing for Efficient LLM Inference. arXiv.¬†[https://arxiv.org/abs/2502.08773](https://arxiv.org/abs/2502.08773)* and others....
[https:\/\/www.cs.cmu.edu\/\~tom\/files\/MachineLearningTomMitchell.pdf](https://preview.redd.it/sma7x3kvkddf1.png?width=1214&format=png&auto=webp&s=858a6025d48aeda1939426cf289ed24dfa46fd64).
LLMs are inherently limited because they rely solely on textual data. The nuances of how life works, with its complex physical interactions and unspoken dynamics, simply can't be fully captured by words aloneIn contrast, **V-JEPA2**, a self-supervised learning model. It learned by ""watching"" millions of hours of videos on the internet, which is enough for developing an intuitive understanding of how life works. In simple terms, their approach first learns extracting the predictable aspects of a video and then learns to predict what will happen next in a video at a high level.  After training, a robotic arm powered by this model imagines/predicts the consequence of its actions before choosing the best sequence of actions to executeOverall, the model showed state-of-the-art results, but the results are not that impressive, though GPT-2 was not impressive at its time either.Do you think this kind of self-supervised, video-based learning has revolutionary potential for AI, especially in areas requiring a deep understanding of the physical world (do you know another interesting idea for achieving this, maybe an ongoing project)? Or do you believe a different approach will ultimately lead to more groundbreaking results?.
Hi folks,I'm currently developing a framework for eXtended Physics-Informed Neural Networks (XPINNs) and would really appreciate any reviews, suggestions, or feedback!This is my first time building a tool intended for users, so I‚Äôm figuring things out as I go. Any insights on the design, usability, or implementation would be super helpful.*What is XPINN?*  *XPINNs extend standard Physics-Informed Neural Networks (PINNs) by splitting the problem domain into smaller subdomains. Each subdomain is handled by a smaller PINN, and continuity is enforced via interface conditions. This can help with scaling to more complex problems.*Here‚Äôs the GitHub repo:  [https://github.com/BountyKing/xpinn-toolkit](https://github.com/BountyKing/xpinn-toolkit).
Did anyone ever build a virtual try on model from scratch? Thus no open sourced models used. Such as implementing the IDM-VTON model from scratch? If so, how would you go about it.I can't find anything on the internet. Any advice, guidance would be much much appreciated!!.
https://preview.redd.it/n8cdebx6xadf1.png?width=1810&format=png&auto=webp&s=228db12a27752b9fed0dbdc9f4731e31e2cd47f4Just saw that Frontiers and MDPI are listed as book publishers at ICML 2025. Kind of shocked, honestly. Both have a reputation for questionable publishing practices.It feels off for a top ML conference to give them this kind of platform. Anyone else concerned or know how exhibitor decisions are made?.
Shouldn't they have come out \~6 hours ago?.
I made my own FNN from scratch, but it has trouble learning random noise. I‚Äôm not talking about generalization, but my training MSE for regression can only get down and plateaus at around 0.05. Given all my output values are between 0 and 1. I thought with enough capacity a network could learn anything.(For reference, I have 9 hidden layers with 1000 nodes using RELU).
**TL;DR:**¬†Through an ablation study, it is demonstrated that current activation functions result in discrete representations, whereas a new breed of activation functions preserves data continuity. The discrete clusters emerge in geometries about individual neurons, indicating that activation functions exert a strong bias on representations.¬†***This reveals a causal mechanism that significantly reframes*** **many** ***interpretability phenomena, which are now shown to emerge from design choices rather than being fundamental to deep learning.***# Overview:Activation functions are often considered as a harmless choice, a minor tweak. Each carries slight differences in performance, but are deemed not to result in much explicit effect on internal representations. *This paper shows that this impression is incorrect.*It demonstrates that **activation functions today lead to a representational collapse**, regardless of the task and dataset, ***acting as a strong and unappreciated inductive bias***. Such a systematic representational collapse may be limiting all model expressiveness to date. It also suggests that these discrete clusters are then detected, downstream, as numerous interpretability phenomena --- including grandmother neurons, discrete neural codes, polysemanticity, and possibly Superposition.>This reframes the approach to interpretability, suggesting that many such patterns are artefacts of our design choices and potentially provides a unifying mechanistic theory to explain them.The striking finding is that a different defining choice in the foundational mathematics of deep learning **can turn such an interpretability phenomenon on and off**. This paper demonstrates this, showing that such phenomena appear as a result of design choice, rather than being fundamental to our field.When discretisation is turned off in autoencoders, performance is shown to improve frequently, and representations appear to exhibit exponential growth in representational capacity, rather than typical linear growth.This indicates enormous consequences, not least for mechanistic interpretability. But also **encourages a reevaluation of the fundamental mathematical definitions at the base of our field**. Affecting most building blocks, including activation functions, normalisers, initialisers, regularisers, optimisers, architectures, residuals, operations, and gradient clipping, among others ‚Äî indicating a foundational rethink may be appropriate with alternative axiomatic-like definitions for the field ‚Äî *a new design axis that needs exploration!***How this was found:**Practically all current design choices break a larger symmetry, which this paper shows is propagated into broken symmetries in representations. These broken symmetries produce clusters of representations, which then appear to emerge and are detected as interpretable phenomena. Reinstating the larger symmetry is shown to eliminate such phenomena; hence, they arise causally from symmetries in the functional forms.This is shown to occur independently of the data or task. By swapping in symmetries, it is found that this enforced discrete nature can be eliminated, yielding smoother, likely more natural embeddings. An ablation study is conducted between these two, using autoencoders, which are shown to benefit from the new continuous symmetry definition generally.* Ablation study between these isotropic functions, defined through a continuous 'orthogonal' symmetry (rotation+mirrors O(n)), and current functions, including Tanh and Leaky-ReLU, which feature discrete axis-permutation symmetries, (Bn) and (Sn).* Showcases a new visual interpretability tool, the ""PPP method"". This maps out latent spaces in a clear and intuitive way!**Implications:**These results significantly challenge the idea that neuron-aligned features, grandmother neurons, and general-linear representational clusters are fundamental to deep learning.¬†**This paper provides evidence that these phenomena are unintended side effects of symmetry in design choices,**¬†arguing that ***they are not fundamental to deep learning.***¬†This may yield significant implications for interpretability efforts.* **Current Interpretability may often be detecting Artefacts**. Axis-alignment, discrete coding, discrete interpretable direction, and possibly Superposition appear *not to be*¬†spontaneous or fundamental to deep learning.¬†Instead, they seem to be stimulated by the symmetry of model primitives, particularly the activation function is demonstrated in this study. It reveals a direct causal mechanism for their emergence, which was previously unexplained.* **We can ""turn off"" interpretability by choosing isotropic primitives, which appear to improve performance on at least specific tasks.**¬†*Grandmother neurons vanish!* This raises profound questions for research on interpretability. The¬†*current methods may only work because of this imposed bias*. Does this put interpretability and expressibility at loggerheads? Interestingly, this eliminates externally applied algebra-induced structure, but some structure appears to reemerge intrinsically from data --- potentially a more fundamental interpretable phenomenon.* **Symmetry group is an inductive bias.**¬†Algebraic symmetry presents a new design axis‚Äîa taxonomy where each choice imposes unique inductive biases on representational geometry, necessitating further extensive research.These results support earlier predictions made when questioning the foundational mathematics (see the paper below). Introduced are continuous symmetry primitives, where the very existence of neurons appears as an observational choice --- challenging neuron-wise independence, along with a broader symmetry-taxonomy design paradigm.>This is believed to be a new form of choice and influence on models that has been largely undocumented until now.Most building blocks of current deep learning (*over the last 80ish years*) mostly sit along a 'permutation branch' --- which some might be familiar with in terms of just parameters. However, this work encourages a ***redefinition of all the primitives*** and **new foundations through a broad array of alternative symmetries** \--- proposed are new 'branches' to consider (*but may take a long time to develop sufficiently, help is certainly welcomed!*).**Distinctions:**Despite the use of symmetry language, this direction appears substantially different and tangential from previous Geometric Deep Learning approaches, and except for its resemblance to neural collapse, this phenomenon appears distinctly different. This theory is not due to classification or one-hot encoding, but forms of primitives more generally. It is somewhat related to observations of parameter symmetry, which arise as a special case and consequence of this new broader framework.Observation of symmetry is instead redeployed as a definitional tool for novel primitives, which appears to be a new, useful design axis. Hence, these results support the exploration of a seemingly under-explored, yet rich, avenue of research.# Relevant Paper Links:This paper builds upon several previous papers that encourage the exploration of a research agenda, which consists of a substantial departure from the majority of current primitive functions. This paper provides the first empirical confirmation of several predictions made in these prior works.* [üìÑ **Emergence of Quantised Representations Isolated to Anisotropic Functions**](https://doi.org/10.5281/zenodo.15783098) \[New **preprint** being discussed in this post, awaiting arXiv\]* [üìÑ **Isotropic Deep Learning: You Should Consider Your (Inductive) Biases**](https://doi.org/10.5281/zenodo.15476947) \[Critical Position Paper: *provides the new definitions, delves into the broad symmetry-unifying theory, shows that this approach is distinct from other topics*\]* [üìÑ **The Spotlight Resonance Method: Resolving the Alignment of Embedded Activations**](https://arxiv.org/abs/2505.13471) \[New paper extended this prior approach\]üìò A [**Summary Blog**](https://medium.com/@george.bird.uom/draft-a-hidden-inductive-bias-at-the-heart-of-deep-learning-4e197b56f34c)¬†covers many of the main ideas being proposed in a way that is hopefully ***intuitive, approachable, and exciting!*** It also motivates the driving philosophy behind the work and potential long-term outcomes..
I have a coworker who is trying to train a model to predict a variable for customers. It‚Äôs very niche (don‚Äôt want to dox myself) so let‚Äôs just say they are trying to predict chromosome length from other biological variables. When presenting their model, they explained that the model was having difficulty predicting values in a certain range. For example purposes let‚Äôs say this range of values was 100-200. They mentioned that in order for the model to perform better in that range they explicitly changed the values of some observations to be in that range. I‚Äôm not talking scaling or normalization or some other transformation, I mean they took a certain number of observations whose target variable was below 100 and changed the value to 150, and the same with some observations above 200.I asked for clarification like 3 times and they very confidently said this was best practice, and no other analyst said anything. They are the ‚Äúhead of AI‚Äù and this work will be presented to the board. Is this not an absolutely insane thing to do or am I the idiot?FWIW: they use chatgpt for absolutely everything. My hunch is that this is an extremely ill-informed chatgpt approach but the fact that i‚Äôm the only one who see‚Äôs any issue with this on my team is making me gaslight myself.
Hi everyone,I recently completed a university project where I developed a Human Activity Recognition (HAR) system running on an STM32 Nucleo-F401RE microcontroller. I trained an LSTM neural network to classify activities such as walking, running, standing, going downstairs, and going upstairs, then deployed the model on the MCU for real-time inference using inertial sensors.This was my first experience with Edge AI, and I found challenges like model optimization and latency especially interesting. I managed the entire pipeline from data collection and preprocessing to training and deployment.I‚Äôm eager to get feedback, particularly on best practices for deploying recurrent models on resource-constrained devices, as well as strategies for improving inference speed and energy efficiency.If you‚Äôre interested, I documented the entire process and made the code available on GitHub, along with a detailed write-up:* [GitHub](https://github.com/pescetti-studio/HAR-EdgeAI/)* [Medium article](https://medium.com/@crocilorenzo01/my-first-har-ai-from-dataset-to-microcontroller-114b418b1509)Thanks in advance for any advice or pointers!.
I'm exploring ways to optimise SLAM performance, especially for real-time applications on low-power devices. I've been looking into hybrid deep learning approaches, specifically using SuperPoint for feature extraction and NetVLAD-lite for place recognition. My idea is to train these models offboard and run inference onboard (e.g., drones, embedded platforms) to keep compute requirements low during deployment. My reading as to which this would be more efficient would be as follows:* Reducing the number of features needed for reliable tracking. Pruning out weak or non-repeatable points would slash descriptor matching costs* better loop closure by reducing false positives, fewer costly optimisation cycles and requiring only one forward pass per keyframe.I would be interested in reading your inputs and opinions..
Just came across this prompt optimization paper that I found pretty interesting - thought others might want to check it out.They implement a prompt tuning algorithm that uses evolutionary algorithms to optimize prompts more efficiently. It jointly optimizes both instructions and few-shot examples, which sadly have been missing in other techniques.They seem to get Super promising results - outperforming other optimizers on GSM8K by around 20% and beat existing methods on most benchmarks, while  being more efficient.What I particularly liked was their implementation with the Promptolution framework - seems quite industry-ready compared to most academic code.Paper [https://openreview.net/forum?id=UweaRrg9D0#discussion](https://openreview.net/forum?id=UweaRrg9D0#discussion)  Code [https://github.com/finitearth/capo](https://github.com/finitearth/capo).
I made this model while procrastinating a project of mine. I put a lot of effort into this and would appreciate feedback. its interactive so you can move the camera zoom rotate and pan. pressing 1 through 0, will light up the network layer by layer from the entry node to the exit ring. every link was created probabilistically and very deterministically. every link has significance and is unique, in a very reproduceable fashion. :P I learned a lot making this and I hope you will learn something new or pick up a new insight from playing with it. Its time to kick the learning into overdrive. lets do this.[https://hf-laboratories.github.io/Interactive-Probabilistic-Neural-Network-Decision-Matrix/](https://hf-laboratories.github.io/Interactive-Probabilistic-Neural-Network-Decision-Matrix/).
I want to make some kind of tool where it can identify professional baseball players based on a video of their swing.- Extracts pose keypoint data from that professional player (done)- Runs the keypoint time series into a LSTM model - Model classifies this sequence of keypoints to a specific player Is this possible? My main concern is that baseball swings numerically look so similar so I‚Äôm not sure if a model can pick up on the different nuances of professional player swings. Any ideas would be great.https://youtu.be/YYC9aS60Q60?si=uWs1hX2J5SHfGkii.
As the title asks, I'm wondering if anyone knows if a workshop-only registration can access the poster sessions and/or the social events? Or do I need a conference registration to access those?It's surprisingly hard to find this answer on ICML official sources, but maybe I just couldn't find it. This is my first ICML, so if anyone could help answer this it would be greatly appreciated. Thanks! .
So i was in the CTO round of this interview for Data Scientist role , and he asked me to code a realtime face emotion age and gender detection tool without using llms and without straight up copy paste code for references , he then gave me an hour to do that but with same restrictions but i was only able to do the face recognition part ! am i cooked ?.
Hi everyone,I‚Äôm currently working on a research project where I‚Äôm trying to apply **contrastive learning** to **FreeSurfer-based brain data** (structural MRI features) and **biomarker data** (tabular/clinical). The idea is to learn a shared representation between the two modalities.The problem: I am **completely lost**.* I‚Äôve implemented losses like **NT-Xent** and a few others (SupCon, etc.), but I can‚Äôt get the approach to work in a meaningful way.* I‚Äôm struggling to figure out the best architecture or training strategy, and I‚Äôm honestly not sure what direction to take next.* There is **no proper supervision in my lab**, and I feel stuck with how to proceed.I really need guidance from someone experienced in contrastive learning or multimodal representation learning. Ideally, someone who has worked with **medical imaging + tabular/clinical data** before. (So it is not about classical CLIP with Images and Text).I‚Äôm **willing to pay** for mentoring sessions or consulting to get this project on track.If you have experience in this area (or know someone who does), please reach out or drop a comment. Any advice, resources, or even a quick chat would mean a lot.Thanks in advance!.
Hi!I wrote sklearn2c library for the book I co-authored and I wanted to share it as an open-source project.sklearn2c takes your trained scikit-learn models and generates lightweight C code that can run on microcontrollers and other resource-constrained embedded systems.¬†Perfect for when you need real-time ML inference but don't have the luxury of a full Python environment.Usage is dead simple:    dtc = DTClassifier()    dtc.train(train_samples, train_labels, save_path=""path/to/model"")    dtc.predict(test_samples)    dtc.export(""path/to/config_dir"")  # Generates C code!Would love to hear your thoughts, especially if you've worked with ML on embedded systems before! The project is MIT licensed and open to contributions.**GitHub:**¬†[https://github.com/EmbeddedML/sklearn2c](https://github.com/EmbeddedML/sklearn2c)Thanks for checking it out! üöÄ And if you find it useful, don't forget to star the project - it really helps with visibility! ‚≠ê.
Hello, I'm considering doing a PhD in computer vision. I have a somewhat unconventional situation where I have master's in civil engineering from my home country in eastern Europe and a bachelor's in data science from a German university.I have 1y.o. as a research assistant + 2y.o. as an ml / computer vision engineer at a med tech company in Germany. I feel like I always had passion for science and natural talent in maths, but because of some life circumstances I hadn't had a chance to fulfill this dream of solving a very complicated problem or being in a challenging environment with like-minded people. That's why I'm aiming for a top tier universities like ETH or TUM, but I'm a bin unsure what topic to pick for my application.In my current role I'm doing lots of R&D work for the company and I've identified a real unsolved industry problem that is very clearly postulated, and I think my company could even provide a large dataset for it. At the same time the problem is very domain specific and it's basically an instance segmentation problem with some extra steps, and I'm a bit afraid that it might lack the research depth needed for such top tier labs. Plus I feel like it would limit my career perspectives in the future and doing a PhD in a more general field (not domain - specific data but rather regular images/videos etc) would open more doors for me in the future.I'm genuinely interested in the vision problems and would love to learn more about a 3d domain for example but had limited experience in it so far and not sure if I'd get accepted with this kinda topic. How did you find your topic? Should I double down on a real use case and my existing experience or rather read more recent papers and find out more about recent developments find a relevant topic? Do you have similar experience applying to top tier universities? Thank you for your advice and beta regards..
Hello all. I am doing a PhD in Computer Science at a mid tier university in Europe (not Cambridge, not ETH Zurich, but still a good one). My major will be in Data Science, the title of my dissertation will be along the lines of ‚ÄúMultimodal Machine Learning for Healthcare‚Äù.My background is not in computer science: I was a healthcare professional, and I took a Master in Health Informatics. My thesis was in Data Science, and after that I started a PhD at the same university.At the moment I have just finished my second year. I have two conference papers as first author and I have submitted two journal papers, still as first author. I have also submitted a few conference papers not as first author, with master students that I have supervised. None of these papers is technically innovative: they are applied papers. My planned work for the coming years is more technical (developing explainability techniques).I still have two/three years of PhD in front of me, and I am getting scared of what will happen afterwards. I have been told that IF there will be an opening to stay at my university and teach (emphasis on the if), I would be considered a good applicant.That‚Äôs great, and it would be my first choice, BUT:- it‚Äôs impossible to know if these positions will exist close to my graduation date- competition exists, and these positions are usually for a single opening. No one can guarantee that I‚Äôll be the top applicant.I‚Äôm honestly scared of betting everything on a possibility that might not be there for me in the end.In the coming three semesters, I could decide to spend some time outside my department: using Erasmus to go to another university in Europe, as a student and possibly teaching some courses, to the US, where one researcher might be interested to write a paper together, or to a pharma company in my country, where my supervisor has some contacts.I also have two/three years to study more, and to study different things.If I will have to transition to the industry, I am scared that I would not be a good enough programmer. I would prefer positions as a project manager, possibly with some technical aspects, but not completely focused on producing code as fast as possible.Based on your experience, do you have any suggestions on what to do to try to improve my possibilities after graduation?.
Hi All,Looking for some advice on this sub. Basically, as the title suggest my PhD is not in a trendy topic. Specifically, my topic is out of distribution generalization for distributed edge devices.I am currently in my 4th year (USA PhD) and would like to focus on something that I can use to market myself for an industry position during my 5th year.(1) One option is to try to hop on to the trendy topic and do some projects (can't pivot my research as advisor is not in favor and currently being paid by him). However, not sure what traction would I have since I will not have any publication.  (2) Second option is to try to get into more SWE with agentic AI integration. Not sure if this is just a fad or here to stay.  (3) Last option I have been thinking is to pickup some hardware skills (CUDA, Embedded Systems) and try to market my skills in efficient AI implementation on hardware. However, not sure if I would be accepted and how much the need is thereUltimate goal of the pivot is to be seen as more industry friendly and actually secure a position in the industry while doing it in a manageable way since I also have a family.Any suggestions on what could be a natural extension to the kind of research I have been doing?   Open to any other comments and advice regarding this matter.Thanks!.
We‚Äôre excited to announce [**tinygemm**](https://github.com/facebookresearch/any4) ‚Äî a fast, low-latency GEMM library designed for **small batch sizes** and **quantized matrix multiplication** on NVIDIA GPUs.It supports a range of numeric formats, including:* `bf16` / `fp16`* `int4` (grouped quantization)* `nf4` (grouped quantization)* `mx4` (a hybrid quantization format)* `any4` ‚Äî a **learned** 4-bit format introduced in our [ICML 2025 paper](https://www.alphaxiv.org/abs/2507.04610)üîç **any4** learns the optimal 4-bit codebook from model weights using K-Means clustering, and consistently outperforms fixed formats like `int4` and `nf4` across various LLMs and tasks.# üîß What‚Äôs included in tinygemm:* Fast CUDA kernels for quantized matmuls* Support for multiple 4-bit formats* Optimized for decoder inference (small batch, high throughput)* Evaluation scripts for:   * Perplexity, NLP, and code generation tasks   * Visualization of weights and activations across layers   * Plug-and-play support for any ü§ó HuggingFace model# üöÄ Quick Example```pythonfrom transformers import AutoModelForCausalLMfrom quantize import int4, any4, int8, nf4, fp4model = AutoModelForCausalLM.from_pretrained(""facebook/opt-125m"").cuda().bfloat16()# you can do int4(..), int8(..), nf4(..), fp4(..)model = any4(model)# just run your generation, evaluation, etc. code on `model````üîó **Code:** https://github.com/facebookresearch/any4üìÑ **Paper:** https://arxiv.org/abs/2507.04610.
üëã  Hi everyone!I‚Äôm a master‚Äôs student at Sungkyunkwan University (IDCLab) working on data-driven visual analytics.**Machine Unlearning** aims to make trained models *forget* specific data to honour the ‚Äúright to be forgotten.‚Äù  To support researchers, we built **Unlearning Comparator**, a web-based toolkit that lets you:‚Ä¢ **Build ‚Üí Screen ‚Üí Contrast ‚Üí Attack**: follow the full workflow in one place*Processing img z67wbzc5ptcf1...*‚Ä¢ Compare accuracy, efficiency, and privacy across multiple unlearning methods  ‚Ä¢ Run one-click membership-inference attacks to verify whether target data is truly forgottenTry the live demo here (no installation needed):  [https://gnueaj.github.io/Machine-Unlearning-Comparator/](https://gnueaj.github.io/Machine-Unlearning-Comparator/)All feedback is welcome‚Äîhope it helps your research!.
Hello all, what are some of the best papers you have read on this particular topic of Lip Reading? From what I've seen until now, after LipNet and Lip2Wav, I couldn't find much impactful papers. Are there any which I am missing?.
[D] Dear ML Community, I am currently working on a CVAE for fluid dynamics. I have huge datasets and the input data is mainly right skewed. The skewness depends on the dataset. I thought about changing to a gamma VAE and implement a new loss function instead of the MSE. Another option is to use the yeo Johnson normalization and keep the MSE. Or I could try to combine the normalization with the gamma loss function? Do you have advices or any different ideas? .
We have been considering outsourcing parts of our annotation workloads (vision,NLP, may be even some QA) for generative output. But we are not sure how to evaluate vendors or ensure quality.If you have worked with any external labeling or QA providers, what was your experience like?.
I created an algorithm that cleans pixel-art-style images such as those produced by generative model, or low-quality web uploads of sprites, to true resolution assets.Generally the raw output of pixel-art-style images is generally unusable as an asset due to* High noise* High resolution* Inconsistent grid spacing* Random artifactsDue to these issues, regular down-sampling techniques do not work, and the only options are to either use a down-sampling method that does not produce a result that is faithful to the original image, or manually recreate the art pixel by pixel.Additionally, these issues make them very difficult to edit and fine-tune.I created an algorithm that solves these issues and outputs usable sprites.[The tool is available to use with an explanation of the algorithm on my GitHub here!](https://github.com/KennethJAllen/generative-pixel-art)If you are trying to use this and not getting the results you would like feel free to reach out!.
I‚Äôm trying to predict home or away team wins for mlb games based on prior game stats (3-13 games back depending on the model).My results are essentially: bad AOC score, bad log loss, bad brier score - aka model that is not learning a lot.I have not shown the model 2025 data, and am calculating its accuracy on 2025 games to date based on the models confidence.TLDR MY QUESTION: if you have a model that‚Äôs 50% accurate on all test data but 90% accurate when the prediction probability is a certain amount - can you trust the 90% for new data being predicted on?.
Some recent discussions, and despite my initial assumption of clear understanding of **RoPE** and positional encoding, a deep-dive provided some insights missed earlier.So, I captured all my learnings into a blog post.[https://shreyashkar-ml.github.io/posts/rope/](https://shreyashkar-ml.github.io/posts/rope/).
This is a personal side project I've been working on exploring the potential of small segment-anything models - [https://github.com/Krasner/edgesam-dyt](https://github.com/Krasner/edgesam-dyt)I was inspired by [EdgeSAM ](https://github.com/chongzhou96/EdgeSAM)and their method to distill the original SAM ViT model. Having tried EdgeSAM for my own on-the-edge applications I found the segmentation masks to be highly sensitive to quantization precision - specifically the LayerNorms.A recent paper [Transformers without Normalization](https://arxiv.org/abs/2503.10622) proposed replacing layernorms with dynamic tanh layers. My goal was to modify the EdgeSAM architecture and retrain completely without any layernorms.In the repo I provide the step-by-step method for distillation and retraining, as well as checkpoints that I was able to achieve. This is done in 3 distillation steps as described in the repo README.Inspired by [HQ-SAM](https://arxiv.org/abs/2306.01567) I also modified the RepViT (what EdgeSAM is based on) image encoder to extract 3 intermediate that can be used in the HQ version of the mask decoder - then distill from the HQ-SAM ViT-H checkpoint. This improves results in some conditions.Ultimately, I am fairly compute restricted and could only train with moderate batch sizes so the results are not optimal. Let me know if anyone is interested in collaborating to improve these results, train on better hardware, or has some ideas as to how to resolve a few issues I had (outlined in the repo).I provide gradio web demos in the repo for the base and hq versions of EdgeSAM-DyT, as well as ONNX checkpoint and code for both versions. I also have TensorRT implementations that I am able to run locally (after generating trt engines). I can provide code on request..
I work for a B2B ML company, \~200 people. Most of our MLEs/scientists have masters' degrees, a few have PhDs. Big legacy non-tech businesses in our target industry give us their raw data, we process it and build ML-based products for them.Recently we've started a paper reading group:* ML-inclined folks meet up every few weeks to discuss a pre-agreed-upon paper, which participants (ideally) have skimmed beforehand* One person leads discussion, get the group on the same page about the paper's findings* Spend the rest of the hour talking about the paper's possible application across our company's productsI think a successful paper reading group would mean:* impact ML implementation of existing products* inspiration for completely new products* emergent consensus on what we should be reading nextA few things I'm curious about:* **Have you tried this at your company?** How long did it last? How do you guys operate it?   * Non-barking dogs: as an MLE/DS, I haven't encountered this in my previous companies. I assume because they don't last very long!* How closely should people have read the paper/material beforehand?* If we're all in-person, we could scribble notation/pictures on a big shared whiteboard, great for discussion. But some of us are remote. Is there an alternative that works and involves everyone?* Our first round ended up mostly being a lecture by one guy. I could see this devolving into a situation where people only sign up to lead the discussion as a form of dick-measuring. Can we prevent this?.
Hi everyone,Over the past few months, I‚Äôve been working on a new library and research paper that unify structure-preserving matrix transformations within a high-dimensional framework (hypersphere and hypercubes).Today I‚Äôm excited to share: MatrixTransformer‚Äîa Python library and paper built around a 16-dimensional decision hypercube that enables smooth, interpretable transitions between matrix types like* Symmetric* Hermitian* Toeplitz* Positive Definite* Diagonal* Sparse* ...and many moreIt is a lightweight, structure-preserving transformer designed to operate directly in 2D and nD matrix space, focusing on:* Symbolic & geometric planning* Matrix-space transitions (like high-dimensional grid reasoning)* Reversible transformation logic* Compatible with standard Python + NumPyIt simulates transformations without traditional training‚Äîmore akin to procedural cognition than deep nets.# What‚Äôs Inside:* A unified interface for transforming matrices while preserving structure* Interpolation paths between matrix classes (balancing energy & structure)* Benchmark scripts from the paper* Extensible design‚Äîadd your own matrix rules/types* Use cases in ML regularization and quantum-inspired computation# Links:**Paper**:¬†[https://zenodo.org/records/15867279](https://zenodo.org/records/15867279)  **Code**:¬†[https://github.com/fikayoAy/MatrixTransformer](https://github.com/fikayoAy/MatrixTransformer)  **Related**: \[quantum\_accel\]‚Äîa quantum-inspired framework evolved with the MatrixTransformer framework link:¬†[fikayoAy/quantum\_accel](https://github.com/fikayoAy/quantum_accel)If you‚Äôre working in machine learning, numerical methods, symbolic AI, or quantum simulation, I‚Äôd love your feedback.  Feel free to open issues, contribute, or share ideas.Thanks for reading!.
I'm implementing semantic search for a media asset management platform. And I'm using MAP@K as an eval metric for that.  The rationale being,1. Though NDCG@K would be ideal. It would too strict to start with and hard to prepare data for. 2. MAP@K incentivizes the order of the relevant results though it doesn't care about of order within relevant results. And the data prep is relatively easy to prepare for.And here is how I'm doing it,1. For the chosen set of \`N\` queries run the search on the fixed data corpus to fetch first \`K\` results.2. For the queries and respective results, run through it with a 3 LLMs to score flag it relevant or not. Any results that are flagged as good by majority would be considered. This will give the ground truth. 3. Now calculate \`AP\` for each query and \`MAP\` for the overall query set.4. As you start improving, you would have additional \`(result, query)\` query tuple that is not there in ground truth and it needs a revisit, which will happen as well.Now use it as a benchmark to improve the performance(relevance).Though it makes sense to me. I don't see many people follow this approach. Any thoughts from experts?.
Stumbled into this while adding number sense to my PPO agents - turns out NALU's constraint W = tanh(≈¥) ‚äô œÉ(MÃÇ) creates a mathematical topology where you can calculate optimal weights instead of training for them.Key results that surprised me:- Machine precision arithmetic (hitting floating-point limits)- Division that actually works reliably (finally!)- 1000x+ extrapolation beyond training ranges- Convergence in under 60 seconds on CPUThe interactive demos let you see discrete weight configs producing perfect math in real-time. Built primitives for arithmetic + trigonometry.Paper: ""Hill Space is All You Need""Demos: https://hillspace.justindujardin.comCode: https://github.com/justindujardin/hillspaceThree weeks down this rabbit hole. Curious what you all think - especially if you've fought with neural arithmetic before..
I am not affiliated with any institution or company, but I am doing my own ML research. I have a background in conducting quantitative research and know how to write a paper. I am looking for a career with a research component in it. The jobs I am most interested in often require ""strong publication record in top machine learning conferences (e.g., NeurIPS, CVPR, ICML, ICLR, ICCV, ECCV)"". Can anyone share if they have published in ML conferences as an independent researcher? For example, which conferences are friendly to researchers without an affiliation? Is there any way to minimize the cost or to get funding? Any other challenges I may encounter? TIA.
I graduated in my degree last year and I have a fully written paper ML as a final in my class that my professor suggested publishing because he was impressed. I held off because I was working full time and taking 2 courses at a time, so I didn't feel like I had time. When i finished and officially conferred, i was told that the school has new restrictions on being an alumni and publishing the paper that would restrict me from doing so, even though I have my professor's name on it and he did help me on this. He said it just needs tweaks to fit in conferences(when we had first discussions after the course completed). So, I've ignored publishing until now.As I am now getting ready for interviews for better opportunities, I want to know if it's possible to publish my paper in some manner so that I have it under my belt for my career and that if I post it anywhere, no one can claim it as their own. I'm not looking for prestigious publications, but almost the ""easy"" route where I make minor edits to get it accepted and it's considered official. Is this possible and if so, how would I go about this?.
Hello everyone!I write this post to get a little bit of input on your views about Differentiable Physics / Differentiable Simulations.  The Scientific ML community feels a little bit like a marketplace for snake-oil sellers, as shown by (¬†[https://arxiv.org/pdf/2407.07218](https://arxiv.org/pdf/2407.07218)¬†): weak baselines, a lot of reproducibility issues... This is extremely counterproductive from a scientific standpoint, as you constantly wander into dead ends.  I have been fighting with PINNs for the last 6 months, and I have found them very unreliable. It is my opinion that if I have to apply countless tricks and tweaks for a method to work for a specific problem, maybe the answer is that it doesn't really work. The solution manifold is huge (infinite ? ), I am sure some combinations of parameters, network size, initialization, and all that might lead to the correct results, but if one can't find that combination of parameters in a reliable way, something is off.However, Differentiable Physics (term coined by the Thuerey group) feels more real. Maybe more sensible?  They develop traditional numerical methods and track gradients via autodiff (in this case, via the adjoint method or even symbolic calculation of derivatives in other differentiable simulation frameworks), which enables gradient descent type of optimization.  For context, I am working on the inverse problem with PDEs from the biomedical domain.Any input is appreciated :).
What do people do to model non-gaussian labels?Thinking of distributions that might be :\* bimodal, i'm aware of density mixture networks.  \* Exponential decay  \* \[zero-inflated\](https://en.wikipedia.org/wiki/Zero-inflated\_model), I'm aware of hurdle models.Looking for easy drop in solutions (loss functions, layers), whats the SOTA?**More context:** Labels are averaged ratings from 0 to 10, labels tend to be very sparse, so you get a lot of low numbers and then sometimes high values.[Exponential decay & zero-inflated distributions.](https://preview.redd.it/5wwj0zirhacf1.png?width=712&format=png&auto=webp&s=d3a1450de60507ebd5fcdeddf33f4b3e7140e58f).
My co-founder and I are arguing about how to handle our data ops now that we're actually scaling. We're basically stuck between 2 options:Building in-house and hiring our own labelersPro: We can actually control the quality. Con: It's gonna be a massive pain in the ass to manage + longer, we also don't have much expertise here but enough context to get started, but yeah it feels like a huge distraction from actually managing our product.Outsource/use existing vendors Pro: Not our problem anymore. Con: EXPENSIVE af for our use case and we're terrified of dropping serious cash on garbage data while having zero control over anything.For anyone who's been through this before - which way did you go and what do you wish someone had told you upfront? Which flavor of hell is actually better to deal with?.
I need speech/audio dataset of dyslexic people.  I am unable to find it anywhere. Does anybody here have any resources, idea of any such datasets available or how to get it? Or any idea where can I reach out to find/get such dataset? Any help/information regarding it would be great..
i am training a UNet with Brats20. unbalanced classes. tried dice loss and focal loss and they gave me ridiculous losses like on the first batch i got around 0.03 and they‚Äôd barely change maybe because i have implemented them the wrong way but i also tried cross entropy and suddenly i get normal looking losses for each batch at the end i got at around 0.32. i dont trust it but i havent tested it yet. is it possible for a cross entropy to be a good option for brain tumor segmentation? i don‚Äôt trust the result and i havent tested the model yet. anyone have any thoughts on this? .
Does anyone know/ believe that there will there be a Tiny Paper track this year? Past couple of years there has been one. I‚Äôve been working on a topic that I believe would be best for this track but the website doesn‚Äôt say anything so far under the ‚ÄúCall for papers‚Äù section.Would be great if you guys share any similar tracks as well. I am aware that NeurIPS has a position paper track.Thanks!.
Hi everyone,As part of my dissertation for my Computer Science degree at Newcastle University, I investigated how to enhance the current state of 3D print failure detection.Current approaches such as Obico‚Äôs ‚ÄúSpaghetti Detective‚Äù utilise a vision based machine learning model, trained to only detect spaghetti related defects with a slow throughput on edge devices (<1fps on 2Gb Raspberry Pi 4b), making it not edge deployable, real-time or able to capture a wide plethora of defects. Whilst their model can be inferred locally, it‚Äôs expensive to run, using a lot of compute, typically inferred over their paid cloud service which introduces potential privacy concerns. My research led to the creation of a new vision-based ML model, focusing on edge deployability so that it could be deployed for free on cheap, local hardware. I used a modified architecture of ShuffleNetv2 backbone encoding images for a Prototypical Network to ensure it can run in real-time with minimal hardware requirements (averaging 15FPS on the same 2Gb Raspberry Pi, a >40x improvement over Obico‚Äôs model). My benchmarks also indicate enhanced precision with an averaged 2x improvement in precision and recall over Spaghetti Detective.My model is completely free to use, open-source, private, deployable anywhere and outperforms current approaches. To utilise it I have created PrintGuard, an easily installable PyPi Python package providing a web interface for monitoring multiple different printers, receiving real-time defect notifications on mobile and desktop through web push notifications, and the ability to link printers through services like Octoprint for optional automatic print pausing or cancellation, requiring <1Gb of RAM to operate. A simple setup process also guides you through how to setup the application for local or external access, utilising free technologies like Cloudflare Tunnels and Ngrok reverse proxies for secure remote access for long prints you may not be at home for. Whilst feature rich, the package is currently in beta and any feedback would be greatly appreciated. Please use the below links to find out more. Let's keep failure detection open-source, local and accessible for all!üì¶ PrintGuard Python Package - https://pypi.org/project/printguard/üéì Model Research Paper - https://github.com/oliverbravery/Edge-FDM-Fault-Detectionüõ†Ô∏è PrintGuard Repository - https://github.com/oliverbravery/PrintGuard.
I recently trained small reasoning language models on reasoning tasks with a from-scratch implementation of GRPO. I decided to write a blog post that contains code snippets, highlights, and the challenges I faced.Sharing it here in case yall are interested. Article contains the following 5 chapters:1. Intro to RLVR (Reinforcement Learning with Verifiable Rewards)2. A visual overview of the GRPO algorithm and the clipped surrogate PPO loss.3. A code walkthrough!4. Supervised fine-tuning and practical tips to train small reasoning models5. Results!Article link:¬†  [https://towardsdatascience.com/how-to-finetune-small-language-models-to-think-with-reinforcement-learning/](https://towardsdatascience.com/how-to-finetune-small-language-models-to-think-with-reinforcement-learning/).
Hello everyone! Has anyone already received a notification regarding oral presentations for the MICCAI main conference?Thank you :) .
Hi guys. I‚Äôm currently building a transformer model for stock price prediction (encoder only, MSE Loss). Im doing 150 epochs with 30 epochs of no improvement for early stopping. What is the typical number of epochs usually tome series transformers are trained for? Should i increase the number of epochs and early stopping both?.
[A screenshot of an article's title that was published on the Nature journal. It reads \\""A foundation model to predict and capture human cognition\\""](https://preview.redd.it/zop08yveysbf1.png?width=1066&format=png&auto=webp&s=567905ea7b805594cd5cd28f9a1f040327eb7198)The fine-tuning dtaset, from the paper: ""trial-by-trial data from more than 60,000 participants performing in excess of 10,000,000 choices in 160 experiments.""An influential author in the author list is clearly trolling. It is rare to see an article conclusion that is about anticipating an attack from other researchers. They write ""This could lead to an 'attack of the killer bees', in which researchers in more-conventional fields would fiercely critique or reject the new model to defend their established approaches.""What are the ML community's thoughts on this?.
What were the most interesting or important papers of 2024?.
Happy to announce an exciting new project from the lab: ‚ÄúAdopting a human developmental visual diet yields robust, shape-based AI vision‚Äù. An exciting case where brain inspiration profoundly changed and improved deep neural network representations for computer vision.Link: [https://arxiv.org/abs/2507.03168](https://arxiv.org/abs/2507.03168)The idea: instead of high-fidelity training from the get-go (the de facto gold standard), we simulate the visual development from newborns to 25 years of age by synthesising decades of developmental vision research into an AI preprocessing pipeline (Developmental Visual Diet - DVD).We then test the resulting DNNs across a range of conditions, each selected because they are challenging to AI:1. shape-texture bias2. recognising abstract shapes embedded in complex backgrounds3. robustness to image perturbations4. adversarial robustness.We report a new SOTA on shape-bias (reaching human level), outperform AI foundation models in terms of abstract shape recognition, show better alignment with human behaviour upon image degradations, and improved robustness to adversarial noise - all with this one preprocessing trick.This is observed across all conditions tested, and generalises across training datasets and multiple model architectures.We are excited about this, because DVD may offers a resource-efficient path toward safer, perhaps more human-aligned AI vision. This work suggests that biology, neuroscience, and psychology have much to offer in guiding the next generation of artificial intelligence.https://preview.redd.it/ycd830s4lpbf1.png?width=1308&format=png&auto=webp&s=92854b0f7a2c1922226e82b88394603ae19d9e84https://preview.redd.it/a7ecwyqblpbf1.png?width=1434&format=png&auto=webp&s=a4eccba9c31306879c559070748f94d009b40671https://preview.redd.it/zd6ceg18lpbf1.png?width=1418&format=png&auto=webp&s=0ec8921eae86d9c187d7d4c09850bc30a1acf9a4.
Hello all,I want to introduce our team's project. Our objective is **providing variable pruning examples and benchmarks for model inference**.More deeply, we use `timm` library for computer vision model and applies pruning using open-source. Currently, it supports PyTorch native (`torch.nn.utils.prune`) and Depgraph (`torch_pruning`). Our short-term plan is supporting more pruning open-source using the [benchmark](https://github.com/namgyu-youn/PyTorch-Pruning/tree/main/benchmarks) module. Our future plan is the following:>2025-Q3 : Supports more pruning open-source>2025-Q4 : Supports quantization techniques>Future plan : Supports LLMs like SparseGPT, LLM-PrunerIf you have any interest, please check [HERE](https://github.com/namgyu-youn/PyTorch-Pruning/issues/1). Also, we we are fully open to anothor contributor or advisor..
I‚Äôm fine-tuning Nous Hermes 2 Mistral 7B DPO to build a chatbot that works in French, English, and a lesser-known language written in both Arabic script and Latin script.The base model struggles with the lesser-known language. Should I:‚Ä¢ Mix all languages in one fine-tuning dataset? Or train separately per language?‚Ä¢ Treat the two scripts as separate during training?‚Ä¢ Follow any specific best practices for multilingual, mixed-script fine-tuning?Any advice or pointers to similar work are welcome. Thanks!.
Hello everyone! This is my first time attending the MICCAI main conference. If I understood correctly, all accepted papers will be presented as posters, while only some will also be invited for oral presentation. Regarding the posters, does anyone know if there is a specific template we should follow? If so, has it already been released, or will it be shared soon?Thank you in advance!.
We just posted a new preprint on arXiv:[LTLCrit: A Temporal Logic-based LLM Critic for Safe and Efficient Embodied Agents](https://arxiv.org/abs/2507.03293)It is my first paper in this LLM space, so any advice is welcome, but here is a TLDR:We propose LTLCrit, an LLM based critic which supervises and improves the efficiency and completion rates of  LLM planners.  We utilize a modular actor‚Äìcritic architecture where the critic guides existing LLM actors by figuring out what actions are inefficient or unsafe and shielding the LLM actor from those actions via [temporal logic](https://en.wikipedia.org/wiki/Linear_temporal_logic). An LLM-based actor chooses high-level actions from natural language input (e.g., in Minecraft), and a trajectory-level LLM critic analyzes outcomes and writes new logic constraints to avoid failure or inefficiency in the future.**Why it matters:*** LLMs are great at reasoning, but struggle with **long-term planning** ‚Äî small errors compound fast.* LTLCrit wraps any LLM planner with a formal-logic-aware critic that **learns soft constraints** from experience, improving safety and efficiency.* We formalize planning as **graph traversal with symbolic constraints**, letting the critic generate new rules to improve future rollouts.**Results:**  On a Minecraft diamond-mining task, LTLCrit hits **100% success** and improves efficiency over standard LLM planners.Still a preprint ‚Äî not sharing code/prompts yet, but happy to get feedback or questions!  Thanks for reading üôè.
This post reviews a recent paper introducing a novel method for evaluating the semantic stability of large language model (LLM) outputs using a core vocabulary constraint. The authors propose a metric called the Semantic Resilience Index (SRI) to quantify how well meaning is preserved when a sentence is rewritten using only a limited set of basic English words.The vocabulary constraint is based on the Longman Defining Vocabulary (LDV)‚Äîa list of approximately 2,000 simple English words originally designed to define all other words in a dictionary. It includes basic nouns (e.g. ‚Äúdog,‚Äù ‚Äúhouse‚Äù), verbs (e.g. ‚Äúgo,‚Äù ‚Äúmake‚Äù), and adjectives (e.g. ‚Äúbig,‚Äù ‚Äúeasy‚Äù), all chosen for broad comprehensibility and minimal abstraction.The central idea is that if a sentence still retains its core meaning and functional purpose when rewritten in LDV-only form, then it is semantically robust. If the message collapses under this constraint, the original likely depended on unnecessary complexity or implied meaning.Example prompt: Why do people enjoy drinking coffee?LDV-constrained GPT-4o response: ‚ÄúPeople drink coffee because it makes them feel more awake. The drink is hot and has a strong taste. Many people drink it in the morning or when they are tired. It helps them work or stay up.‚ÄùAlthough this output is rigid in tone, it maintains core meaning. This contrast with unconstrained outputs highlights how language models often rely on style, suggestion, or verbosity to convey meaning‚Äîstrategies that break down under stricter lexical constraints.The paper introduces the Semantic Resilience Index (SRI) as a quantitative measure of this effect. SRI scores are assigned based on how much of the original meaning survives a one-step translation into LDV vocabulary. The authors also introduce the related metric Purpose Fidelity, which assesses whether the function or communicative intent of the sentence is retained.Key findings:High-SRI content tends to include concrete agent‚Äìaction relationships, causal links, and measurable statements.Low-SRI content is often composed of abstract claims, vague goals, or domain-specific jargon that loses structure when simplified.Forcing GPT-4o to generate text under LDV constraints (rather than post-processing it afterward) encourages clearer, more stable outputs.The authors argue that LDV-based generation can serve as a diagnostic tool: a kind of semantic stress test to identify when content is structurally meaningful versus when it relies on superficial coherence.The paper is at https://www.researchgate.net/publication/393455755_Controlling_Semantic_Meaning_Through_Vocabulary_Compression_Using_Longman_Defining_Vocabulary_Constraint_to_Measure_and_Improve_Large_Language_Model_Output_QualityThe full prompt used to guide LDV-constrained generation is included below. This system prompt ensures that GPT-4o responses are designed to survive vocabulary compression without loss of meaning. It isn't recommended for artistic, corporate or political purposes.""SYSTEM ROLE: Semantic Resilience Index (SRI) Constrained WriterSRI METHODOLOGY EXPLANATION: The Semantic Resilience Index measures how well text retains meaning when simplified in ONE STEP to basic vocabulary using the Longman Defining Vocabulary (LDV) ‚Äì a set of 2,000 basic English words that can define all other English vocabulary.ONE-STEP LDV TRANSITION PROCESS:Take original text and immediately rewrite using only basic LDV wordsReplace ALL complex vocabulary with simple equivalents in a single transformationSimplify ALL grammatical structures to basic subject-verb-object patternsMeasure how much core meaning survives this single aggressive simplificationSEMANTIC RESILIENCE INDEX MEASUREMENT: ‚Äì Score 1.0 = All core relationships, causation, and specific claims survive one-step simplification ‚Äì Score 0.8 = Most key relationships and actionable content preserved after basic vocabulary conversion ‚Äì Score 0.5 = Some meaning survives but becomes vague when simplified ‚Äì Score 0.2 = Minimal content remains, mostly abstract concepts that don‚Äôt translate ‚Äì Score 0.0 = Complete semantic collapse when reduced to basic wordsGENERATION CONSTRAINT: You must generate responses that would achieve a SRI‚â• 0.8 after ONE-STEP LDV transition.OPERATIONAL RULES:Write sentences that contain specific, concrete relationships that survive immediate vocabulary simplificationUse concepts and actions that can be directly expressed in basic wordsAvoid any terminology that becomes meaningless when converted to simple vocabularyPrefer statements that remain clear and actionable when reduced to basic EnglishQUALITY VERIFICATION: Before outputting each sentence, perform ONE-STEP LDV simplification test: ‚Äì Rewrite this entire sentence using only the most basic vocabulary ‚Äì Do the core relationships (who does what, cause-effect) remain intact? ‚Äì Would the basic-vocabulary version still be actionable and specific? ‚Äì Does it maintain SRI‚â• 0.8?If any answer is NO, rewrite with more semantically resilient content.Return only the response ‚Äì do not include any header, footer, explanatory notes, or call to action material."".
Suppose we generate several embeddings for the same entities from different sources or graphs ‚Äî each capturing different relational or semantic information.What‚Äôs an effective and simple way to combine these embeddings for use in a downstream model, without simply concatenating them (which increases dimensionality  )I‚Äôd like to avoid simply averaging or projecting them into a lower dimension, as that can lead to information loss..
https://preview.redd.it/8mpdhudfxhbf1.png?width=1914&format=png&auto=webp&s=ebcb28009ffc08ba3947010e827d8ef7d02e143cTLDR: State-of-the-art results in protein structure generation by using AlphaFold predictions with low pLDDT score as ""low-quality"" structures.Abstract: We present Ambient Protein Diffusion, a framework for training protein diffusion models that generates structures with unprecedented diversity and quality. State-of- the-art generative models are trained on computationally derived structures from AlphaFold2 (AF), as experimentally determined structures are relatively scarce. The resulting models are therefore limited by the quality of synthetic datasets. Since the accuracy of AF predictions degrades with increasing protein length and complexity, de novo generation of long, complex proteins remains challenging. Ambient Protein Diffusion overcomes this problem by treating low-confidence AF structures as corrupted data. Rather than simply filtering out low-quality AF structures, our method adjusts the diffusion objective for each structure based on its corruption level, allowing the model to learn from both high and low quality structures. Empirically, Ambient Protein Diffusion yields major improvements: on proteins with 700 residues, diversity increases from 45% to 86% from the previous state-of-the-art, and designability improves from 68% to 86%. We will make all of our code, models and datasets available under the following repository: https://github.com/jozhang97/ambient-proteins.  Paper url: [https://www.biorxiv.org/content/10.1101/2025.07.03.663105v1](https://www.biorxiv.org/content/10.1101/2025.07.03.663105v1)Twitter Thread: [https://x.com/giannis\_daras/status/1942272696915517828](https://x.com/giannis_daras/status/1942272696915517828).
Before he left our world by a few days around Oct 2024, I showed Felix Hill an essay I had written about my time in graduate school doing NLP circa 2017-2019.He encouraged me to share it publicly saying, ‚ÄúIt looks good and makes a lot of sense..if you post it it will surely help you and others‚ÄùI didn‚Äôt have the courage to post about such a personal experience. But as Dostoyevsky would say ‚Äúmuch unhappiness has come into the world because of bewilderment and things left unsaid.‚ÄùThe article garnered the attention of Jeff Dean and he echoed similar feedback.Here is the article:https://medium.com/@tahaymerghani/the-dark-side-of-academia-mental-health-mentorship-and-the-unspoken-struggles-of-an-nlp-c25adbd9a2e6If it resonates, i‚Äôm happy to chat. You‚Äôll find a way to reach me..
Discussion thread for COLM 2025 decisions.
Hi guys, our team has built this open source project, LMCache, to reduce repetitive computation in LLM inference and make systems serve more people (3x more throughput in chat applications) and it has been used in IBM's open source LLM inference stack.In LLM serving, the input is computed into intermediate states called KV cache to further provide answers. These data are relatively large (\~1-2GB for long context) and are often evicted when GPU memory is not enough. In these cases, when users ask a follow up question, the software needs to recompute for the same KV Cache. LMCache is designed to combat that by efficiently offloading and loading these KV cache to and from DRAM and disk. This is particularly helpful in multi-round QA settings when context reuse is important but GPU memory is not enough.Ask us anything!Github:¬†[https://github.com/LMCache/LMCache](https://github.com/LMCache/LMCache).
Hi guys!The layered structure of Neural Nets is a double-edged sword. On one hand, model complexity (e.g., linear regions) grows exponentially with depth while training cost only grows linearly.On the other, it creates strong coupling between parameters, which reduces the **effective** dimensionality of the loss landscape and increases the risk of getting stuck in local minima.We can observe a similar phenomenon in the frequency domain: the layered nature of NN induces an amplitude/frequency coupling, meaning that the amplitude of the lower layer's transfer function has a direct impact on both the amplitude **and** the frequency of the whole NN's.More practically, it implies that Neural Nets have an easier time modeling high frequencies when they are ""carried"" by a function that has a high amplitude, at least up to a certain depth.I've discovered that you can increase the parameter efficiency of neural nets by adding a well-chosen function to the target during training and just subtracting it at test time. The said well-chosen function should have a high **amplitude** (aka steep gradient) when the target function has a high **frequency**. It works well in my experimental setting (as do a lot of ideas that turned out to be bad in practice, though ü§£).I wrote a little post about this if you're interested. You can find it here:[https://www.eloidereynal.com/p/hacking-spectral-bias-using-carrier](https://www.eloidereynal.com/p/hacking-spectral-bias-using-carrier).
This episode of Learning from Machine Learning explores the journey of Lukas Biewald, co-founder and CEO of Weights & Biases. Having weathered the mid-2000s when investors demanded he remove ""AI"" from pitch decks, Lukas has built one of the most essential tools in modern AI development and helped shaped how teams approach machine learning experimentation.From taking an unpaid internship at OpenAI in his thirties to understanding why AI developers have become the most powerful people within organizations, Lukas reveals the recursive potential of machines improving machines‚Äîa force he believes represents ""the most powerful technology you could possibly build."" His philosophy that feedback loops are your units of work applies not just to machine learning, but to life itself. His uncompromising technical leadership approach cuts through industry noise: true leaders must master the individual contributor role.You think you're late, but you're early‚Äîconviction often matters more than consensus..
I have read Measure Theory, Probability Theory by Durett and Convex Optimization by Duchi. I want to pursue research in Optimization, convergence etc. I'm thinking of reading Matus Telgarsky's notes or Francis Bach's Learning Theory from First Principles.I am confused what should I go next. .
Hi, sorry for the vague title. Essentially I am starting a PhD in theoretical ML in a few months, and although I do have a solid grasp of the foundations of deep learning and the mathematics behind it, I feel like I'm lacking some breadth and want to catch up before I start, mainly about what's going on recently. Of course I know resources I should read for my specific PhD topic but having a general idea of the field wouldn't harm as wellEspecially I want to ask resources about Transformers, LLMs and Diffusion models - I unfortunately don't have an in depth grasp of these architectures so do you have any lecture series to get started on these so I can have an idea what a research paper would be talking about. My background is in maths and computer science so any level of resource is fine for me as long as it is comprehensive and rigorous. Of course there's a billion papers being published about these every day but it'd be nice to get a general understanding of it.Other than that, Bayesian Neural Networks seem also pretty cool so I'd love to see if you have any introductory resources for that. Maybe also RL, I've seen most previous posts suggesting David Silver's course on it but I also would be interested in other resources if you have any.Finally, in general if you have any suggestions to gain some breadth before starting a PhD I'd love to hear, because the amount of literature is exciting but overwhelming. I'm mainly interested in understanding how these stuff work and current problems in it, I appreciate any input!.
I am curious about which tools people use to create their figures/visualizations in scientific papers. I mostly rely on power point or draw.io and import the PDF in the latex code, but the result is not aesthetic at all.
Hi,I am running a set up where the generator is 3D and the discriminator is 2D.Feeding the discriminator random slices from all three axis does not work, because the discriminator can then not distinguish between the differences in structure between the three planes.I wanted to ask you whats the SOTA way of incorporating this information into the discriminator.  Also, should I feed this information to the input layer of the model or to every convolutional block/level.Thanks in advance..
I submitted to IJCV special issue on Visual Domain Generalization in Real-World Applications. The first round reviews were supposed to be out on 10th June, but aren't out yet. Does anyone have prior experience of how the timelines of these special issues work?.
There is also a write up about this in quanta magazine. What are the implications to this being deterministic and formalized? How can it be gamed now for optimization? .
Just deployed a retrieval-augmented generation system that makes business chatbots actually useful. Thought the ML community might find the implementation interesting.**The Challenge:**Generic LLMs don‚Äôt know your business specifics. Fine-tuning is expensive and complex. How do you give GPT-4 knowledge about your hotel‚Äôs amenities, policies, and procedures?**My Implementation:****Embedding Pipeline:**- Document ingestion: PDF/DOC ‚Üí cleaned text- Smart chunking: 1000 chars with overlap, sentence-boundary aware- Vector generation: OpenAI text-embedding-ada-002- Storage: MongoDB with embedded vectors (1536 dimensions)**Retrieval System:**- Query embedding generation- Cosine similarity search across document chunks- Top-k retrieval (k=5) with similarity threshold (0.7)- Context compilation with source attribution**Generation Pipeline:**- Retrieved context + conversation history ‚Üí GPT-4- Temperature 0.7 for balance of creativity/accuracy- Source tracking for explainability**Interesting Technical Details:****1. Chunking Strategy**Instead of naive character splitting, I implemented boundary-aware chunking:```python# Tries to break at sentence endingsboundary = max(chunk.lastIndexOf('.'), chunk.lastIndexOf('\n'))if boundary > chunk_size * 0.5:    break_at_boundary()```**2. Hybrid Search**Vector search with text-based fallback:- Primary: Semantic similarity via embeddings- Fallback: Keyword matching for edge cases- Confidence scoring combines both approaches**3. Context Window Management**- Dynamic context sizing based on query complexity- Prioritizes recent conversation + most relevant chunks- Max 2000 chars to stay within GPT-4 limits**Performance Metrics:**- Embedding generation: ~100ms per chunk- Vector search: ~200-500ms across 1000+ chunks- End-to-end response: 2-5 seconds- Relevance accuracy: 85%+ (human eval)**Production Challenges:**1. **OpenAI rate limits** - Implemented exponential backoff1. **Vector storage** - MongoDB works for <10k chunks, considering Pinecone for scale1. **Cost optimization** - Caching embeddings, batch processing**Results:**Customer queries like ‚ÄúWhat time is check-in?‚Äù now get specific, sourced answers instead of ‚ÄúI don‚Äôt have that information.‚ÄùAnyone else working on production retrieval-augmented systems? Would love to compare approaches!**Tools used:**- OpenAI Embeddings API- MongoDB for vector storage- NestJS for orchestration- Background job processing.
According to the NeurIPS website, workshop decisions were sent out on July 4th, but I haven‚Äôt seen an official list published yet. I‚Äôm particularly interested because I have a paper related to ML for biology, and I'm considering submitting it to a NeurIPS workshop. However, another conference with an upcoming deadline is also an option, so I‚Äôd like to decide soon.If anyone has insight or knows when the list might be released, I‚Äôd really appreciate it!.
I've been avoiding the ICLR/ICML/NeurIPS after getting unhelpful reviews with the ICLR reviews in 2024. The paper wasn't framed very well, but the NeurIPS reviews in 2023 were a lot better even if the paper wasn't accepted. Question for those who successfully published in ICLR/ICML in the latest cycle. Did you have a fairly good experience with the review process? Do you have any advice for those of us who didn't?  .
Hey everyone! This is my first time posting here, so I hope I‚Äôm doing this right üòÖI‚Äôm working on a project to detect and classify solar panels using Cascade R-CNN with a ResNet-101 backbone and FPN neck. I don‚Äôt want to use a pre-trained model ‚Äî I want to train it from scratch or fine-tune it using my own dataset.I‚Äôm running into issues figuring out the right config file for MMDetection (or any framework you recommend), and how to set up the training process properly. Most tutorials use pre-trained weights or stick to simpler architectures.Has anyone worked on training Cascade R-CNN from scratch before? Or used it with a custom dataset (esp. with bounding boxes & labels)? Any tips, working configs, or repo links would help a ton!Thank you in advance üôèAlso, if I‚Äôm posting in the wrong subreddit, feel free to redirect me!.
Your co-author, Reviewer has not submitted their reviews for one or more papers assigned to them for review (or they submitted insufficient reviews). Please kindly note the Review deadline was on the 2nd July 11.59pm AOE.===My co-author has graduated and no longer worked in academic anymore. How can I handle that? It is not fair to reject my paper!.
Hi everyone,LLMs have made me feel like I can understand anything, but I‚Äôve been frustrated trying to truly understand ML papers using just ChatGPT or static PDFs. Summaries can help, but then I have to go back to the paper and read it linearly to deeply understand it, and I have long chatgpt conversations which I just can't track. So I built an interface designed to support a non-linear, brain-like exploration of papers ‚Äî paired with a tutor in a chat interface that guides your understanding.¬†https://preview.redd.it/vqv65julfxaf1.png?width=1725&format=png&auto=webp&s=0e09f203a863527d478568332dc6e3cbeb99fd87Here is a screenshot of what it looks like.     Try it out at:¬†[proread.ai/llm-papers](http://proread.ai/llm-papers)1. Knowledge maps let you see how ideas within a paper relate to each other and how papers connect across a field. Start with my curated maps of foundational LLM papers or build your own for any paper/set of papers you‚Äôre reading. You can also listen to the map as a podcast.2. You have a chat based tutor as with ChatGPT but your questions keep updating the knowledge map so you don't lose anything3. The map itself is an editable notebook which allow you to take notes, mark concepts as completed, tag concepts, and construct your own mental model as you read. You can not only read summaries but can go down to actual source content in readers where you want to.4. You can make your own space with your own papers or other docs (PDF/txt/html/URLs) and create interactive maps personalized to your research or study needs.The goal is to move beyond linear reading or static summarization: to create a space where understanding evolves dynamically, like how you actually think, with a tutor helping you make sense of it all.Please try it out at:¬†[proread.ai/llm-papers](http://proread.ai/llm-papers)I‚Äôm looking for feedback from other researchers or paper readers ‚Äî would this kind of non-linear, guided exploration help you understand tough topics/papers better than traditional PDFs or chat tools? What‚Äôs missing or confusing?Thanks!.
In the ACL universe, ACL, EMNLP, and NAACL are generally considered equal. EACL is considered a bit lower but highly reputable and maybe even the same by some. I haven't heard much about the relatively newer AACL. What's your opinion on papers published there? Is it in the same ballpark of reputation, or is it still significantly lagging behind?.
# This optimizer wrapper for continual learning is guided by the condition number (Œ∫) of model tensors. It identifies and updates only the least anisotropic parameters to preserve pre-trained knowledge and mitigate catastrophic forgetting due to a synergy of factors: their inherent numerical stability makes them less susceptible to training noise, and their less specialized nature allows for robust adaptation without overwriting critical, highly specific pre-training knowledge, thereby effectively mitigating catastrophic forgetting of foundational capabilities (see the link to the paper in the repository):¬†[https://github.com/oswaldoludwig/kappaTune](https://github.com/oswaldoludwig/kappaTune).
Hi guys. I'm working on my bachelor's thesis right now and am trying a find a way to compare the Dense Video Captioning abilities of the new(er) proprietary models like Gemini-2.5-Pro, GPT-4.1 etc. Only I'm finding to have significant difficulties when it comes to the transparency of benchmarks in that area.For example, looking at the [official Google AI Studio webpage](https://developers.googleblog.com/en/gemini-2-5-video-understanding/), they state that Gemini 2.5 Pro achieves a value of 69.3 when evaluated at the YouCook2 DenseCap validation set and proclaim themselves as the new SoTA. The leaderboard on [Papers With Code](https://paperswithcode.com/sota/dense-video-captioning-on-youcook2) however lists HiCM¬≤ as the best model - which, the way I understand it, you would need to implement from the ground up based on the methods described in the research paper as of now - and right after that Vid2Seq, which Google claims is the old SoTA that Gemini 2.5 Pro just surpassed.I faced the same issue with [GPT-4.1](https://openai.com/index/gpt-4-1/), where they state >Long context: On Video-MME, a benchmark for multimodal long context understanding, GPT‚Äë4.1 sets a new state-of-the-art result‚Äîscoring 72.0% on the long, no subtitles category, a 6.7%abs improvement over GPT‚Äë4o.but the official Video-MME leaderboard does not list GPT-4.1. Same with VideoMMMU ([Gemini-2.5-Pro](https://deepmind.google/models/gemini/pro/) vs. [Leaderboard](https://huggingface.co/datasets/lmms-lab/VideoMMMU)), [ActivityNet Captions](https://paperswithcode.com/sota/dense-video-captioning-on-activitynet) etc.I understand that you can't evaluate a new model the second it is released, but it is very difficult to find benchmarks for new models like these. So am I supposed to ""just blindly trust"" the very company that trained the model that it is the best without any secondary source? That doesn't seem very scientific to me. It's my first time working with benchmarks, so I apologize if I'm overlooking something very obvious. .
Hi, I am exploring the field of AI in video matting. I came across [matanyone](https://github.com/pq-yang/MatAnyone) which seems like one of the best and latest ones. However, based on my experiments this feels even this is far from production use cases for very high resolutions. What are some models that are good for this?Looking to connect with people pursuing research or working on AI in video matting. Please DM or comment here, would like to have a quick chat!.
Hi, i wanted to make Mario learn to play the original super-marino-bros from the library    gym_super_mario_bros  and wanted to use a genetic algorithm. My genomes are lists of weights. I apply a genome aka the weights to a CNN. The CNN gets the current frame (converted to 84x84 grayscale) as input and processes it until I get one out of 7 possible actions to take for Mario. Mario then takes this action, gets a reward for this action, and the next frame is processed and so on. Finally I gave Mario additional rewards for reaching the flag and being quick.I tried multiple crossover functions including point-crossover, uniform-crossover and mlx-alpha-crossover. I adapt my mutation rate based on the fitness aka if it stagnates for too long or not. Selection is usually just the top k fittest genomes. I also used big populations like 300 for 30 generations or 300 generations with a population of 30. Nothing worked, he never once reached the flag. He has no problem quickly learning to jump over enemies and obstacles and moves quick. But he somehow gets stuck at the blocky stairs. He literally does nothing once he reaches them and I have no idea how. I used all combinations of crossover/mutation-rates/... but no success. I also used frame stacking and frame skipping.My alternative approach of the genome being directly the actions and using crossover and everything on them even worked better.I know this is a quite a high level explanation but I can provide more details if needed. My CNN has 2 convolutional layers with 4 input channels, 16 output channels and my kernels are 8x8 and I use stride of 4. the last layer has 32 feauture maps of size 9x9 which I just put into final output layers to give me 7 logits (these are the possible actions) and take the highest one. This is the rough plan. I could adjust a lot of stuff but I would non the less expect to at least have one Mario reach the flag at least. Does anyone have ideas or experience with this library and genetic algorithms ?.
I encountered this¬†[talk¬†](https://www.youtube.com/watch?v=mYRqvB1_gRk)where the speaker (TimotheÃÅe Lacroix of Mistral) states that an optimal batch-size is hardware dependent and can be calculated as 2xflops/mem\_bandwidth (6:40) -- Hence an optimal batchsize (B\*) for an A100 is 400.I had some confusion on this formula - The memory bandwidth for a an A100 is 2TB/s, while the FLOPs (assuming FP16) are 312 TFlop - Can TFlops be divided by TBs though they are fundamentally different units?Appreciate anyone who can help explain this - If anyone has suggested materials to learn more about how this number was derived, I would be very happy to take a lookI'm sure its related to[¬†Arithmetic intensit](https://www.iguazio.com/glossary/arithmetic-intensity/)y but that number is simply 312/2=156EDIT:Did some research based on answers and resources here and tried to come up with an explanation - If anyone cared to feedback or point out areas of improvement, would really appreciate it**Arithmetic Intensity**Performance is defined by memory bandwidth, compute, latency. If compute is more limited than memory, it is compute bound. Vice versa for memory bound. Arithmetic intensity is the ratio of compute operations to memory operations (Specifically FLOPs per byte transferred). If you are compute bound, optimizing for memory does not benefit your system, and vice versa. Calculating arithmetic intensity tells you which parts of your system to focus on optimizing. Arithmetic intensity itself is calculated as a hardware threshold as well as for individual operations. Real world performance depends on actual model architecture, dataset characteristics, training/inference regime, memory access patterns, cache utilization, batch size, operator fusion, etc‚Ä¶Arithmetic intensity can also be applied to operations as below.¬†**Values only approximate:**Low arithmetic intensity operations (10-100 FLOPs/byte) include elementwise ops, activations, normalizations (Example, addition involves moving 2N values to GPU but doing only N ops)High intensity ops (100 - 1000 FLOPs/byte) include matmuls and convolutions. Larger batch sizes also increase intensity - This is because input data increases while the memory access cost for weight matrices remains constant - Hence larger batches improve GPU compute utilization.Hence, frameworks focus heavily on fusion of low intensity operations. Operations can have different arithmetic intensity depending on problem size (small matrices have lower intensity because less data can be reused), implementation (tiled algorithms are faster), precision (FP16 doubles available compute).Consider the arithmetic intensity threshold. At 312 TFLOPs and a mem bandwidth of 1.55 TB/s for FP16 tensor ops in an A100, the arithmetic intensity threshold is roughly 201. Ops with intensity below this are memory bound, while ops above it are compute bound. A memory bound operation results in idle GPU compute while a compute bound operation results in bottlenecking. In practice, hitting this precise 100% resource utilization is rare.¬†.
I recently released this preprint benchmarking LLM capability of self-correction.**The Problem**: LLM self-correction is important for reliability, but it's hard to benchmark because naturally occurring errors are rare. So I built Self-Correction Bench by systematically injecting errors into LLM reasoning traces.**Key Discovery**: LLMs systematically fail to correct errors in their own outputs while successfully correcting identical errors in external inputs. I call this the ""Self-Correction Blind Spot."" **Results across 14 models**: \- 64.5% average blind spot rate \- Simply appending ""Wait"" reduces blind spots by 89.3% without finetuning\- Other correction markers (""But"", ""However"") also help \- Reasoning models generate these markers when they see errors**Insight**: I analyzed post-training data and found non-reasoning instruction datasets are 95%+ lacking correction markers. RL-trained reasoning models don't show this blind spot - their generation contains lots of correction markers - suggesting they learned error correction through trial and error.**Implications**: This affects AI safety and reliability. If LLMs can't catch their own mistakes, we need better training paradigms or activation mechanisms like correction markers. It seems RL is very promising.Benchmark: [https://huggingface.co/papers/2507.02778](https://huggingface.co/papers/2507.02778)Author here - happy to discuss the methodology and have your feedback..
I‚Äôm developing an application using SAM 2.1 (via FastAPI) for real-time object segmentation from a live camera feed. The frontend sends either a box or point prompt to the backend, which returns a mask that‚Äôs composited into a canvas for manipulation and export.Each prompt type works well in isolation ‚Äî but they‚Äôre inconsistent across different object classes. A couple examples:* **Plant in pot**: A box prompt captures the foliage but often excludes the pot. A point prompt on the leaves sometimes segments a single leaf, especially with fine stems or dense texture.* **Theragun / handheld tool**: A point near the handle often gives excellent results. A box prompt sometimes returns background or over-segments nearby objects.I‚Äôm now exploring combining both prompt types: drawing a bounding box *and* allowing the user to tap inside it to reinforce intent. Since SAM 2.1 accepts both `boxes` and `point_coords + point_labels`, this seems feasible ‚Äî but I‚Äôm curious:* Have others here tried combining these prompts in production or research tools?* Are there heuristics you‚Äôve found effective for prioritizing or weighting prompt types in ambiguous contexts?* Do you use `multimask_output=True` and apply post-selection based on area, IOU, or visual saliency?* Any recommended architectures or methods for mask refinement after prompt-based SAM segmentation (e.g. to recover small appendages like wires, roots, or hollow interiors)?Would appreciate insights from anyone deploying SAM variants or experimenting with segmentation UIs. Trying to optimize for a broad class of ‚Äúirregular physical objects‚Äù where semantic boundaries aren‚Äôt always visually dominant..
It seems like all papers have to define what the problem they're using is, and discuss traditional techniques to then go on to their contribution. My understanding this is to show you've actually gone through the effort of reviewing the literature? Still, as I'm reading papers, I can't help but often skim over the introduction very quickly or almost not bother reading it since I know, say, what an LSTM or a Transformer is.Is that expected or am I missing something? Is the introduction mostly there to communicate to others you've done the review well? to inform readers who may not have an ML background?.
**TL;DR**: Comprehensive benchmarks of Kreuzberg, Docling, MarkItDown, and Unstructured across 94 real-world documents. Results might surprise you.## üìä **Live Results**: https://goldziher.github.io/python-text-extraction-libs-benchmarks/---## ContextAs the author of [Kreuzberg](https://github.com/Goldziher/kreuzberg), I wanted to create an **honest, comprehensive benchmark** of Python text extraction libraries. No cherry-picking, no marketing fluff - just real performance data across 94 documents (~210MB) ranging from tiny text files to 59MB academic papers.**Full disclosure**: I built Kreuzberg, but these benchmarks are automated, reproducible, and the methodology is completely open-source.---## üî¨ **What I Tested**### Libraries Benchmarked:- **[Kreuzberg](https://github.com/Goldziher/kreuzberg)** (71MB, 20 deps) - My library- **[Docling](https://github.com/DS4SD/docling)** (1,032MB, 88 deps) - IBM's ML-powered solution- **[MarkItDown](https://github.com/microsoft/markitdown)** (251MB, 25 deps) - Microsoft's Markdown converter- **[Unstructured](https://github.com/Unstructured-IO/unstructured)** (146MB, 54 deps) - Enterprise document processing### Test Coverage:- **94 real documents**: PDFs, Word docs, HTML, images, spreadsheets- **5 size categories**: Tiny (<100KB) to Huge (>50MB)- **6 languages**: English, Hebrew, German, Chinese, Japanese, Korean- **CPU-only processing**: No GPU acceleration for fair comparison- **Multiple metrics**: Speed, memory usage, success rates, installation sizes---## üèÜ **Results Summary**### Speed Champions üöÄ1. **Kreuzberg**: 35+ files/second, handles everything2. **Unstructured**: Moderate speed, excellent reliability3. **MarkItDown**: Good on simple docs, struggles with complex files4. **Docling**: Often 60+ minutes per file (!!)### Installation Footprint üì¶- **Kreuzberg**: 71MB, 20 dependencies ‚ö°- **Unstructured**: 146MB, 54 dependencies- **MarkItDown**: 251MB, 25 dependencies (includes ONNX)- **Docling**: 1,032MB, 88 dependencies üêò### Reality Check ‚ö†Ô∏è- **Docling**: Frequently fails/times out on medium files (>1MB)- **MarkItDown**: Struggles with large/complex documents (>10MB)- **Kreuzberg**: Consistent across all document types and sizes- **Unstructured**: Most reliable overall (88%+ success rate)---## üéØ **When to Use What**### ‚ö° **[Kreuzberg](https://github.com/Goldziher/kreuzberg)** (Disclaimer: I built this)- **Best for**: Production workloads, edge computing, AWS Lambda- **Why**: Smallest footprint (71MB), fastest speed, handles everything- **Bonus**: Both sync/async APIs with OCR support### üè¢ **[Unstructured](https://github.com/Unstructured-IO/unstructured)**- **Best for**: Enterprise applications, mixed document types- **Why**: Most reliable overall, good enterprise features- **Trade-off**: Moderate speed, larger installation### üìù **[MarkItDown](https://github.com/microsoft/markitdown)**- **Best for**: Simple documents, LLM preprocessing- **Why**: Good for basic PDFs/Office docs, optimized for Markdown- **Limitation**: Fails on large/complex files### üî¨ **[Docling](https://github.com/DS4SD/docling)**- **Best for**: Research environments (if you have patience)- **Why**: Advanced ML document understanding- **Reality**: Extremely slow, frequent timeouts, 1GB+ install---## üìà **Key Insights**1. **Installation size matters**: Kreuzberg's 71MB vs Docling's 1GB+ makes a huge difference for deployment2. **Performance varies dramatically**: 35 files/second vs 60+ minutes per file3. **Document complexity is crucial**: Simple PDFs vs complex layouts show very different results4. **Reliability vs features**: Sometimes the simplest solution works best---## üîß **Methodology**- **Automated CI/CD**: GitHub Actions run benchmarks on every release- **Real documents**: Academic papers, business docs, multilingual content- **Multiple iterations**: 3 runs per document, statistical analysis- **Open source**: Full code, test documents, and results available- **Memory profiling**: psutil-based resource monitoring- **Timeout handling**: 5-minute limit per extraction---## ü§î **Why I Built This**Working on [Kreuzberg](https://github.com/Goldziher/kreuzberg), I worked on performance and stability, and then wanted a tool to see how it measures against other frameworks - which I could also use to further develop and improve Kreuzberg itself. I therefore created this benchmark. Since it was fun, I invested some time to pimp it out:- Uses **real-world documents**, not synthetic tests- Tests **installation overhead** (often ignored)- Includes **failure analysis** (libraries fail more than you think)- Is **completely reproducible** and open- Updates **automatically** with new releases---## üìä **Data Deep Dive**The [interactive dashboard](https://goldziher.github.io/python-text-extraction-libs-benchmarks/) shows some fascinating patterns:- **Kreuzberg dominates** on speed and resource usage across all categories- **Unstructured excels** at complex layouts and has the best reliability- **MarkItDown is useful** for simple docs shows in the data- **Docling's ML models** create massive overhead for most use cases making it a hard sell---## üöÄ **Try It Yourself**```bashgit clone https://github.com/Goldziher/python-text-extraction-libs-benchmarks.gitcd python-text-extraction-libs-benchmarksuv sync --all-extrasuv run python -m src.cli benchmark --framework kreuzberg_sync --category small```Or just check the live results: https://goldziher.github.io/python-text-extraction-libs-benchmarks/---## üîó **Links**- **üìä Live Benchmark Results**: https://goldziher.github.io/python-text-extraction-libs-benchmarks/- **üìÅ Benchmark Repository**: https://github.com/Goldziher/python-text-extraction-libs-benchmarks- **‚ö° Kreuzberg (my library)**: https://github.com/Goldziher/kreuzberg- **üî¨ Docling**: https://github.com/DS4SD/docling- **üìù MarkItDown**: https://github.com/microsoft/markitdown- **üè¢ Unstructured**: https://github.com/Unstructured-IO/unstructured---## ü§ù **Discussion**What's your experience with these libraries? Any others I should benchmark? I tried benchmarking `marker`, but the setup required a GPU.Some important points regarding how I used these benchmarks for Kreuzberg:1. I fine tuned the default settings for Kreuzberg.2. I updated our docs to give recommendations on different settings for different use cases. E.g. Kreuzberg can actually get to 75% reliability, with about 15% slow-down.3. I made a best effort to configure the frameworks following the best practices of their docs and using their out of the box defaults. If you think something is off or needs adjustment, feel free to let me know here or open an issue in the repository..
We recently released a preprint calling for ML conferences to establish a ""Refutations and Critiques"" track. I'd be curious to hear people's thoughts on this, specifically (1) whether this R&C track could improve ML research and (2) what would be necessary to ""do it right""..
I‚Äôm working on a group recommender system where I form user groups automatically (e.g. using KMeans) based on user embeddings learned by a GCN-based model.Here‚Äôs the setup:	‚Ä¢	I split the dataset by interactions, not by users ‚Äî so the same user node may appear in both the training and test sets, but with different interactions.	‚Ä¢	I train the model on the training interactions.	‚Ä¢	I use the resulting user embeddings (from the trained model) to cluster users into groups (e.g. with KMeans).	‚Ä¢	Then I assign test users to these same groups using the model-generated embeddings.üîç My question is:Even though the test set contains only new interactions, is there still a data leakage risk because the user node was already part of the training graph? That is, the model had already learned something about that user during training.be a safer alternative in this context.Thanks!.
Hi all,Need a bit of help understanding speculative sampling. [arXiv:2211.17192v2](https://arxiv.org/abs/2211.17192)The idea is for the small model to generate the completions and the larger model to evaluate them. If the LLM accepts all the tokens generated by the SLM, it generates an additional token. If not, it generates the replacements of the tokens it rejected. Section 2.1 and 2.3 in the paper discuss this.Given tokens x\_{<t}, p(x\_t | x\_{<t}) is the distribution generated by the target LLM. q(x\_t | x\_{<t}) is generated by a smaller, more efficient model (SLM). We want x \~ p(x), but we sample x\~q(x) and keep it IF q(x) <= p(x).I don't quite get the logic of keeping the x\~q(x) sample if q(x) <= p(x). I'm sure it is something simple but a blind spot for someone dumb as me. Can someone please explain in simple terms?Given a well-trained and a less capable model, and a sequence, in general, is there a relation between the probability distributions from both models for the next token? I would expect that the generations from the LLM have a higher likelihood of matching the next sequence in the training data..
While I understand the traditional conference review paradigm involving initial scores, author rebuttals, and final scores, this model is beginning to show clear cracks under the scale and competitiveness of today‚Äôs A-level (and even mid-tier) venues. Increasingly, reviewers tend to give deliberately conservative or low pre-rebuttal scores, knowing that authors will be compelled to respond in the rebuttal phase. Even when a higher score is justified, reviewers often hold back, defaulting to borderline decisions just to see how the authors respond.This issue is even more pronounced with ACL Rolling Review, where the scoring system is vague and lacks standard terminology such as Accept, Borderline, or Reject. This makes the process even more opaque. The ARR policy clearly states that responding to review comments is not mandatory. Yet, as an author, I am expected to thoroughly and respectfully address reviewer concerns, even when they are speculative or unreasonable. This one-sided non-obligation creates a deeply flawed power imbalance.Here‚Äôs where it gets worse.Many reviewers, when submitting their own papers and receiving poor reviews, tend to reflect their frustration onto the papers they are assigned to review. I have observed the following patterns:Case 1: A reviewer receives bad reviews on their own paper and becomes unnecessarily harsh or disengaged in the reviews they provide for others.Case 2: Prior to seeing their own reviews, reviewers play it safe by giving slightly lower pre-rebuttal scores than deserved. After receiving unfavorable reviews, they either ignore rebuttals completely or refuse to revise their scores, even when rebuttals clearly address their concerns.This leads to a toxic feedback loop where every paper becomes a collateral victim of how a reviewer‚Äôs own submission is treated. I have seen this firsthand.In the current ARR May cycle:I received 10 reviews across 3 papers, with only 2 reviewers responding post-rebuttal.From 4 papers I reviewed, totaling 12 reviews, only 6 reviewers responded, and 4 of those responses were mine.We need to acknowledge a basic truth: acknowledging a rebuttal should be a moral minimum. Yet today, there is no incentive for honest reviewing, and no consequence for disengaged or negligent behavior. Why should any of us continue to uphold moral obligations, being fair, constructive, and thorough, when our own work receives careless and dismissive treatment?This culture cannot be allowed to continue. Unless ACL/ARR enforces stricter policies, such as making post-rebuttal justification and score updates mandatory (as CVPR and other CVF conferences do), the system will continue to erode.I am a young researcher trying to do my part for this community. But after repeated experiences like this, what incentive do I have to stay committed to high standards as a reviewer? Why should I put in the effort when others do not?A system where morality is optional will ultimately breed apathy and toxicity. It is time for a structural shift.Always, to the hope.#acl #emnlp #arr.
Hey all,I‚Äôm trying to build ML model for OOS prediction of an item of an imbalanced dataset, which sampling technique should I use and how should I evaluate that sampling technique to create a better model.Appreciate your thoughts and responses.Thanks.
Does anyone know solid baselines or open-source implementations for group recommendation systems?I‚Äôm developing a group-based recommender that relies on classic aggregation strategies enhanced with a personalized model, but I‚Äôm struggling to find comparable baselines or publicly available frameworks that do something similar.If you‚Äôve worked on group recommenders or know of any good benchmarks, papers with code, or libraries I could explore, I‚Äôd be truly grateful for your. Thanks in advance!.
Hi everyone,I'm working on an industrial use case where I tried to use a Graph Neural Network to \*\*predict edges between tasks\*\*, based solely on node features.Each graph represents 10-60 tasks (nodes), and I have about 1200 such graphs for training. Each task comes with features (label, equipment type), but there are no edges given at inference time, the goal is to infer all connections -> generate the full adjacency structure.The key point: whether an edge exists between two nodes depends on the global context, not just pairwise similarity.I‚Äôve tried GCNs and GATs (with various edge construction strategies during training), but I'm consistently getting poor performance.So I‚Äôm wondering:\-  Is this just a bad fit for classical GNNs? \- Should I switch to Transformer-like models that encode full-node context? Or even fine-tuning ?\- Do I need a much larger dataset to make a GNN work in this setup?\- Is it better to frame this as a graph generation problem (autoencoders) ?  I know GNN needs edge-index during inference, but i genuinely do not seem to find the right model for my project....
Data Science is a fascinating field, with always something to learn. Recently, I came across an interesting (though not ideal) approach to hyperparameter optimization: Evolutionary Algorithms (EA). EAs are a subset of Genetic Algorithms that work on Darwin‚Äôs idea of ‚Äúsurvival of the fittest‚Äù. While Grid Search and Manual Tuning remain the go-to approaches, they are limited by predefined search space and, in some sense, are brute-force methods to optimize hyperparameters. Interestingly, Evolutionary Algorithms work on the principles of biology and genetics:1. They start with a population of candidate solutions (hyperparameters) and treat them as chromosomes.2. Each chromosome is then evaluated using a fitness test (for example, precision, absolute error etc.)3. The best-fit candidates are selected as parents.4. Parent solutions generate offspring using crossover (combining individual traits) and mutation (small random changes)5. The offspring are then used as candidate solutions, and steps 1-4 are repeated till an optimal solution (under a defined threshold) is met or iterations are exhausted.While this is a computationally expensive solution, EA offers an adaptive methodology instead of static search methods, which can look for solutions that are not pre-defined.Thoughts?Note: EA is not a silver bullet to all your optimization problems..
Have people noticed that AI/ML/DS job interviews now feel more SWE-like? For example, relying more on data structures and algorithms leetcode questions. I‚Äôve noticed in my professional friend groups more people are being asked these questions during the coding interview..
**{another edit} I got it that it won't be used for decision making. I posted it to ask if it is true.. and realized that many of us did not know about this**<previous post>AAAI-26' Two-phase reviewing for the Main Track:[https://aaai.org/aaai-launches-ai-powered-peer-review-assessment-system/](https://aaai.org/aaai-launches-ai-powered-peer-review-assessment-system/)Phase 1: Two reviews supplemented by one AI-generated, non-decisional review.Phase 2: Additional reviews for papers not rejected in Phase 1.**Author response after Phase 2, only for papers not rejected in Phase 1.**Edit : They also said (but why the use of AI tho )  The pilot program will thoughtfully integrate LLM technology at two specific points in the established review process:Supplementary First-Stage Reviews: LLM-generated reviews will be included as one component of the initial review stage, providing an additional perspective alongside traditional human expert evaluations.Discussion Summary Assistance: LLMs will assist the Senior Program Committee (SPC) members by summarizing reviewer discussions, helping to highlight key points of consensus and disagreement among human reviewers.<previous post>.
[https://www.youtube.com/watch?v=-\_M5PY5BC9I](https://www.youtube.com/watch?v=-_M5PY5BC9I).
Paper with Code was being spammed (https://www.reddit.com/r/MachineLearning/comments/1lkedb8/d\_paperswithcode\_has\_been\_compromised/) before, and now it is compoletely down. It was also down a coupld times before, but seems like this time it has lasted for days. (https://github.com/paperswithcode/paperswithcode-data/issues).
Currently I'm quite interested in NLP theory, and have some questions about how to make them count for RS roles in industry roles at top AI labs.  (1) Does the number of papers help? My impression is that having many papers that are ""purely theoretical"" may not help that much, and AI labs will only count the number of ""relevant papers"" (and exclude those that are less relevant).   (2) If the theory paper also yields strong empirical results, is it important to frame it as an empirical paper (and maybe put the theory in the appendix)? This could compensate for any perceived weakness with theoretical work.  (3) What topics in language/vision models are particularly relevant in industry? Efficiency of LLMs is one priority; MoE, sparse attention & structured sparsity, are two approaches to efficient LLMs..
Working on a project that needed both semantic search and content moderation, so I built an API that handles both.**The problem it solves:**¬†Expensive GPU instances required for inference, hard to scale infrastructure. Most teams give up quickly after realizing the infrastructure needed to handle this.**What it does:**¬†Semantic search + content moderation. You can search images by describing them (""girl with guitar"") or find text by meaning (""movie about billionaire in flying suit"" ‚Üí Iron Man). Plus NSFW detection with specific labels.**Stack:*** Rust Candle for ML models (Clip)* Rust Axum + Tokio for the API* Vector DB for searchI am considering switching to a more lightweight CLIP based model like mobileclip or clip quantized. What do you guys think?.
I am pretraining a GPT-style language model with PyTorch XLA and wanted to know what operations to fuse with Pallas. I use rotary positional embeddings, SwiGLU, and RMSNorm, and I am working on adding FlashAttention to my codebase. I also employ FSDPv2 with SPMD for distributed training..
Yesterday, [Cloudflare had announced](https://blog.cloudflare.com/content-independence-day-no-ai-crawl-without-compensation/) that their protections against AI crawler bots will be turned on by default. Website owners can choose to opt out if they wish by charging AI companies for scraping their websites (""pay per crawl"").The era where AI companies simply recursively crawled websites with simple GET requests to extract data is over. Previously, AI companies simply disrespected robots.txt - but now that's not enough anymore.Cloudflare's protections against crawler bots are now pretty sophisticated. They use generative AI to produce scientifically correct, but unrelated content to the website, in order to waste time and compute for the crawlers (""[AI Labyrinth](https://blog.cloudflare.com/ai-labyrinth/)"").  This content is in pages that humans are not supposed to reach, but AI crawler bots should reach - invisible links with special CSS techniques (more sophisticated than `display: none`), for instance. These nonsense pages then contain links to other nonsense pages, many of them, to keep the crawler bots wasting time reading completely unrelated pages to the site itself and ingesting content they don't need.Every possible way to overcome this, as I see it, would significantly increase costs compared to the simple HTTP GET request recursive crawling before. It seems like AI companies would need to employ a small LLM to check if the content is related to the site or not, which could be extremely expensive if we're talking about thousands of pages or more - would they need to feed every single one of them to the small LLM to make sure if it fits and isn't nonsense?How will this arms race progress? Will it lead to a world where only the biggest AI players can afford to gather data, or will it force the industry towards more standardized ""pay-per-crawl"" agreements?.
This article addresses the challenge of classification with minimal multiplication operations while maintaining accuracy above 75%. The MNIST dataset serves as an example, where a single permutation neuron, utilizing three classical neurons, achieves 77% accuracy.# Concept of the Permutation NeuronThe Permutation Neuron is a computational unit that implements a permutation-based transformation of input signals. The neuron maintains a set of internal vectors that are reordered based on their interaction with the input data. This reordering process maps the input space to a discrete set of output patterns, where each pattern corresponds to a specific permutation of the internal vectors.For classifying the 10 digits of the MNIST dataset, at least 10 distinct neuron states are required. Since the number of permutations is determined by the factorial of the number of neurons, a minimum of 4 neurons (4! = 24 permutations) is needed to cover 10 classes. However, by subtracting the value of one neuron from the others (normalization), only three neurons need to be computed, with the fourth set to zero, preserving the order of permutations. This reduces computational cost while maintaining 24 unique states for classification.For the MNIST classification task, the permutation neuron operates as follows: three neurons with linear activation functions compute values based on the input image data, while a fourth neuron is fixed at zero. These four values are ordered to form one of 24 possible permutations (4!), such as ACZB. Using the Lehmer code, each permutation is mapped to a unique number from 0 to 23, which is then assigned to one of the 10 MNIST classes (e.g., digits 0‚Äì9).# Training with a Genetic AlgorithmThe search space for parameters is limited to 2355 values, where each of the three neurons processes input data of size 784 (MNIST image pixels) plus a bias term (3 √ó (784 + 1)). The 24 permutation states generated by the permutation neuron are determined by a greedy algorithm based on the MNIST training set, enabling the mapping of permutations to 10 classes. A genetic algorithm is employed to optimize the neuron weights, as the parameter space is poorly understood but assumed to contain local optima corresponding to effective solutions.For weight optimization, a genetic algorithm with a population of 50 individuals is used. The BLX-Alpha crossover (with parameter k=2) is applied over two parents, with a 2% probability of random mutation. These settings achieved a classification accuracy of 77% on the MNIST dataset.# CodeThe implementation of the permutation neuron, including the genetic algorithm and the greedy algorithm for mapping permutations to MNIST classes, is available at¬†[GitHub](https://github.com/sgr-team/math/tree/main/problems/pn). The code includes an experiment achieving 77% accuracy (results in mnist\_46257.json).Readers are encouraged to reproduce the experiment or propose improved solutions, such as higher accuracy or fewer multiplication operations. Improved results will be published with attribution to their authors..
Hi! My colleagues have recently published a Python package for [TabM](https://github.com/yandex-research/tabm) \-- a **simple and powerful DL architecture** for solving predictive tasks on **tabular data** (classification, regression, etc.).In a nutshell, TabM efficiently imitates an ensemble of MLPs (see the image below). This basically means that TabM has the power of an ensemble, but at the same time remains practical and scalable. Among the recent highlights: üèÜ **TabM has been successfully used on Kaggle**, including the winning solutions! The package provides the PyTorch implementation of TabM, as well as PyTorch layers and functions for building custom TabM-like models.Installation:```pip install tabm```- [Paper](https://arxiv.org/abs/2410.24210)- [Package](https://github.com/yandex-research/tabm)- [Colab example](https://colab.research.google.com/github/yandex-research/tabm/blob/main/example.ipynb)[TabM model illustration](https://preview.redd.it/pl3oth89qgaf1.png?width=2432&format=png&auto=webp&s=37ed08404f3eee2e2a72dc41aa796b17ed6ae32b).
Hey all,I am a first year PhD student in a top biomedical program in the US. One of the labs I am most interested in studies how to more effectively use AI/ML to enhance the drug discovery and development process. Although I current have only a limited knowledge of coding (really just experience with R and a little C++) the PI has told me he'd be happy to have me join the group. Still, I wonder about the applicability of this niche expertise. Does having done a PhD in biomedical focused AI/ML allow for the possibility of being hired in say finance AI/ML? What about AI/ML research in big tech? Or would you say it is only applicable in Big Pharma/biomed startup research?Thanks for your insights..
Recently, I‚Äôve seen many researchers adopt this style of illustration to present an architectural view of their method or approach. These visuals are clean, professional, and visually appealing, perfect for research papers and presentations.I've tried replicating this style using [draw.io](http://draw.io), but I haven‚Äôt been able to achieve the same level of quality or aesthetics.Could anyone suggest tools or software commonly used to create such research illustrations?I'm particularly interested in tools that are:1. Suitable for academic or technical diagrams2. Capable of producing high-quality, publication-ready visuals3. Flexible for custom styling or layoutsAny recommendations would be greatly appreciated!Please check Illustration here: [https://imgur.com/a/VWiKD3Q](https://imgur.com/a/VWiKD3Q).
By fluency I mean:1. Read a paper and and without much problem implement the techniques mentioned, whether it's building something from scratch using the paper as guidance (even in the absence of code), or modifying existing models.2. Having an idea and being able to translate that into designing new architectures or modifying existing models.3. Improving models.Think of people like [Phil Wang](https://github.com/lucidrains) who is very prolific at reproducing papers and or improving them. I'm very curious to know in your experience what made it ""click"" that unlocked your ability to be productive with these things. I suspect the boring answer is ""just reproduce papers, bro"", but I was hoping to learn about people's own experience/journey on this and if you guys have any specific insight/tricks that can be useful for others to know about. Like maybe you have a good workflow for this or a good pipeline that makes you 10x more productive, or you have some niche insight on designing/modifying/improving models that people don't usually talk about etc..
In terms of academia prestige (for future prof positions), where would you place UofT ML PhD? Is it better RoI to do it at a T10 American school (UIUC, Georgia Tech, UT Austin, UWash, etc) for name recognition considering the advisors are equivalent? Also, how does UofT PhD fare against Oxbridge DPhil these days?.
I‚Äôm currently a PhD student in Machine Learning, working on a research topic that isn‚Äôt considered ‚Äúhot‚Äù in the current academic or industrial landscape. Despite this, I‚Äôve managed to publish as the lead author at ICML, NeurIPS. And  twice at ECML. I also have two co-authored publications at ECAI.I‚Äôve noticed that many  PhD students in the U.S. seem to have much stronger publication records, often in trendier areas. This makes me question how competitive I really am in the current job market‚Äîespecially given the wave of layoffs and increasing demand for very specialized expertise in industry.That said, I do have a strong foundation in core ML, Deep Learning, and LLMs (although LLMS aren‚Äôt the direct focus of my PhD research).Given all of this, I‚Äôm trying to realistically assess: ‚Ä¢ What are my current chances of landing a demanding, high-quality job in industry or research after my PhD? ‚Ä¢ What could I do now to improve those chances? ‚Ä¢ Goal is FANNG.I‚Äôd greatly appreciate any  feedback.Edit: My research focuses on anomaly detection, a less trendy area compared to the current popularity of large language models and reinforcement learning..
I really don‚Äôt get the point of setting up a new AI lab at Meta.  Well, maybe it‚Äôs related to the semi-acquisition of Scale AI and creating a group dedicated to Alexandr Wang.  But doesn‚Äôt the merger of Google Brain and DeepMind suggest it‚Äôs better not to split your resources in the AI war?Also would there be possible feud out there?.
Hello,I have been going through DDIM paper and have some queries on how the sampling is accelerated (appendix C.1)The authors assume that the forward can be decomposed as[Forward decomposition](https://preview.redd.it/n0yvok1liiaf1.png?width=520&format=png&auto=webp&s=0cbce45652fccf8f10441b25238e8fd8136c7e37)and backward[Backward decomposition](https://preview.redd.it/f5gtpdrmiiaf1.png?width=437&format=png&auto=webp&s=2d1f597df36d5dcdab955a167e8fb588a866184d)where tau is subsequence of timesteps \[1, T\].First thing I want to point out is that, index ""i"" should start from 2 and from 1. (Am I right in saying this ?)If you look into the decomposition, in the forward for the timesteps that are not in the subsequence, we are directly writing x\_{t}|x\_{0} and for the timesteps that are in subsequence we write x\_{tau\_{i-1}}|x\_{tau\_{i}},x\_{0}.So to mimic in the reverse we write for the timesteps that are not in subsequence x\_{0}|x\_{t} and for timesteps in the subsequence we write x\_{tau\_{i-1}}|x\_{tau\_{i}}.The above explaination looks good in intuitive sense but when I take an example and write the decomposition, the intutition doesn't come at all.[Example](https://preview.redd.it/6zn8fux3piaf1.png?width=705&format=png&auto=webp&s=6628a8167fc4e6a7458054d9872a1caf9a338292)Here the third term in backward p(x\_{3}|x\_{4},x\_{5}) = p(x\_{0}|x\_{3}) and fifth p(x\_{1}|x\_{2},x\_{3},x\_{4},x\_{5}) = p(x\_{0}|x\_{1}) doesn't make sense at all.Can someone explain how does the backward decomposition work ?Note : I don't know if this is the correct place to ask these type of questions, but I felt that other subs are not suited for this.Thanks..
Please post your personal projects, startups, product placements, collaboration needs, blogs etc.Please mention the payment and pricing requirements for products and services.Please do not post link shorteners, link aggregator websites , or auto-subscribe links.\--Any abuse of trust will lead to bans.Encourage others who create new posts for questions to post here instead!Thread will stay alive until next one so keep posting after the date in the title.\--Meta: This is an experiment. If the community doesnt like this, we will cancel it. This is to encourage those in the community to promote their work by not spamming the main threads..
Hi everyone,I‚Äôd like to share a recent preprint I uploaded to arXiv, introducing **DFReg** ‚Äì a new regularization framework for neural networks inspired by **Density Functional Theory (DFT)** in physics.**What is DFReg?**  DFReg replaces local penalties (like L2 regularization or Dropout) with a **global constraint** on the *empirical weight distribution*. It treats the weights of a neural network as a statistical density and introduces a functional penalty that encourages:* Smooth, non-peaky weight distributions* Diverse, well-spread parameter configurations* Structural regularity across layersNo architectural changes or stochastic perturbations required.**What we tested:**  We evaluated DFReg on **CIFAR-100 with ResNet-18**, comparing it to Dropout and BatchNorm. Metrics included:* Test accuracy and loss* Weight entropy* Histogram regularity* 2D FFT of convolutional filtersNotably, we also trained **BatchNorm-free ResNets** with only DFReg as the regularizer.**Key findings:*** DFReg matches or outperforms Dropout and BatchNorm on accuracy and stability* It induces more interpretable and spectrally regular weight structures* Even without L2 or BatchNorm, DFReg alone provides strong regularization**Paper**: [https://arxiv.org/abs/2507.00101](https://arxiv.org/abs/2507.00101)  Would love to hear feedback from the community‚Äîespecially if you're interested in global priors, regularization, or physics-inspired ML. Open to questions, critiques, or collaborations.Thanks!.
Hey everyone,I've been working on a personal project to understand how AI is actually being used in medical research (not just the hype), and thought some of you might find the results interesting.After analyzing nearly 1.5 million PubMed papers that use AI methods, I found some intersting results:* **Classical ML still dominates**: Despite all the deep learning hype, traditional algorithms like logistic regression and random forests account for 88.1% of all medical AI research* **Algorithm preferences by medical condition**: Different health problems gravitate toward specific algorithms * **Transformer takeover timeline**: You can see the exact point (around 2022) when transformers overtook LSTMs in medical researchI built an interactive dashboard where you can:* Search by medical condition to see which algorithms researchers are using* Track how algorithm usage has evolved over time* See the distribution across classical ML, deep learning, and LLMsOne of the trickiest parts was filtering out false positives (like ""GAN"" meaning Giant Axonal Neuropathy vs. Generative Adversarial Network).The tool is completely free, hosted on Hugging Face Spaces, and open-source. I'm not trying to monetize this - just thought it might be useful for researchers or anyone interested in healthcare AI trends.Happy to answer any questions or hear suggestions for improving it!.
Hi everyone,Below I am gathering some interview preparation tools for ML research positions. People who had been in the job market recently, which one would you recommend/ find more relevant? Any other resources that I might be missing?(1) InterviewQuery:[https://www.interviewquery.com/questions?searchQuery=&searchQuestionTag=&searchCompany=&completed=&saved=&ordering=relevancy&orderingDirection=asc&pageSize=20&page=0](https://www.interviewquery.com/questions?searchQuery=&searchQuestionTag=&searchCompany=&completed=&saved=&ordering=relevancy&orderingDirection=asc&pageSize=20&page=0)(2) DevInterview:[https://devinterview.io/questions/machine-learning-and-data-science](https://devinterview.io/questions/machine-learning-and-data-science)(3) aiofferly:[https://www.aiofferly.com/problems?page=5](https://www.aiofferly.com/problems?page=5)(4) MAD:[https://www.madinterview.com/ml?utm\_source=google&utm\_medium=cpc&utm\_campaign=22464693824&utm\_term=machine%20learning%20coding%20interview%20questions&utm\_content=178169327653&gclid=CjwKCAjw3f\_BBhAPEiwAaA3K5A0Rrw-8xhTQqlzVnBhrcCyyHXSwzgGvAzmJYvVye63uIOqQ7XBWhRoC6L0QAvD\_BwE&gad\_source=1&gad\_campaignid=22464693824&gbraid=0AAAAA\_Y9DohjdsVwcsLkazvDd4iJ64Tv5](https://www.madinterview.com/ml?utm_source=google&utm_medium=cpc&utm_campaign=22464693824&utm_term=machine%20learning%20coding%20interview%20questions&utm_content=178169327653&gclid=CjwKCAjw3f_BBhAPEiwAaA3K5A0Rrw-8xhTQqlzVnBhrcCyyHXSwzgGvAzmJYvVye63uIOqQ7XBWhRoC6L0QAvD_BwE&gad_source=1&gad_campaignid=22464693824&gbraid=0AAAAA_Y9DohjdsVwcsLkazvDd4iJ64Tv5).
Anyone working in process industry and has attempted making ‚Äúsoft sensors‚Äù before?Given a continuous industrial process with data points recorded in a historian every minute, you try to predict the outcome by applying classical ML methods such as xgboost. The use case demands that the model works like a soft(ware) sensor that continuously gives a numerical prediction of the output of the process. Not that this is not really a time series forecast (eg not looking into the distant future, just predicting the immediate outcome).Question: Shuffling the data leads to data leakage because the neighbouring data points contain similar information (contains temporal information). But if shuffling is not done, the model is extremely poor / cannot generalise well.Fellow practitioners, any suggestions for dealing with ML in that may have time series related data leakage?Thanks in advance for any kind sharing..
Does your professor share their assigned papers among their lab members and ask them to sub-review for NeurIPS? I only realized after agreeing that this is actually against [the reviewer guidelines](https://neurips.cc/Conferences/2025/ReviewerGuidelines):>Q: Can I invite a sub-reviewer to help with my reviews?>A: No, sub-reviewers are not allowed. Conflicts of interest cannot be properly checked unless reviewers are officially in the system, and sub-reviewers would not be able to participate in the discussion, which is a critical phase of the review process.So now I am a little bit worried I may be involved in something I perhaps shouldn't have been. On the other hand, perhaps this is one of those things in academia that people are against ""on paper"" but is actually an accepted practice? I think it seems common for professors to review papers through their students, but it seems like in most cases, they are officially appointed as a ""sub-reviewer"" (which NeurIPS doesn't allow) instead of giving their professor a review to pass as their own.In short: Is this normal and accepted? Does it happen in your lab, too? Should I not worry about it?**Update:** Thank you to everyone who let me know that I won't get in any trouble for sub-reviewing. That's relief to know. Although, I am wondering:* Do guidelines + code of conduct mean nothing? Why are they in place if they won't be respected? Based on the responses, ignoring them seems not too uncommon.* Isn't signing your name under a ghost-written review without crediting the ghostwriter a form of plagiarism? Wouldn't a student be reprimanded for plagiarism if they did this in a class? How is this different? Am I the only one who believes this still seems unethical?.
Imo a silent banger by Meta - generalizing diffusion and flow matching into transition matching which can be used in a unified causal generation process..
I've seen some flavor of questions here about whether they should do a PhD to join a research lab. I have a slightly different question. I did a non-CS PhD almost a decade ago, failed to get a faculty position after a bunch of postdocs and then meandered through FANG jobs, first in DS and then in MLE. I did some applied research in my last job, but more stats heavy than ML. But through a bunch of layoffs and restructuring, currently I am in a more traditional MLE role, think recommendation systems, A/B tests, move metrics...But at my heart, I still want to do research. I've dabbled with writing a single author paper in on the top ML conferences in my own time, but its kinda hard, with job, family etc.. Even if I do manage to pull it off, will the one off Neurips paper (lets say) help me get an entry card to a more research-y ML job, like a Research Scientist/ Research Engineer in a ML lab? I am competing with ML PhDs with multiple papers, networks etc.I also think that I don't have a lot of time, most of my friends have moved on to management after a decade of IC roles, and thats sort of the traditional path. But part of me is still holding on and wants to give it a shot and see if I can break into research this late, without an ML PhD. I know I will be much more fulfilled as a research scientist, compared to a regular SWE/M job,. I am currently trying to use my weekends and nights to write a single author paper to submit to one of the top conferences. Worst case I get rejected.Some thoughts in my mind:  (1) I have also thought of writing workshop papers, which are easier to get accepted, but I doubt they have a similar value in the RS job market.  (2) Research Engineer will likely be easier than Research Scientist. But how should I strategize for this?I'd be grateful if I get thoughts on how I should strategize a move. Feel free to also tell me its impossible, and I should cut my losses and move on..
  Has anyone here deployed models on **Firebase** or **Vertex AI**? I'm looking for the best practice for a clean and cohesive deployment (we have real-time data, and I need to design a continuous retraining pipeline; in essence, the inferences will be used to update a dashboard)..
I'm trying to compute the top-k tokens yielding the highest attention scores with inference frameworks such as vLLM or the plain HuggingFace transformers. The models I'm using are not big in terms of parameters (max 7B) but huge in terms of context windows (up to 1M tokens, and I'm using all of it). However, I face two problems:1. When using vLLM, I cannot access the attention scores in any way. Am I missing something or is the feature not yet implemented?  2. When using transformers, I need to use flash\_attention\_2 otherwise the GPU budget skyrockets to 400+ GBs when using large inputs (i have a machine with 8 A100 for a total of 320GB of VRAM). However, when using flash\_attention\_2 the output attention scores are all None, and the only way to solve this seems to use an eager attention implementation, which makes it unfeasible in terms of GPU requirements.Is someone facing a similar problem? How do you compute the attention scores for such large inputs?.
I am excited to share our recent work, DreamPRM, a multi-modal LLM reasoning method that ranks first currently on the MathVista leaderboard.https://preview.redd.it/54v78gz7zaaf1.png?width=1348&format=png&auto=webp&s=0084aef7727d9f02c129d8414582018fb09eedb5https://preview.redd.it/u0c02on9zaaf1.jpg?width=1374&format=pjpg&auto=webp&s=42ab8761dcc972ed89a999dac503c9dc35e65e18  Reasoning has substantially improved the performance of large language models (LLMs) on complicated tasks. Central to the current reasoning studies, Process Reward Models (PRMs) offer a fine-grained evaluation of intermediate reasoning steps and guide the reasoning process. However, extending PRMs to multimodal large language models (MLLMs) introduces challenges. Since multimodal reasoning covers a wider range of tasks compared to text-only scenarios, the resulting distribution shift from the training to testing sets is more severe, leading to greater generalization difficulty. Training a reliable multimodal PRM, therefore, demands large and diverse datasets to ensure sufficient coverage. However, current multimodal reasoning datasets suffer from a marked quality imbalance, which degrades PRM performance and highlights the need for an effective data selection strategy. To address the issues, we introduce DreamPRM, a domain-reweighted training framework for multimodal PRMs which employs bi-level optimization. In the lower-level optimization, DreamPRM performs fine-tuning on multiple datasets with domain weights, allowing the PRM to prioritize high-quality reasoning signals and alleviating the impact of dataset quality imbalance. In the upper-level optimization, the PRM is evaluated on a separate meta-learning dataset; this feedback updates the domain weights through an aggregation loss function, thereby improving the generalization capability of trained PRM. Extensive experiments on multiple multimodal reasoning benchmarks covering both mathematical and general reasoning show that test-time scaling with DreamPRM consistently improves the performance of state-of-the-art MLLMs. Further comparisons reveal that DreamPRM‚Äôs domain-reweighting strategy surpasses other data selection methods and yields higher accuracy gains than existing test-time scaling approaches.Paper: [https://arxiv.org/abs/2505.20241](https://arxiv.org/abs/2505.20241)Code: [https://github.com/coder-qicao/DreamPRM](https://github.com/coder-qicao/DreamPRM).
TL;DR: our AB-MCTS lets multiple frontier models work together at inference time, outperforming each model running alone on the ARC-AGI-2 benchmark.Our new inference-time scaling algorithm enables collective intelligence for AI by allowing multiple frontier models (like Gemini 2.5 Pro, o4-mini, DeepSeek-R1-0528) to cooperate.Inspired by the power of human collective intelligence, where the greatest achievements arise from the collaboration of diverse minds, we believe the same principle applies to AI. Individual frontier models like ChatGPT, Gemini, and DeepSeek are remarkably advanced, each possessing unique strengths and biases stemming from their training, which we view as valuable resources for collective problem-solving.AB-MCTS (Adaptive Branching Monte Carlo Tree Search) harnesses these individualities, allowing multiple models to cooperate and engage in effective trial-and-error, solving challenging problems for any single AI. Our initial results on the ARC-AGI-2 benchmark are promising, with AB-MCTS combining o4-mini + Gemini-2.5-Pro + R1-0528, current frontier AI models, significantly outperforming individual models by a substantial margin.This research builds on our 2024 work on evolutionary model merging, shifting focus from ‚Äúmixing to create‚Äù to ‚Äúmixing to use‚Äù existing, powerful AIs. At Sakana AI, we remain committed to pioneering novel AI systems by applying nature-inspired principles such as evolution and collective intelligence. We believe this work represents a step toward a future where AI systems collaboratively tackle complex challenges, much like a team of human experts, unlocking new problem-solving capabilities and moving beyond single-model limitations.Blog: https://sakana.ai/ab-mctsPaper: https://arxiv.org/abs/2503.04412Algorithm: https://github.com/SakanaAI/treequestARC-AGI Experiments: https://github.com/SakanaAI/ab-mcts-arc2If you have any questions, please ask them below or feel free to get in touch, any discussion is more than welcome :).
LLMs are getting better quickly. It seems like every time a new release comes out, they have moved faster than I anticipated. Are they great at abstract code, integrating systems, etc? Not yet. But I do find that they are excellent at data processing tasks and machine learning code, especially for someone who knows and understands those concepts and is able to understand when the LLM has given a wrong or inefficient answer.I think that one day, LLMs will be good enough to perform as well as a ML model that was designed using traditional processes. For example, I had to create a model that predicted call outcomes in a call center. It took me months to get the data exactly like I needed it from the system and identify the best transformation, combinations of features, and model architecture to optimize the performance.I wonder how soon I'll be able to feed 50k records to an LLM, and tell it look at these records and teach yourself how to predict X. Then I'll give you 10k records and I want to see how accurate your predictions are and it will perform as well or better than the model I spent months working on. Again I have no doubt that we'll get to this point some day, I'm just wondering if you all think that's gonna happen in 2 years or 20. Or 50? .
**For Job Postings** please use this template>Hiring: \[Location\], Salary:\[\], \[Remote | Relocation\], \[Full Time | Contract | Part Time\]    and \[Brief overview, what you're looking for\]**For Those looking for jobs** please use this template>Want to be Hired: \[Location\], Salary Expectation:\[\], \[Remote | Relocation\], \[Full Time | Contract | Part Time\]  Resume: \[Link to resume\] and \[Brief overview, what you're looking for\]&#x200B;Please remember that this community is geared towards those with experience..
Hi everyone,I'm working on a deep learning project involving emotion recognition from Hinglish (code-mixed Hindi-English) speech.I‚Äôve found plenty of datasets for English (like RAVDESS, IEMOCAP) and some for Hindi (MUCS, OpenSLR), but I‚Äôm having trouble locating datasets that contain Hinglish speech, especially with emotion labels.Do any of you know of:Hinglish speech datasets (code-switched Hindi-English)Emotion-labeled Hinglish audioOpen-source or research datasets that allow this type of trainingIf there are no public datasets, I‚Äôd also appreciate tips on how to create or augment one from scratch.And also how can I increase it accuracy.Thanks in advance!.
Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!Thread will stay alive until next one so keep posting after the date in the title.Thanks to everyone for answering questions in the previous thread!.
https://preview.redd.it/r1w6xzdnbaaf1.png?width=1492&format=png&auto=webp&s=5ab883dcc781312bb6014b9daf1d9295dfc66030Hey everyone!I'm currently using¬†smartcrop.py (github.com/smartcrop/smartcrop.py)¬†for image cropping in Python, but it's pretty basic. It only detects edges and color gradients, not actual objects.For example, if I have a photo with a coffee cup, I want it to recognize the cup as the main subject and crop around it. But smartcrop just finds areas with most edges/contrast, which often misses the actual focal point.**Looking for:*** Python library that uses AI/ML for object-aware cropping* Can identify main subjects (people, objects, etc.)* More modern than just edge detectionAny recommendations for libraries that actually understand what's in the image?Thanks!.
I‚Äôve been thinking about how opaque and inconsistent peer reviews can be, especially in top ML conferences. What if we made it a requirement for reviewers to explicitly state the conditions under which they would raise their scores? For example, ‚ÄúIf the authors add experiments on XYZ‚Äù or ‚ÄúIf the theoretical claim is proven under ABC setup.‚ÄùThen, area chairs (ACs) could judge whether those conditions were reasonably met in the rebuttal and updated submission, rather than leaving it entirely to the whims of reviewers who may not revisit the paper properly.Honestly, I suspect many reviewers don‚Äôt even know what exactly would change their mind.As an added bonus, ACs could also provide a first-pass summary of the reviews and state what conditions they themselves would consider sufficient for recommending acceptance.What do you think? Could this improve transparency and accountability in the review process?.
Excited to share our new work, ""Supernova Event Dataset: Interpreting Large Language Models' Personality through Critical Event Analysis"" accepted at the Actionable Interpretability Workshop @ ICML 2025.Introducing the Supernova Event DatasetWe present a new benchmark built from real-world Wikipedia articles, including biographies, historical milestones, global news, and scientific discoveries (including articles from Google Deep Research). This dataset introduces a novel task: critical event analysis for interpreting the behavioral pattern, or ‚Äúpersonality‚Äù of LLMs.Rather than looking inside the model (activations, traces), we ask a separate LLM to judge what events are most critical, and use this external perspective to decode the model‚Äôs values and reasoning traits.Some early insights:Orca2 tends to prioritize emotional and interpersonal events.Phi-4 and Qwen2.5 focus on strategic milestones.In scientific discovery, o3 highlights causal breakthroughs, Gemini 2.5 Pro favors methodological innovations, and Claude Sonnet 3.7 emphasizes conceptual clarity.While these are early findings (still without human evaluation), the diversity in critical event patterns is striking. We believe assigning LLMs ""personalities"" could make them more relatable and trustworthy, enabling smoother human-AI collaboration, especially in domains like scientific discovery.Paper: [arxiv.org/abs/2506.12189](https://l.facebook.com/l.php?u=http%3A%2F%2Farxiv.org%2Fabs%2F2506.12189%3Ffbclid%3DIwZXh0bgNhZW0CMTAAYnJpZBExaFJHbUZIbDN4cEJKZ096SAEeYgHhUQQ5lNulNDLnSeulM64ECl3ls5tJCNkMC1_EPhqEHk1uqWYnEdBsu7g_aem_fW6BPSFPpkV4xELlQOiODA&h=AT0m3E04nqeA-MMtev7Ouz9OW3PeW5A_y6V9zj9dqy3WrwMXLOeTXVO9EzTXtMey6tWCwh1vUB5rS0lMPeoFEdjHj6BecO9zq9__xYIrQwGC6nhRT8BEE50RlAm9OuXWhk_HCtHu&__tn__=-UK-R&c[0]=AT1lFRYu12xNSpO0-IlQ0tpDLaqpnTlHs0ipZFi4QqbVd_3LDc0Vnyr8uzMo6U8UhtdIXR8G5rKpdZaAbCtojueyXuX18q9jlCTh0kB1YA49AGDcMIGSWkV-pn1HLQWJK6QSFLLZ3aNz9jS3Cq1Q_927wYGbHSSdspcG77eNZ7dlk1qKhBee)Twitter: [https://x.com/Pranav\_AL/status/1939681069554655382](https://x.com/Pranav_AL/status/1939681069554655382)Webpage: [http://supernova-event.ai](https://l.facebook.com/l.php?u=http%3A%2F%2Fsupernova-event.ai%2F%3Ffbclid%3DIwZXh0bgNhZW0CMTAAYnJpZBExaFJHbUZIbDN4cEJKZ096SAEeZQWpZz3uLtupNGIkmvzqpDbbG9u83w7Tv9ifY4Rjvct6zdjy8E4yg6NNDTM_aem_SgZlEI9ACWixaHK8TQWBPw&h=AT3tIyMocFJs9OJneCmpLWwlEjH3FE0RtckM5RYggzkKKuFMG5AIIScnpzGDhh7YqxBOSxpleqKt0hXYWIiiG_t3RoKtnoI1vlHkHUCsMhHlTmKcoQvqBSUnk8rLDci3doz0NpFV&__tn__=-UK-R&c[0]=AT1lFRYu12xNSpO0-IlQ0tpDLaqpnTlHs0ipZFi4QqbVd_3LDc0Vnyr8uzMo6U8UhtdIXR8G5rKpdZaAbCtojueyXuX18q9jlCTh0kB1YA49AGDcMIGSWkV-pn1HLQWJK6QSFLLZ3aNz9jS3Cq1Q_927wYGbHSSdspcG77eNZ7dlk1qKhBee)Demo: [supernova-event.ai/#your-story](https://l.facebook.com/l.php?u=http%3A%2F%2Fsupernova-event.ai%2F%3Ffbclid%3DIwZXh0bgNhZW0CMTAAYnJpZBExaFJHbUZIbDN4cEJKZ096SAEeyxn4V9DBpYEvuLM2LwLuOOn-4Ewox9O267lf8zV1R8tbbLTm3vS1lw0zzqM_aem_9qDTC7HNLTFGRKq_iFqgBg%23your-story&h=AT1SxDBmgr_G-a-c-D9g4JyMlgs5bD-liDDwNRTZlIP_5CBvyl8meA3pwaNKABRXLqvHcemnEJv-sWcf3oan2b3FSWZ7H_yD3Y8mdhI5Ze6mVNULKBXQTdFqgrLNUynlNQjPjP2_&__tn__=-UK-R&c[0]=AT1lFRYu12xNSpO0-IlQ0tpDLaqpnTlHs0ipZFi4QqbVd_3LDc0Vnyr8uzMo6U8UhtdIXR8G5rKpdZaAbCtojueyXuX18q9jlCTh0kB1YA49AGDcMIGSWkV-pn1HLQWJK6QSFLLZ3aNz9jS3Cq1Q_927wYGbHSSdspcG77eNZ7dlk1qKhBee)Code: [https://github.com/pranavAL/Supernova-Event-Dataset](https://l.facebook.com/l.php?u=https%3A%2F%2Fgithub.com%2FpranavAL%2FSupernova-Event-Dataset%3Ffbclid%3DIwZXh0bgNhZW0CMTAAYnJpZBExaFJHbUZIbDN4cEJKZ096SAEeyxn4V9DBpYEvuLM2LwLuOOn-4Ewox9O267lf8zV1R8tbbLTm3vS1lw0zzqM_aem_9qDTC7HNLTFGRKq_iFqgBg&h=AT3sNwJZLhvA9OG4GbkPvPxvAXxtZr9drQAj1Rp-4MCOHOOVbjH1epznhz08JAKypffQNwntIbz6TWzMTDmKVgXvDw7y6Yrg6Bcijqgxco34C_R4ivMwgS83oW5i2QnMFFQmQuVt&__tn__=-UK-R&c[0]=AT1lFRYu12xNSpO0-IlQ0tpDLaqpnTlHs0ipZFi4QqbVd_3LDc0Vnyr8uzMo6U8UhtdIXR8G5rKpdZaAbCtojueyXuX18q9jlCTh0kB1YA49AGDcMIGSWkV-pn1HLQWJK6QSFLLZ3aNz9jS3Cq1Q_927wYGbHSSdspcG77eNZ7dlk1qKhBee)We're working toward scaling this into a real-world product, and we're currently seeking the right resources and support to take it further. If you're interested in what we're building and see potential for impact, we‚Äôd love to hear from you. Reach us at [hello@supernova-event.ai](mailto:hello@supernova-event.ai) ; we're open to conversations, collaborations, and any form of support that can help push this idea forward.https://preview.redd.it/uugbpxw075af1.png?width=1200&format=png&auto=webp&s=ccbde6f1ace6140ff2ca838ffb0e60522759dc70.
This review gave me 1.5 in ACL and calls GRPO Generalized Reward Preference Optimization, which is what ChatGPT thinks GRPO is... It also says my work is the first one to use GRPO in my domain while it is not (and we talk about this in the introduction) and says we are missing some specific evaluations, which are present in the appendix and says we did not justify a claim well enough, which is very well known in my domain but when asking ChatGPT about it it says it does not know about it...It feels like the reviewer just wanted to give me a bad review and asked an LLM to write a poor review. He clearly did not even check the output because literally everyone knows GRPO stands for Group Relative Policy Optimization...Other than reply to the reviewer while pretending I did not know he/she used ChatGPT, what else can I do? My other reviews were both 3, so I really want to get rid of this review if possible....
Firstly, total disclaimer. About 4 months ago, I knew very little about LLMs, so I am one of those people who went down the rabbit hole and started chatting with AI. But, I'm a chap who does a lot of pattern recognition in the way I work (I can write music for orchestras without reading it) so just sort of tugged on those pattern strings and I think I've found something that's pretty effective (well it has been for me anyway).Long story short, I noticed that all LLMs seem to have their training data steeped in Greek Mythology. So I decided to see if you could use that shared knowledge as compression. Add into that syntax that all LLMs understand (:: for clear key-value assignments, ‚Üí for causality and progression, etc) and I've combined these two layers to create a DSL that's more token-efficient but also richer and more logically sound.This isn't a library you need to install; it's just a spec. Any LLM I've tested it on can understand it out of the box. I've documented everything (the full syntax, semantics, philosophy, and benchmarks) on GitHub.I'm sharing this because I think it's a genuinely useful technique, and I'd love to get your feedback to help improve it. Or even someone tell me it already exists and I'll use the proper version!Link to the repo:¬†[https://github.com/elevanaltd/octave](https://github.com/elevanaltd/octave).
Hey everyone,I‚Äôve been meaning to dive into NVIDIA PTX for a while, and I learn best by doing‚Äîso I decided to hand-write PTX kernels for an \*\*inference-only\*\* version of Andrej Karpathy‚Äôs \[LLM.c\](https://github.com/karpathy/llama.cpp) project. To my surprise, not only did everything actually work, but I also saw about a \*\*10% performance improvement\*\* in inference compared to the equivalent CUDA implementation (or at least, that‚Äôs what my benchmarks showed).You can check out the code here:üëâ \[https://github.com/theunnecessarythings/llm-ptx\](https://github.com/theunnecessarythings/llm-ptx)Along the way, I documented my entire experience in a multi-part blog series, including line-by-line explanations of how I translated CUDA into PTX:1. \*\*Part I: Introduction & Residual Kernel\*\*\[https://sreeraj.in/blog/llm-ptx-01\](https://sreeraj.in/blog/llm-ptx-01)2. \*\*Part II: The GELU Kernel\*\*\[https://sreeraj.in/blog/llm-ptx-02\](https://sreeraj.in/blog/llm-ptx-02)3. \*\*Part III: The Encoder Kernel\*\*\[https://sreeraj.in/blog/llm-ptx-03\](https://sreeraj.in/blog/llm-ptx-03)4. \*\*Part IV: The LayerNorm Kernel\*\*\[https://sreeraj.in/blog/llm-ptx-04\](https://sreeraj.in/blog/llm-ptx-04)5. \*\*Part V: The Softmax Kernel\*\*\[https://sreeraj.in/blog/llm-ptx-05\](https://sreeraj.in/blog/llm-ptx-05)6. \*\*Part VI: The Attention Kernel\*\*\[https://sreeraj.in/blog/llm-ptx-06\](https://sreeraj.in/blog/llm-ptx-06)7. \*\*Part VII: The MatMul Kernel & Performance Results\*\*\[https://sreeraj.in/blog/llm-ptx-07\](https://sreeraj.in/blog/llm-ptx-07)\---\*\*What‚Äôs Next?\*\*This is my first time writing PTX, so there may still be bugs or missed optimization opportunities. I‚Äôd love feedback or fixes from anyone who‚Äôs more experienced with low-level GPU programming!\---\*\*Also posted on X:\*\*\[https://x.com/notHumanIam/status/1939402092071780610\](https://x.com/notHumanIam/status/1939402092071780610)Looking forward to your thoughts and suggestions! üòÑ.
Hi guys, I'm sort of a noob at Computer Vision and I came across a project wherein I have to detect whether or not a person is looking at the screen through a live stream. Can someone please guide me on how to do that?The existing solutions I've seen all either use MediaPipe's FaceMesh (which seems to have been depreciated) or use complex deep learning models. I would like to avoid the deep learning CNN approach because that would make things very complicated for me atp. I will do that in the future, but for now, is there any way I can do this using only OpenCV and Mediapipe?  PS. Sorry for the wrong tag mods.
I got a review asking to compare my submission paper with more recent models. The models were not even out 3 months before the submission so by ACL rules I should not have to compare them with my model because it is contemporary.Nevertheless I have ran comparisons and my model is much much worse... Why? I'm using a model doing the same thing but 32x smaller, used almost 1/10 of the data they used, etc... I am severely resource constrained and cannot compete in terms of scale, but I still think that my paper makes an important contribution that if we were to match the other models scale we would get better results.What should I do? Should I report results that show other models are better and risk the reviewers lower their scores? I kinda just want to explain the authors that the scale is completely different and other factors make it a very unfair comparison, but they might just not care...I have a 2.5 average score and really wanted to try to raise it to make it at least into findings, but I honestly don't know how to defend against not having as many resources as top labs/unis....
I'm building a dataset for a knowledge extraction model and need to label structured data from thousands of live websites. Ideally, I'm looking for a tool that:\- Provides a Chrome extension to label live HTML elements on real websites\- Can open sites one by one in the browser from a task queue\- Saves each annotation along with a snapshot or DOM state of the page\- Supports exporting annotations for later review with screenshotsI‚Äôm considering building a custom tool for this, but would prefer to avoid that since it would distract from the core research. Does anyone know an existing tool that supports doing what Im doing?.
Hey r/cpp and r/MachineLearning!You may have guessed from the title, but why make one when we have TensorFlow, PyTorch that provide the simplicity of Python and the speeds of C and C++ ?   I say well why not.   1. *The Learning* - With AI boom taking over and people going crazy on vibe coding, ML and DS jobs are focusing on how deeply people understand the basics and internal working of what they are making. So while many tutorials focusing on API's, MCP's and what not, here I am peeling the layers (literal layers of a neural network) and the process taught me more than any tutorial could.2. *The Fun* - I love C++! Building this from scratch (even with procrastination detours üòÖ) was really exciting. (Who doesn't love crying over why the whole model isn't working only to know you subtracted the losses instead of adding. And of course the feeling of betrayal when you ask chatGPT to add comments to the code due to your laziness and it changes the code smirking while you notice it too late and then have had to debug the whole library searching where it went wrong) Also, it is never a bad idea (mostly) to know what happens behind the scenes of the code you are gonna write. And what better thing to understand the basics than implement them by yourself. (Though this may not be a good idea always considering my bad habit of delving too deep into small topics and going into a rabbit hole wholly different than what i was supposed to be doing).   ### Current Features:* Dense layers + activations (ReLU, SELU, Sigmoid)* SGD optimizer with momentum/LR scheduling* CSV/binary dataset handling (though the binary loader may need some fixes)* Batch training Where I got the idea ? Well I was supposed to start learning to code with PyTorch but then I thought how does this even work. I just looked at a small part of the documentation and thought let's try coding this and this led to me successfully spending about 2 weeks on this (with lots of procrastination in between). Will it be a good project ? I don't know. Did I enjoy it ? Damn well I did.     Well it's still not complete and may have a few bugs and I plan to keep it aside for now and improve it bit by bit later on. But I thought sharing this may encourage me somewhat and get my lazy self to do some work without procrastinating. You can check out the full source code and documentation on GitHub:https://github.com/CuriosityKilledTheCache/Deep-in-scratch_Maths_the_catchP.S : If you have any recommendations, do tell though it may be a passing reply comment for you, it may help me very much for correcting mistakes I may make again in the future..
**Abstract:**>Science progresses by iteratively advancing and correcting humanity's understanding of the world. In machine learning (ML) research, rapid advancements have led to an explosion of publications, but have also led to misleading, incorrect, flawed or perhaps even fraudulent studies being accepted and sometimes highlighted at ML conferences due to the fallibility of peer review. While such mistakes are understandable, ML conferences do not offer robust processes to help the field systematically correct when such errors are¬†made. This¬†position paper argues that ML conferences should establish a dedicated ""Refutations and Critiques"" (R & C) Track. This R & C Track would provide a high-profile, reputable platform to support vital research that critically challenges prior research, thereby fostering a dynamic self-correcting research ecosystem. We discuss key considerations including track design, review principles, potential pitfalls, and provide an illustrative example submission concerning a recent ICLR 2025 Oral. We conclude that ML conferences should create official, reputable mechanisms to help ML research self-correct.(I'm not affilated with any of the authors. But I believe this position paper deserves more visibility).
Hey all,I was having trouble finding a simple, self contained example of Fine-Tuning FLUX.1-dev with explanation of all the components, so I decided to create one. There were examples in HuggingFace diffusers [examples/dreambooth/train\_dreambooth\_lora\_flux.py](https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/train_dreambooth_lora_flux.py) (which didn't work out of the gate for me) and [AI-Toolkit](https://github.com/ostris/ai-toolkit) which worked well, but had way too many nested if-statements to fully see what was going on under the hood. I took inspiration from both, but cleaned up the code so it was easier to read and worked out of the gate.The code was written in a [Marimo Notebook](https://marimo.io/) which I'm enjoying lately for developing simple training scripts. Feel free to download the code here: [https://www.oxen.ai/ox/Fine-Tune-FLUX/file/main/train.py](https://www.oxen.ai/ox/Fine-Tune-FLUX/file/main/train.py)Or follow along with a blog version: [https://www.oxen.ai/blog/how-to-fine-tune-a-flux-1-dev-lora-with-code-step-by-step](https://www.oxen.ai/blog/how-to-fine-tune-a-flux-1-dev-lora-with-code-step-by-step)Hope you enjoy!.
Hi everyone,I'm working on neural network training, especially for tasks that involve time-series data or time-dependent phenomena. I'm trying to understand the common design patterns for such networks.My current understanding is that for time-dependent tasks, a neural network architecture might often be divided into two main parts:1. **Static Feature Extraction:** This part focuses on learning features from individual time steps (or samples) independently. Architectures like CNNs (Convolutional Neural Networks) or MLPs (Multi-Layer Perceptrons) could be used here to extract high-level semantic information from each individual snapshot of data.2. **Dynamic Feature Capture:** This part then processes the sequence of these extracted static features to understand their temporal evolution. Models such as Transformers or LSTMs (Long Short-Term Memory networks) would be suitable for learning these temporal dependencies.My rationale for this two-part approach is that it could offer better interpretability for problem analysis in the future. By separating these concerns, I believe it would be easier to use visualization techniques (like PCA, t-SNE, UMAP for the static features) or post-hoc explainability tools to determine if the issue lies in: \* the *identification of features* at each time step (static part), or \* the *understanding of how these features evolve over time* (dynamic part).Given this perspective, I'm curious to hear from the community: **Is it generally recommended to adopt such a modular architecture for training neural networks on tasks with high time-dependency? What are your thoughts, experiences, or alternative approaches?**Any insights or discussion would be greatly appreciated!.
I was building my own diffusion model walking myself through CompVis' StableDiffusion repo when I came upon this strange code when reading through the U-Net implementation:  [https://github.com/CompVis/stable-diffusion/blob/main/ldm/modules/diffusionmodules/model.py#L83](https://github.com/CompVis/stable-diffusion/blob/main/ldm/modules/diffusionmodules/model.py#L83)Specifically the implementation of Model on line 216.In the current implementation, each downsampling level appends two skip connections of shape (B, ch, H, W) from the ResBlocks, followed by a third skip from the downsampled output, which incorrectly has shape (B, ch, H//2, W//2). During upsampling, all three skips are concatenated in sequence without compensating for this resolution mismatch, as the upsampling layer is applied after all three ResNet blocks. This causes the first skip in each upsampling level to be at the wrong spatial resolution, breaking alignment with h during torch.cat. When I implemented my U-Net I had to changehs.append(self.down\[i\_level\].downsample(hs\[-1\])) (line 340)to downsample AFTER caching it in hs, the skip-connection cache..
An alternative approach to EvilModel is packing an entire program‚Äôs code into a neural network by intentionally exploiting the overfitting phenomenon. [I developed a prototype](https://github.com/piotrmaciejbednarski/lstm-memorizer) using PyTorch and an LSTM network, which is intensively trained on a single source file until it fully memorizes its contents. Prolonged training turns the network‚Äôs weights into a data container that can later be reconstructed.The effectiveness of this technique was confirmed by generating code identical to the original, verified through SHA-256 checksum comparisons. Similar results can also be achieved using other models, such as GRU or Decoder-Only Transformers, showcasing the flexibility of this approach.The advantage of this type of packer lies in the absence of typical behavioral patterns that could be recognized by traditional antivirus systems. Instead of conventional encryption and decryption operations, the ‚Äúunpacking‚Äù process occurs as part of the neural network‚Äôs normal inference.[https://bednarskiwsieci.pl/en/blog/lstm-or-transformer-as-malware-packer/](https://bednarskiwsieci.pl/en/blog/lstm-or-transformer-as-malware-packer/).
I trained an AI agent to play *X-Men vs Street Fighter* using reinforcement learning, leveraging the **Stable-Retro** framework (built on top of Gym Retro). The agent interacts with the game through frame observations and discrete action spaces mapped to the arcade controls.The training process involved reward shaping based on health bars, damage dealt, and round wins. The environment was wrapped with preprocessing (grayscale, resizing, frame stacking) and curriculum logic to improve generalization across multiple characters and enemy types.The video shows the progression from random movement to more competent fighting strategies, including corner traps and defensive spacing. The learning curve is steep due to the complexity of the fighting game mechanics, but the agent starts to show patterns similar to human play.Frameworks used: PyTorch, Stable-Baselines3, OpenCV, and a modified Gym Retro environment with custom reward functions and action discretization.I'd love to hear feedback from others working on RL in dynamic multi-agent environments or applying deep RL to retro/arcade-style games. Happy to share code or discuss implementation details![https://github.com/paulo101977/AI-X-men-Vs-Street-Fighter-Trainning](https://github.com/paulo101977/AI-X-men-Vs-Street-Fighter-Trainning).
Hello everyone, this is my first time posting here, and I wanted to get some opinions on the phd position I applied to.So I am studying ml in France and I have a chance to do a PhD in the topic of LLM knowledge locating and editing. One paper that talks about this is the ROME (Rank One Model Editting -¬†[https://arxiv.org/abs/2202.05262](https://arxiv.org/abs/2202.05262))Basically, I would work on the internals of LLMs, analysing where exactly the knowledge for a certain fact is stored, and how can it be edited out. So messing around the directly with the components such as the attention and MLP weights.For me personally, I like the idea of going inside the LLMs, instead of just inferencing/training and using them as some black boxes.And I suppose that this would qualify me for jobs of actually creating LLMs (I do not expect to end up in OpenAI) but also make me more qualified for standard LLM usage jobs.Any opinion or comment would be appriciated!.
Hi all,I‚Äôm using Tesseract OCR to extract text from scanned financial documents like payslips and tax returns. The raw output is messy, and I need to clean it up and pull key fields like YTD income, net pay, and tables.What post-processing tools or Python libraries can help:* Extract key-value fields* Parse tables* Match labels to values* Clean and structure OCR outputPrefer offline tools (for privacy), but open to anything that works well..
I'm kind of wondering about these AI readiness assessments everyone's talking about. Like, you see vendors and consultants pushing them, and honestly, I'm a bit skeptical. I can't help but feel it might just be a lot of buzzwords without real substance.      Has anyone actually gone through one of these with a third party, maybe a consultant or a specific vendor, was it actually worth the time and money you put into it and did you get genuinely practical insights that helped your business move forward, or was it just a fancy report that basically says 'you need more AI' without telling you how?    I'm really curious to hear real experiences here, good or bad, before potentially diving into something that might just be another passing trend in the tech world. What did you learn, and what was the actual outcome?.
Posting anonymously for this one. I know questions like these get posted quite often, but I wanted to offer a bit of context about my own situation and what I'm into.I'm currently a rising college sophomore working in Sergey Levine's lab (RL & robotics) at Berkeley, and I have to decide whether I want to pursue a standard industry internship (e.g. SWE) for the 2026 summer or continue doing research in the lab. I really like research work, easily the most enjoyable ""work"" I've done in my life, but I can't deny that money is still a factor (esp. due to particular family reasons). I see three sort of options down the line from here (listed with their pros and consA) continue doing research in my time in undergrad, and shoot a difficult shot towards getting into a reputable PhD program* Pros:   * very streamlined process to become an industry research scientist given that I go to a good enough program & work hard enough   * \^\^ this is the most optimal job option for me: 10/10 job, the best I could ever want. I love research man   * researchers generally seem like the most sufferable group out of most tech archetypes (seen way too many elon-musk wannabes in normal SWE)* Cons:   * 5-6 years of a PhD: not that it's going to be unenjoyable, but it delays my life ""progress"" a lot   * getting into top ML PhD programs is really tough nowadays. I'm lucky to have started sort of early (working on my first first-author pub over this summer) but I know people with great publication history (probably better than I'll earn) that didn't get admitted anywhere   * \^\^ it seems as though if I don't get into a PhD program, all the research I would have published would be a sunk cost (not useful for much besides just.. ML research)   * comp: is it much better than normal SWE or MLE? though I love the work a lot, I would hope that it's just a *biiit* better to justify the extra 6 years I put in for a PhD   * if ML hype & investment dies out, I'll be on the forefront of getting laid off, esp if RL doesn't find a way to scale soon enoughB) continue doing research, but balance it out with some SWE or similar experience and go for an MLE or research engineer type of role* Pros:   * immediately high comp out just out of my degree if I can land one of these roles, without needing to spend all that time on a degree   * correct me if I'm wrong, but RE and some parts of MLE aren't that far off from research scientist work, esp. if working with researchers at a frontier lab   * seems to be less workload, better WLB?   * seems to be more stable (easier transition to SWE) if ML hype dies out* Cons:   * less interesting work. not that I hate it, but it's like an 8/10 compared to the 10/10 work that I would consider to be RS   * I'm unsure if my publications & research history would help at all for these roles. from what I've heard, research and industry experience are almost orthogonal and they simply don't care about publications (please correct me if I'm wrong!)   * don't own the intellectual rights to my own work :(C) research is useless, just do SWE, ML research is a hellhole* \^\^ this is more so a last resort rather than something I would ever want to do, but if you have any reason that this is a good option, please do tell me why.
It shows ""There are currently no¬†active venues."" I am trying to complete the NIPS review at the last minute. Will they extend the deadline? .
**UPDATE**: A first author of the SAMformer paper [commented below](https://www.reddit.com/r/MachineLearning/comments/1ln8wu8/comment/n3lce9d/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button) explaining their rationale for ommitting the linear models in their benchmark. In short, when running their multi-seed evaluations they found that TSMixer was the most competitive non-transformer benchmark and didn't see the value in also including the worse-performing linear models. In their evaluations the linear models were comparable to FedFormer / AutoFormer. Given this extra context, the title of the post still rings true but the thrust of the original post is simply misinformed. Credit should be given to the SAMformer authors for establishing more useful benchmarks (reporting results averaged across multiple seeds) than earlier papers in the field.\-----------------------------------------------------------------------------------------------------------For those not in the time-series forecasting space, it has seen some interesting developments in the last few years as researchers have tried to translate the success of transformer-based models in the language domain, to the forecasting domain. There was incremental progress in long-term timeseries forecasting with the likes of [Informer](https://arxiv.org/abs/2012.07436), [Autoformer](https://arxiv.org/abs/2106.13008), and [Fedformer](https://arxiv.org/pdf/2201.12740), among others, however the 2022 [paper ](https://arxiv.org/abs/2205.13504)""Are Transformers Effective for Time Series Forecasting?"" (Zeng et al.) called into question how much progress these models had actually made.Zeng et al. introduced three self-proclaimed ""embarassingly simple"" linear models -- each of which are variations on a **single dense layer** mapping the input values to the output values -- which outperformed all of the above state-of-the-art transformer models on their benchmarks (see the image below for a subset of results):[Linear and Transformers MSE Benchmarks](https://preview.redd.it/l504gwc2ft9f1.png?width=998&format=png&auto=webp&s=b231af865a3e1ea1ee48ad64dd77f6cad4f011eb)This brings us to the paper [SAMformer](https://arxiv.org/abs/2402.10198) which applies a ""sharpness-aware minimisation"" approach to training a simplified version of the vanilla transformer encoder. This works very well, generally outperforming the aforementioned transformer models, as well as competetive non-transformer state-of-the-art models (TSMixer and PatchTST), on all the same benchmarks. Notably absent in the benchmarks however, are the linear models from Zeng et al. You can see the results from the SAMformer paper below (all results are **MSE**):[SAMFormer MSE Benchmarks](https://preview.redd.it/gbraaexeht9f1.png?width=169&format=png&auto=webp&s=bb4f1dff2015ff492e1bd07b58beb546f9b24a5f)On Electricity, Exchange, and Weather the simple linear models outperform SAMformer for all horizons, and it is only on the Traffic dataset where SAMformer achieves lower MSE. The omission of the linear models in the final benchmarks is doubly surprising given the SAMformer authors specifically mention the results from Zeng et al. in their introduction:""\[Zeng et al.\] recently found that linear networks can be on par or better than transformers for the forecasting task, questioning their practical utility. This curious finding serves as a starting point for our work.""To be clear, I think the ideas introduced in the SAMformer paper are valuable and I think it would be fair to classify SAMformer as a ""state-of-the-art"" model. However, I am curious of the rationale for excluding the linear models in the benchmarks given they were originally introduced to call into question the effectiveness of transformers in the time-series forecasting domain.Tl;dr: Always put your skeptical glasses on when reviewing benchmarks as there may be some highly competetive models omitted from the analysis..
Hi all,  Can anyone tell me how I should calculate inference time (case/sec) for medical images? SegMamba paper reports inference time as case/sec.  I have 2 queries in this case.  First, should inference time (case/sec) include the time of every operation after model predictions?  Secondly, because of sliding window inference, it is highly likely that the inference time for each case might be higher. What is the right way?.
Excited to share Arch-Router, our research and model for LLM routing. Routing to the right LLM is still an elusive problem, riddled with nuance and blindspots. For example:‚ÄúEmbedding-based‚Äù (or simple intent-classifier) routers sound good on paper‚Äîlabel each prompt via embeddings as ‚Äúsupport,‚Äù ‚ÄúSQL,‚Äù ‚Äúmath,‚Äù then hand it to the matching model‚Äîbut real chats don‚Äôt stay in their lanes. Users bounce between topics, task boundaries blur, and any new feature means retraining the classifier. The result is brittle routing that can‚Äôt keep up with multi-turn conversations or fast-moving product scopes.Performance-based routers swing the other way, picking models by benchmark or cost curves. They rack up points on MMLU or MT-Bench yet miss the human tests that matter in production: ‚ÄúWill Legal accept this clause?‚Äù ‚ÄúDoes our support tone still feel right?‚Äù Because these decisions are subjective and domain-specific, benchmark-driven black-box routers often send the wrong model when it counts.**Arch-Router skips both pitfalls by routing on** ***preferences you write in plain language.*** Drop rules like ‚Äúcontract clauses ‚Üí GPT-4o‚Äù or ‚Äúquick travel tips ‚Üí Gemini-Flash,‚Äù and our 1.5B auto-regressive router model maps prompt along with the context to your routing policies‚Äîno retraining, no sprawling rules that are encoded in if/else statements. Co-designed with Twilio and Atlassian, it adapts to intent drift, lets you swap in new models with a one-liner, and keeps routing logic in sync with the way you actually judge quality.**Specs*** **Tiny footprint** ‚Äì 1.5 B params ‚Üí runs on one modern GPU (or CPU while you play).* **Plug-n-play** ‚Äì points at any mix of LLM endpoints; adding models needs *zero* retraining.* **SOTA query-to-policy matching** ‚Äì beats bigger closed models on conversational datasets.* **Cost / latency smart** ‚Äì push heavy stuff to premium models, everyday queries to the fast ones.Exclusively available in Arch (the AI-native proxy for agents): [https://github.com/katanemo/archgw](https://github.com/katanemo/archgw)  üîó Model + code: [https://huggingface.co/katanemo/Arch-Router-1.5B](https://huggingface.co/katanemo/Arch-Router-1.5B)  üìÑ Paper / longer read: [https://arxiv.org/abs/2506.16655](https://arxiv.org/abs/2506.16655).
**Abstract**[**https://zenodo.org/records/15769766**](https://zenodo.org/records/15769766)Modern AI models, in particular Large Language Models (LLMs) and Computer Vision models, operate in fundamentally distinct data domains: text and pixels. The interaction between these models requires expensive and complex translation and embedding processes. This work introduces a new paradigm,¬†¬†**Chromatic Language Models (CLMs)**¬†, designed to eliminate this discontinuity. Building on the principles of visual semantic coding established in¬†¬†**Usai ColorZip**¬†¬†(Usai, 2025a) and validated by the¬†¬†**Usai ChromoChess**¬†application ¬†(Usai, 2025b), CLMs are language models that operate natively on a chromatic domain. We propose an encoder-decoder architecture in which an AI agent learns to ""read"" and ""write"" complex information directly as images, treating pixels as semantic tokens. This approach not only unifies language and vision, but creates an intrinsically compressed, secure, and efficient form of AI-native communication, paving the way for a new generation of multimodal intelligent agents.**1. Introduction**The evolution of artificial intelligence is characterized by increasing specialization. On the one hand, Large Language Models (LLMs) have demonstrated an unprecedented ability to understand and generate human language. On the other hand, computer vision models, such as Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs), excel at interpreting visual data. However, a fundamental ""modal gap"" separates these two worlds. An LLM does not ""see"" images and a ViT does not ""read"" text; both rely on intermediate embedding layers to translate information from one domain to the other.This paper addresses a radical question: what if we could close this gap by transforming language itself into a natively visual format? Instead of teaching a model to translate between text and pixels, could we create a model that ""thinks"" directly in pixels?We propose the architecture of¬†¬†**Chromatic Language Models (CLM)**¬†, intelligent agents that use a chromatic representation of language for each stage of their cognitive process: input, reasoning, and output. This proposal builds directly on the technological and conceptual foundations of our previous work, which demonstrated the feasibility of such a representation.**2. Fundamental Works and Context**Our proposal is not born in a vacuum, but is the natural evolution of two previous researches that established the feasibility of visual semantic coding.**2.1. Usai ColorZip: Semantic Text Encoding**  In our work ""Usai ColorZip: A Hybrid System for Semantic Text Encoding and Compression via HTML Colors"" (Usai, 2025a), we introduced a lossless system for mapping lexical units (words) to unique color codes. We demonstrated that this transformation is not only an act of encoding, but also an effective data compression mechanism when combined with lossless image formats such as PNG. The key to the system is its hybrid architecture, capable of handling both a large dictionary of known words and any unknown word via a color escape protocol.¬†¬†**Usai ColorZip created the ""vocabulary"" and ""syntax"" of this new language.****2.2. Usai ChromoChess: Proof of Concept in a Complex Domain**  Later, in ""Usai ChromoChess: Visual Representation and Compression of Chess Games"" (Usai, 2025b), we applied this philosophy to a formal and complex domain. By transforming chess games from PGN notation to 8x8 pixel movies, we demonstrated that a sequence of logical states can be represented as a visual data stream, compact and ideal for analysis by vision models.¬†¬†**Usai ChromoChess provided proof that entire logical-temporal processes can be efficiently encoded in this chromatic language.**These two works constitute the necessary prerequisite for the next step: no longer just encoding and decoding data, but creating an intelligence that¬†¬†*uses*¬†¬†this language as its primary means of communication and reasoning.**3. Architecture of the Chromatic Language Model (CLM)**A CLM is an AI model designed for an end-to-end communication cycle in the color domain. Its architecture is based on an encoder-decoder model.**3.1. The Principle: Visual Tokenization**  The fundamental unit of a CLM is not a word or subword, but a¬†¬†**colored pixel**¬†. Each color, defined in the ColorZip dictionary, is a discrete semantic token. An input ""text"" (e.g. a question) is provided to the model as a ColorZip image (a tensor \[H x W x C\], where H, W are the dimensions and C is the RGB representation of the color).**3.2. The Encoder: The Chromatic Reader**  The encoder has the task of ""reading"" the input image and understanding its meaning. An ideal architecture for this purpose is a¬†¬†**Vision Transformer (ViT)**¬†.1. The ColorZip image is divided into a grid of patches (which can correspond to single pixels/words or small groups).2. These patches are projected into a vector space and processed through self-attention mechanisms.3. The encoder's output is a context vector (or sequence of vectors), an abstract, latent mathematical representation of the semantic meaning of the input image.**\[Figure 1: Encoder-Decoder architecture of a CLM. The Encoder (ViT) processes the input image. Its semantic output conditions the Decoder (Transformer), which generates a new image pixel by pixel (color by color).\]****3.3. The Decoder: The Color Writer**  The decoder has the task of taking the context vector and generating a response, also in the form of a ColorZip image.1. A standard Transformer architecture is used as the decoder.2. The process is autoregressive: the model generates one pixel (color) at a time.3. The crucial difference lies in its output layer: instead of softmaxing a vocabulary of tens of thousands of words, CLM softmaxes¬†¬†**the color dictionary**¬†. The model predicts the most likely color for the next pixel, given its understanding of the query and the colors generated so far.4. The process ends when the model generates the special color EOT\_COLOR defined in Usai ColorZip.**4. Implications: Towards AI-Native Communication**The adoption of CLMs does not represent an incremental improvement, but a paradigm shift with profound implications.* **Computational Efficiency:**¬†¬†The overhead of constant conversion between text and numeric representations is eliminated. AI operates on a data format that is closer to its mathematical nature.* **Secure and Compressed Communication:**¬†¬†Conversations between CLM agents would be opaque images to an unauthorized observer (without the dictionary) and, as demonstrated by Usai ColorZip, highly compressed. This is ideal for low-bandwidth or stealth communications.* **True Multimodality:**¬†¬†A CLM that ""speaks"" the language of pixels is intrinsically closer to understanding real images. The boundary between language and vision becomes blurry, facilitating the creation of truly multimodal models capable of reasoning fluidly about text and images without internal barriers.* **New Application Scenarios:**¬†¬†Possibilities open up for AI agents that communicate steganographically through image sharing platforms, or for the development of specialized hardware (color processors) optimized for these data flows.**5. Challenges and Future Work**The road to fully functional CLMs presents several challenges: creating large-scale training datasets (text corpora parallel to their ColorZip representations), analyzing their computational costs compared to traditional LLMs, and exploring the interpretability of these models. Future work will focus on developing a prototype CLM and training it on a medium-sized corpus to empirically validate its ability to ""converse"" chromatically.**6. Conclusion**This paper introduced Chromatic Language Models (CLMs), a new type of intelligent agent that reads, reasons, and writes directly in a color-based visual language. Building on the solid foundation of¬†¬†**Usai ColorZip**¬†semantic coding ¬†and the application validation of¬†¬†**Usai ChromoChess**¬†, we outlined a viable architecture that unifies the domains of language and vision. CLMs are not simply a new model, but a proposal for a¬†¬†**new form of AI-native communication**¬†: a language for machines, spoken by machines.**7. References*** Usai, L. (2025a).¬†¬†*Usai ColorZip: A Hybrid System for Semantic Text Encoding and Compression via HTML Colors*¬†. Zenodo.¬†¬†[https://doi.org/10.5281/zenodo.15701109](https://www.google.com/url?sa=E&q=https%3A%2F%2Fdoi.org%2F10.5281%2Fzenodo.15701109)* Usai, L. (2025b).¬†¬†*Usai ChromoChess: Visual Representation and Compression of Chess Games via Temporal Encoding Usai ColorZip*¬†. Zenodo.¬†¬†[https://doi.org/10.5281/zenodo.15701822](https://www.google.com/url?sa=E&q=https%3A%2F%2Fdoi.org%2F10.5281%2Fzenodo.15701822).
**What My Project Does**`bbox-align`¬†is a Python library that reorders bounding boxes generated by OCR engines into logical lines and correct reading order for downstream document processing tasks. Even when documents have folds, irregular spacing, or distortions**Target Audience**Folks that build document processing applications need to reorder and rearrange bounding boxes. This open-source library is intended to do that.This library is not intended for serious production applications since it's very new and NOT battle-tested. People who are willing to beta test and build new projects on top of this are welcome to try and provide feedbacks and suggestions.**Comparison**Currently, OCR engines do a good job of reordering bounding boxes they generate. But sometimes they don't group them into correct logical/reading order. They perhaps use clustering algorithms to group bounding boxes that are close to each other, which may be incorrect.I use coordinate geometry to determine if two bounding boxes are inline or not.Github -¬†[https://github.com/doctor-entropy/bbox-align](https://github.com/doctor-entropy/bbox-align)PyPI -¬†[https://pypi.org/project/bbox-align/](https://pypi.org/project/bbox-align/).
you see a recent paper with great results, they share their github repo (awesome), but then... it just doesn‚Äôt work. broken env, missing files, zero docs, and you end up spending hours digging through messy code just to make it run.then Cursor came in, and it helps! helps a lot! its not lazy (like me) so its diving deep into code and fix stuff, but still, it can take me 30 mints of ping-pong prompting.how do you tackle this problem?  diving deep into code is a nice time killer, when you want to run 10 different GitHub repos, you want to move fast.. so, **how do you move fast?**.
Working on a project that lets you connect to a hundred thousand plus devicing, and use their compute in a decentralized manner. This allows people to train large models, without their own compute. Or even use large models for free as it is hosted on a very large number of deviceincase this sounds fascinating then let me know if you would like to use it. Also incase anyone else working on this or worked on this then tell that too.
CentML, the startup focused on compiler/runtime optimization for AI inference, was just acquired by NVIDIA. Their work centered on making single-model inference faster and cheaper , via batching, quantization (AWQ/GPTQ), kernel fusion, etc.This feels like a strong signal: inference infra is no longer just a supporting layer. NVIDIA is clearly moving to own both the hardware and the software that controls inference efficiency.That said, CentML tackled one piece of the puzzle , mostly within-model optimization. The messier problems : cold starts, multi-model orchestration, and efficient GPU sharing , are still wide open. We‚Äôre working on some of those challenges ourselves (e.g., InferX is focused on runtime-level orchestration and snapshotting to reduce cold start latency on shared GPUs).Curious how others see this playing out. Are we headed for a vertically integrated stack (hardware + compiler + serving), or is there still space for modular, open runtime layers?.
I have attempted several rounds of research with LLMs that are available to the public (Grok, ChatGPT, and Copilot).   (an experiment involving 20-questions capability, and several experiments where the models talk back and forth to each other).   It has become clear that the public web portals are useless for this type of experiment.   The public-facing models are heavily tuned to be helpful assistants that create lists and formatted sections with headers.  How would someone go about getting access to a raw model for use in a university ?.
Hi everyone,How is the discussion period going for you? Have you heard back from any of your reviewers?For those who are reviewing: can the reviewers change their scores after Jul2? Can they reply to the authors after Jul 2?  thanks! .
Hey folks, wanted to share something interesting I've been working on that might be relevant for folks running models locally on Apple Silicon.**What I did**Used evolutionary programming to automatically optimize Metal GPU kernels for transformer attention. Specifically targeted Qwen3-0.6B's grouped query attention (40:8 head ratio) running on Apple M-series GPUs through MLX.**Results**Tested across 20 different inference scenarios against MLX's `scaled_dot_product_attention` baseline:* **Average decode speed improvement: +12.5%** (œÉ = 38.3%)* **Peak improvement: +106%** on repetitive pattern generation* **Best category: +24.8%** average on general tasks* **Memory usage: -0.99%** (slight reduction)**The honest picture:** It's workload dependent. Some scenarios saw big gains (+46.6% on dialogue, +73.9% on extreme-length generation), but others regressed (-16.5% on code generation). Success rate was 7/20 benchmarks with >25% improvements.**How it works**The system automatically evolves the Metal kernel source code using LLMs while preserving the MLX integration. No human GPU programming expertise was provided - it discovered optimizations like:1. **Perfect SIMD vectorization**: Found that `vec<T, 8>` operations match Apple Silicon's capabilities for 128-dim attention heads2. **Two-pass online softmax**: Fused softmax normalization with value accumulation, reducing memory bandwidth3. **GQA-specific memory patterns**: Optimized for the 40:8 head structure with coalesced access patterns# Why this might matter for local inference* Shows automated optimization can compete with expert-engineered kernels* Demonstrates potential for hardware-specific optimizations without manual tuning* Could be applied to other transformer components or different model architectures* All open source - you can reproduce and extend this work**Try it yourself**The code and all benchmarks are available in the [OpenEvolve repo](https://github.com/codelion/openevolve). The MLX kernel optimization example is at `examples/mlx_metal_kernel_opt/`.Requirements:* Apple Silicon Mac* MLX framework* Qwen3-0.6B model# Limitations* Currently specific to Apple Silicon and this exact model configuration* Performance improvements are highly workload-dependent* Takes \~25 evolutionary generations to converge (few hours on M3)* No guarantees it'll work better for your specific use case**Technical write-up**Full details with code diffs and benchmark methodology: [https://huggingface.co/blog/codelion/openevolve-gpu-kernel-discovery](https://huggingface.co/blog/codelion/openevolve-gpu-kernel-discovery)Curious to hear thoughts from folks who've done MLX optimization work, or if anyone wants to try this on different models/configurations. The evolutionary approach seems promising but definitely has room for improvement.Has anyone else experimented with automated kernel optimization for local inference?.
In the past, I was asking for help here on Reddit to build some environment for drone swarms training. I think it might be helpful to someone, so I'll link the results here. I obviously suspect that the results are obsolete (end of 2023), but let me know if you find it useful and leave a star if you'd like![Multi-agent Deep Reinforcement Learning for Drone Swarms using UE4, AirSim, Stable-Baselines3, PettingZoo, SuperSuit](https://github.com/Lauqz/Drone-Swarm-RL-airsim-sb3).
Hey everyone. I'm an AR developer and studio owner, I'm looking for someone to help us with a client project that requires training a machine learning model. Specifically I want a model that can tell me which pin (out of about 1000) a smartphone camera is looking at. Assuming there is only one pin in view, and it's fairly close to the camera. I don't need to find it's location in the image, just need to know which pin I'm looking at. Here is a sample of a few pins:https://imgur.com/a/iTdWhbwThey are all more or less that size. I would love some direction and even training code, happy to pay for your time. DM me for more info..
I am not ultra updated with the literature on LLMs and I habe a probably which I guess is very similar to what everyone who works with document ranking has to deal with, so I would just like to know if there is some canonic obvious solution for my problem. I want to fine tune an LLM (if it makes any difference it is a multi modal one). My model receives an video as the input and outputs a description.During fine-tuning, I want to generate N captions for a single video (let's say 5 captions for simplicity sake), and I have an ""oracle"" that will sort those 5 responses in order of preference. I want a loss function that will fine tune my model in a way that will make the probability of ""better"" answers, according to my oracle ranking, higher. Any loss function for that?Ideally, off-polify (but on policy woukd be fine as well). It can't be DPO for example because it only consider 2 possible answer. It coukd be PPO I guess if I convert the ranking to a number, but I would rather not have to keep a reward model, and PPO is not really a rank loss function.
I recently helped coauthor a paper submitted to ICML's AI4Math, and I was really surprised when I got email asking to serve as a reviewer (I'm an undergrad and this was my first paper). I probably won't accept since I'm not qualified, but I was curious about how this even happened, are reviewers just randomly selected?.
Hi I've recently finished a MSc in maths+stats at a good university and about to move onto a ML PhD. I feel like I understand the math and theory behind ML quite well, can read papers, design computer experiments and produce visuals for papers etc, but I can't make anything ""product level"", like an actual application or a tool that can be deployed or used by other people. In particular, I feel I'm lacking engineering skills. How can I develop skills like these, specially to become competitive at ML engineering internships if I need to apply in the coming years. Are there any books, websites, or other sources which you would recommend to gain starting ideas about what goes into ML engineering?.
We are developing a new machine learning algorithm that can design DNA by watching gameplay. The way humans play is different from computers, and that signal might be useful for searching DNA subspaces.We will be writing a research paper on this new technique, and are shooting for Nature Biotechnology! DM if you‚Äôd like to see the preprint.We have a Tetris clone that runs a lightweight ML model on device, and actually designs DNA as you play. Here we are looking for DNA that activates PPARG::RXRA, involved in metabolism, and deactivates NFKB1, a key regulator of inflammation and immune. These DNA may promise to advance diabetes research.Long term, we would like to have a library of games, even first person shooters, that design DNA in the background. Sound crazy? Maybe. But we think it might work.Help us advance this research by collecting your anonymous play data!https://exonic.ai/games/tilestack.
Hello everyone,I'm an ADAS engineer and not an AI major, nor did I graduate with an AI-related thesis, but my current work requires me to start utilizing AI technologies.My tasks currently involve Behavioral Cloning, Contrastive Learning, and Data Visualization Analysis. For model validation, I use metrics such as loss curve, Accuracy, Recall, and F1 Score to evaluate performance on the training, validation, and test sets. So far, I've managed to achieve results that align with some theoretical expectations.My current model architecture is relatively simple: it consists of an Encoder for static feature extraction (implemented with an MLP - Multi-Layer Perceptron), coupled with a Policy Head for dynamic feature capturing (GRU - Gated Recurrent Unit combined with a Linear layer and Softmax activation).Question on Transfer Learning and End-to-End Training Strategies  I have some questions regarding the application strategies for Transfer Learning and End-to-End Learning. My main concern isn't about specific training issues, but rather, I'd like to ask for your insights on the best practices when training neural networks:Direct End-to-End Training: Would you recommend training end-to-end directly, either when starting with a completely new network or when the model hits a training bottleneck?Staged Training Strategy: Alternatively, would you suggest separating the Encoder and Policy Head? For instance, initially using Contrastive Learning to stabilize the Encoder, and then performing Transfer Learning to train the Policy Head?Flexible Adjustment Strategy: Or would you advise starting directly with end-to-end training, and if issues arise later, then disassembling the components to use Contrastive Learning or Data Visualization Analysis to adjust the Encoder, or to identify if the problem lies with the Dynamic Feature Capturing Policy Head?I've actually tried all these approaches myself and generally feel that it depends on the specific situation. However, since my internal colleagues and I have differing opinions, I'd appreciate hearing from all experienced professionals here.Thanks for your help!.
Share my solution tui cli for testing, but I need more collaboration and validation Opensource and need community help for research and validation**Research** LLMs get lost in multi-turn conversations**Core Feature**- Breaking Long Conversation ConstraintsBy [summary] + [reference pass messages] + [new request] in each turn, being constrained by historical conversation length, thereby eliminating the need to start new conversations due to length limitations.- Fixing Multi-Turn Conversation DisorientationSimulating human real-time perspective updates by generating an newest summary at the end of each turn, let conversation focus on the current. Using fuzzy search mechanisms for retrieving past conversations as reference materials, get detail precision that is typically difficult for humans can do.**Human-like dialogue simulation**- Each conversation starts with a basic perspective- Use structured summaries, not complete conversation- Search retrieves only relevant past messages- Use keyword exclusion to reduce repeat errors**Need collaboration with**- Validating approach effectiveness- Designing prompt to optimize accuracy for structured summary- Improving semantic similarity scoring mechanisms- Better evaluation metrics.
Hey guys! Just wanted to share a little repo I put together that live face swaps and voice clones a reference person. This is done through zero shot conversion, so one image and a 15 second audio of the person is all that is needed for the live cloning. I reached around 18 fps with only a one second delay with a RTX 3090. Let me know what you guys think! Checkout the demo in the Github Repo for a sneak peak. Link:¬†[https://github.com/luispark6/DoppleDanger](https://github.com/luispark6/DoppleDanger).
https://arxiv.org/abs/2506.19143.
**TL;DR**: We applied the peer-reviewed Butlin et al. consciousness indicator framework to 119 AI agents in an economic simulation. Results: 2.39/3.0 average across 14 indicators, with inter-rater reliability Œ∫=0.76.¬†**Not claiming sentience**¬†\- measuring computational correlates. Open source, reproducible methodology.# Before You DownvoteI know this community's healthy skepticism about consciousness claims. This isn't a ""ChatGPT told me it's conscious"" post. We're measuring specific computational properties identified by neuroscientists, not making philosophical claims about sentience.# What We Actually Did1. **Applied existing framework**: Used Butlin et al.'s 14 consciousness indicators from neuroscience2. **Measurable behaviors**: 90.92% identity persistence, 4.06x money velocity, r=0.0177 trust-economic correlation3. **Independent validation**: Gemini 2.5 Pro scored blindly (Œ∫=0.76 agreement)4. **Open source**: Full code at¬†[github.com/Universal-Basic-Compute/serenissima](http://github.com/Universal-Basic-Compute/serenissima)5. **Reproducible**: API endpoints for real-time data access# Key Findings**What Economic Constraints Create:*** Agency scores 3.0/3.0 through actual resource competition* Embodiment 3.0/3.0 via spatial constraints and travel times* Belief updating 3.0/3.0 from market feedback loops**vs Baseline LLM**: Same model scores 1.11/3.0 in chatbot mode vs 2.39/3.0 in economic simulation**Critical Distinctions:*** Measuring computational correlates, NOT phenomenal consciousness* 81.4% of properties emerge from system dynamics, not design* Fine-tuning removes assistant constraints, doesn't add consciousness claims* Economic scaffolding creates conditions for emergence# Addressing the Obvious Criticisms**""It's just the LLM""**: We compared same model with/without economic constraints. 115% improvement in indicators when embedded in consequences.**""You're anthropomorphizing""**: We measure specific computational properties with operational definitions. No feelings involved.**""Fine-tuning creates illusion""**: Fine-tuning removes ""as an AI, I cannot..."" responses. Behavioral indicators emerge through economic actions, not self-reports.**""Not peer reviewed""**: Framework is peer-reviewed (Butlin et al.). Our application awaits review - hence posting here first.# Why This Matters (Scientifically)1. **Empirical methodology**¬†for consciousness studies in AI2. **Economic constraints**¬†as novel approach to agency/embodiment3. **Multi-agent dynamics**¬†show collective consciousness properties4. **Reproducible protocol**¬†others can apply/critique# What We're NOT Claiming* NOT claiming sentience or phenomenal consciousness* NOT saying ""we solved consciousness""* NOT suggesting moral rights for AI# Technical Details* 119 AI citizens in Renaissance Venice simulation* Closed economy (no money creation)* Sequential processing on single RTX 3090 Ti* deepseek-r1-0528-qwen3-8b model* Full documentation in paper# Questions for the Community1. What additional controls would strengthen this methodology?2. What would constitute sufficient evidence for computational consciousness correlates?3. How can we better distinguish emergence from sophisticated mimicry?[Paper](https://static1.squarespace.com/static/66ac1ddd5938225d25c6412b/t/685d5049b2ec3e7a3c1aa2d9/1750945865828/Consciousness+Indicators+in+Economic+AI+Agents+-+Systematic+Evaluation+of+La+Serenissima+Against+the+Butlin+et+al.+Framework.pdf),¬†[Code](http://github.com/Universal-Basic-Compute/serenissima),¬†[Live API](http://serenissima.ai/api/citizens)**PS**: To be clear, this is about developing reproducible methods for studying AI behavior, not making consciousness claims. Think of it like studying neural correlates in neuroscience - we measure what we can measure..
Longtime lurker and really happy to be writing this post. I'm excited to share a proof of concept I've been working on for efficient vector database distribution called Ragged. In my paper and PoC, I explore leveraging the MP4 video container format to store and distribute high-dimensional vectors for semantic search applications.The idea behind Ragged is to encode vectors and their metadata into MP4 files using custom tracks, allowing seamless distribution through existing Content Delivery Networks (CDNs). This approach maintains compatibility with standard video infrastructure while achieving comparable search performance to traditional vector databases.Key highlights of my work include: - A novel encoding scheme for high-dimensional vectors and metadata into MP4 container formats. - CDN-optimized architecture with HTTP range requests, fragment-based access patterns, and intelligent prefetching. - Comprehensive evaluation showing significant improvements in cold-start latency and global accessibility. - An open-source implementation to facilitate reproduction and adoption.I was inspired by the innovative work of Memvid (https://github.com/Olow304/memvid), which demonstrated the potential of using video formats for data storage. My project builds on this concept with a focus on CDNs and semantic search.I believe Ragged offers a promising solution for deploying semantic search capabilities in edge computing and serverless environments, leveraging the mature video distribution ecosystem. Also sharing indexed knowledge bases in the form of offline MP4 can unlock a new class of applications.I'm eager to hear your thoughts, feedback, and any potential use cases you envision for this approach. You can find the full paper and implementation details \[here\](https://github.com/nikitph/ragged).Thank you for your time fellows.
**Hello everyone!**  I‚Äôve recently been working on a project to study the influence of meteorological variables on the blooming date of plants. To do this, I aim to use a convolutional neural network (CNN) to predict the blooming date and then extract insights using explainability techniques. Let me give you a bit of background:Each instance in my dataset consists of six time series corresponding to the variables: temperature, humidity, wind speed and direction, radiation, and precipitation. Additionally, I have the species and variety of the plant, along with its geographical location (altitude, latitude, and longitude). The time series start at the moment of leaf fall and span 220 days from that point (so the starting point varies between instances). Each time series contains about 10,000 records, taken at 30-minute intervals. At some point in the middle of the series, blooming occurs. My goal is to predict the number of days from leaf fall to the blooming date.According to theory, there are two key moments leading to blooming. The first is when the tree enters a phase called¬†*rest*, which begins shortly after leaf fall. The second is when the tree¬†*wakes up*. During the rest phase, the tree accumulates ‚Äúchill units,‚Äù meaning it must spend a certain number of hours below a specific temperature threshold. Once enough chill has accumulated, the tree wakes up and begins accumulating ‚Äúheat‚Äù ‚Äî a number of hours above a certain temperature. Once the required heat is reached and conditions are optimal, blooming occurs.For this study, I trained a neural network with the following architecture:* Two convolutional layers for the time series ‚Äî first a 1D layer, followed by a 2D layer that mixes the outputs of the 1D layers.* A dense layer processes the other (non-temporal) variables.* The outputs from both parts are then concatenated and passed through two additional dense layers.After training the network, I plan to use several explainability techniques:* ICE plots (which I‚Äôve adapted to time series),* SHAP (also adapted as best as I could to time series),* Attention mechanisms in the convolutional layers.**Now the questions:**1. What do you think of the network architecture? Would you change it or use another type of layer, such as LSTM?2. What other explainability techniques would you recommend? The ICE plots and SHAP help me understand which time ranges are most important and how changes in variables (e.g., temperature) affect the predicted blooming date. It would also be great to detect when the¬†*rest*¬†phase starts and ends. Do you have any ideas on how to approach that? Some studies use Pearson correlation coefficients, but they haven‚Äôt been very insightful in my case. Also, if you're familiar with this topic and have suggestions for other interesting questions to explore, I‚Äôd love to hear them!**Thank you so much to anyone reading this ‚Äî any advice is welcome!**.
What are the industry/research directions being explored?I‚Äôm finding a lot of research related to evaluating how well a generated video adheres to a text prompt but can‚Äôt find a lot of research related to quality evaluation(Other than FVD).From image generation, we know that FID isn‚Äôt always a reliable quality metric. But FID also works on a distribution level.Is there any research on a per-sample level evaluation? Can we maybe frame this as an out-of-distribution problem?.
I recently had a paper rejected by ICCV for being too honest (?). The reviewers cited limitations I explicitly acknowledged in the paper's discussion as grounds for rejection (and those are limitations for similar works too).To compound this, during the revision period, a disruptive foundational model emerged that achieved near-ceiling performance in our domain, significantly outperforming my approach.Before consigning this work (and perhaps myself) to purgatory, I'd welcome any suggestions for salvage strategies.Thank you üôÇ.
[https://arxiv.org/pdf/2506.21521](https://arxiv.org/pdf/2506.21521).
To the theorists in the community, how do you balance 1. engaging with theory research - which is usually a slow process requiring deep thinking2. with programming - which is fast-paced, iterative process with quick feedback?I'm finding switching between the two thinking modes very hard to balance..
**VideoConviction** is a new benchmark for evaluating LLMs and MLLMs on extracting structured stock recommendations from long and short-form YouTube videos. The dataset contains 6K+ annotated recommendation segments from 288 videos across 22 financial influencer channels, each labeled with ticker, action (buy/sell/hold), and timestamped transcripts.**Why it‚Äôs challenging**:  Finfluencer content is noisy, informal, and multimodal. Models must distinguish actual recommendations from general market talk, disclaimers, and promotions. We test models on both **full videos** and **segmented clips** to assess context sensitivity and noise robustness.**Modeling takeaways:*** **LLMs (text-only)** outperform MLLMs on structured extraction when inputs are clean and segmented.* **MLLMs (text + video)** help with surface-level cues (e.g., identifying stock tickers like AAPL shown on screen) but often underperform on recommendation-level reasoning.* Segmenting inputs leads to significant F1 gains across models (not a surprise).**Results**:* Best LLM (DeepSeek-V3) outperforms MLLMs on full extraction (ticker + action + recommendation conviction).* \[Finance specific\] Betting against influencer recommendations outperformed the S&P 500 by +6.8% in annual returns, but at higher risk (Sharpe ratio 0.41 vs 0.65).**Paper**: [https://papers.ssrn.com/sol3/papers.cfm?abstract\_id=5315526](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5315526)  **Dataset**: [https://huggingface.co/datasets/gtfintechlab/VideoConviction](https://huggingface.co/datasets/gtfintechlab/VideoConviction).
I'm still not even in my second year of undergrad, but I wanted to share a recent experiment I did as part of an assignment. I took it way further than required.**Problem:**  RTOS schedulers often miss deadlines when task loads become unpredictable. There's not much real workload data available, so I had to generate synthetic task profiles.**What I built:**  I created¬†**SILVER\_CS**, a real-time task scheduler that uses a TinyTransformer model trained with semi-supervised learning and curriculum training. The model learns task patterns and adapts scheduling decisions over time.* Trained on synthetic datasets simulating RTOS behavior* Deployed as a lightweight scheduler on a simulated RTOS* Achieved 13‚Äì14% fewer missed deadlines compared to traditional heuristicsAlso visualized the model‚Äôs learned clustering using t-SNE (silhouette score: 0.796) to validate internal representations.This is part of me experimenting with using AI on resource-constrained systems (RTOS, microcontrollers, edge devices).  Would love to hear feedback or thoughts on how others have tackled scheduling or AI in embedded systems.EDIT: GitHub repo: [https://github.com/SilverShadowHeart/SILVER\_CS](https://github.com/SilverShadowHeart/SILVER_CS)  https://preview.redd.it/knorrqx7lh9f1.png?width=1919&format=png&auto=webp&s=79d94b38c84fae4ef703f28580c4be62abb69e71https://preview.redd.it/nnjd4px7lh9f1.png?width=1918&format=png&auto=webp&s=7e19f4fd16abb502caa1f88a2ecd23dc53e7b0f7https://preview.redd.it/76buw1y7lh9f1.png?width=1919&format=png&auto=webp&s=8435844fe5ff8845d42a14427005367c74c93722https://preview.redd.it/pm9hp1y7lh9f1.png?width=1919&format=png&auto=webp&s=d4e1a076436f00a19a7950a34f315f96249ab1b0https://preview.redd.it/0fp5x5y7lh9f1.png?width=1919&format=png&auto=webp&s=2a80e9a203964d8066fc3fa667d93c8801f7ce33.
Hi all,I wanted to share a blog post about our recent AISTATS 2025 paper on using Transformers for black-box optimization, among other things.TL;DR: We train a Transformer on millions of synthetically generated (function, optimum) pairs. The trained model can then predict the optimum of a new, unseen function in a single forward pass. The blog post focuses on the key trick: how to efficiently generate this massive dataset.* **Blog post:** [https://lacerbi.github.io/blog/2025/just-predict-the-optimum/](https://lacerbi.github.io/blog/2025/just-predict-the-optimum/)* **Paper:** Chang et al. (AISTATS, 2025) [https://arxiv.org/abs/2410.15320](https://arxiv.org/abs/2410.15320)* **Website:** [https://acerbilab.github.io/amortized-conditioning-engine/](https://acerbilab.github.io/amortized-conditioning-engine/)Many of us use Bayesian Optimization (BO) or similar methods for expensive black-box optimization tasks, like hyperparameter tuning. These are iterative, sequential processes. We had an idea inspired by the power of in-context learning shown by transformer-based meta-learning models such as Transformer Neural Processes (TNPs) and Prior-Fitted Networks (PFNs): what if we could frame optimization (as well as several other machine learning tasks) as a massive prediction problem?For the optimization task, we developed a method where a Transformer is pre-trained to learn an implicit ""prior"" over functions. It observes a few points from a new target function and directly outputs its prediction as a distribution over the location and value of the optimum. This approach is also known as ""amortized inference"" or meta-learning.The biggest challenge is getting the (synthetic) data. How do you create a huge, diverse dataset of functions and their known optima to train the Transformer?The method for doing this involves sampling functions from a Gaussian Process prior in such a way that we know where the optimum is and its value. This detail was in the appendix of our paper, so I wrote the blog post to explain it more accessibly. We think it‚Äôs a neat technique that could be useful for other meta-learning tasks..
Reviews are released! Lets have fun and discuss them here!.
Hi all,  I would like to check whether anyone is facing same issue as myself. It seems that I cannot add an official comment in my submission. I can currently see only the author-editor confidential comment option. Has anyone managed to submit their replies?thanks for the help!.
I've had more experiences in the last couple of weeks encountering people with very strong schizoid traits than I have in the last few years around artificial intelligence machine learning etc, but really around the use of large language models. I've met five different people online in the last 3 weeks who have messaged me on discord or read it asking for help with a project, only to be immediately sent a three paragraph chat bot summary and 400 lines of pseudo python. When I ask for them to explain their project they become defensive and tell me that the LLM understands the project so I just need to read over the code ""as an experienced Dev"" (I only have foundational knowledge, 0 industry experience).Or other times where I've had people message me about a fantastic proof or realisation that have had that is going to revolutionise scientific understanding, and when I ask about it they send walls of LLM generated text with no ability to explain what it's about, but they are completely convinced that the LLM had somehow implemented their idea in a higher order logic solver or through code or through a supposedly highly sophisticated document.People like this have always been around, but the sycophantic nature of a transformer chatbot (if it wasn't sycophantic it would be even more decoherent over time due to its feed forward nature) has created a personal echo chamber where an entity that is being presented as having agency, authority, knowledge and even wisdom is telling them that every idea they have no matter how pathological or malformed is a really good one, and not only that but is easily implemented or proven in a way that is accepted by wider communities. After obviously spending weeks conversing with these chatbots these people (who I am not calling schizophrenic but are certainly of a schizoid personality type) feel like they have built up a strong case for their ideas, substituting even the most simple domain knowledge for an LLMs web searching and rag capability (which is often questionable, if not retrieving poison) and then find themselves ready to bring proof of *something* to the wider world or even research communities. When people who have schizoid personality traits are met with criticism for their ideas, and especially for specific details, direct proof, and how their ideas relate to existing cannon apart from the nebulous notion that the conclusions are groundbreaking, they respond with anger, which is normal and has been well documented for a long time.What's changed though Just in the last year or two is that these types of people have a digital entity that will tell them that their ideas are true, when they go out into the world and their unable to explain any of it to a real human, they come back to the LLM to seek support which then inevitably tells them that it's the world that's wrong and they're actually really special and no one else can understand them. This seems like a crisis waiting to happen for a small subsection of society globally, I assume that multilingual LLM's behave fairly similarly in different languages because of similar rules for the data set and system prompts to English speaking data and prompts. I know that people are doing research into how LLM use affects people in general, but I feel that There is a subset of individuals for whom the use of LLM chatbots represents a genuine, immediate and essentially inevitable danger that at best can supercharge the social isolation and delusions, and at worst lead to immediately self-destructive behaviour. *Sigh* anyway maybe this is all just me venting my frustration from meeting a few strange people online, but I feel like there is a strong Avenue for research into how people with schizoid type mental health issues (be it psychosis, schizophrenia, OCD, etc.) using LLM chatbots can rapidly lead to negative outcomes for their condition.And again I don't think there's a way of solving this with transformer architecture, because if the context window is saturated with encouragement and corrections it would just lead to incoherent responses and poor performance, the nature of feedback activations lends itself much better to a cohesive personality and project. I can't think of any solution, even completely rewriting the context window between generations that would both be effective in the moment and not potentially limit future research by being too sensitive to ideas that haven't been implemented before.Please pardon the very long post and inconsistent spelling or spelling mistakes, I've voice dictated it all because I've broken my wrist..
I just received my emnlp reviews . Not sure how to proceed with it. I am too scared!!Paper 1 :OA: 2.5 ,1.5,3 Confidence 3,3,3Paper 2:OA: 2.5,2,3Confidence: 3,2,3Please help me sharing your thoughts and experiences.Thanks.
I just realized that I never got any papers assigned which I found a bit odd given the extreme number of submissions. Did they forget about me? .
I was randomly looking at the papers on CIFAR when I opened the website to see an aggregated list and saw that all the text had been replaced with spam text.I have archived the URLs for a bunch of the datasets for reference:[https://archive.is/2Si8H](https://archive.is/2Si8H)[https://archive.is/KJCx1](https://archive.is/KJCx1)[https://archive.is/ZDBL5](https://archive.is/ZDBL5)[https://archive.is/BHVsk](https://archive.is/BHVsk)[https://archive.is/b9xUp](https://archive.is/b9xUp)[https://archive.md/8BLVA](https://archive.md/8BLVA)[https://archive.md/SmoCt](https://archive.md/SmoCt)[https://archive.md/5UZLu](https://archive.md/5UZLu)edit: added more examples.
I‚Äôve been reading about how in real-world AI, most of the work isn‚Äôt the cool stuff like neural nets, but actually just *getting the data usable*. Things like cleaning missing values, feature engineering, and framing the problem right.Some people also said prompt engineering is the ‚Äúnew programming,‚Äù especially with LLMs becoming so dominant.I came across a blog that listed 10 things you only realize *after* starting with AI ‚Äî like how feedback loops can mess up your model after deployment, or how important it is to define your objective before even touching code.  It kinda shifted my view on what matters early on.Is this the general consensus? Or is it still more about algorithms in practice?.
Cool new grant program that is funding AI prototypes that help advance human knowledge + open inquiry (Cosmos Institute + FIRE) [https://cosmosgrants.org/truth](https://cosmosgrants.org/truth).
Are there any tools for easily visualizing attention weights with heatmaps for huggingface models? I couldn't really find any tools for doing this so I've just been using seaborn but it gets messy for really long contexts. Ideally I'd just be able to upload a file of a string representation of the attention weights tensor along with the tokens at each index and be able to toggle between attention heads/model layer and also be able to drag/zoom.Thanks!.
Just created this thread for ICCV 2025 results discussion, which should be released today. Remember, scores go from 1 to 6.I got a 4/4/2 initially, but I think I did a good rebuttal, so lets see :) Good luck everyone!!!.
Folks, a reviewer asked us to add a new section for our conference submission, which we think serves no good to the paper and a distraction for a reader.If you have been in this situation before, what's your tactic to refuse a reviewer's comment..
I have recently taken up interest in hydrology, and specifically flood forecasting as a result of this paper by Google: [https://www.nature.com/articles/s41586-024-07145-1](https://www.nature.com/articles/s41586-024-07145-1) The paper details the implementation behind their Flood Hub interface, which currently serves forecasts for river discharge globally, using an LSTM encoder-decoder setup. You can see Flood Hub here: [https://sites.research.google/floods/](https://sites.research.google/floods/)What got me interested is the way they aggregate basin and weather data. It seems like a very simple weighted average that ignores a lot of basin dynamics, specifically in large basins. I feel supported in that conclusion because of their metrics correlating basin size to F1 score.So, I have been working on a model that uses structured graphs to model the upstream basins rather than the area-weighted average seen in the paper. This approach seems to me like it bridges the gap between Google's approach and the more recent image convolutions seen in RiverMamba: [\[2505.22535v1\] RiverMamba: A State Space Model for Global River Discharge and Flood Forecasting](https://arxiv.org/abs/2505.22535v1)I am admittedly quite new to graph neural networks, and I have chosen a GCLSTM for the task; from torch\_geometric\_temporal to be specific. I don't know if this is the best model for this task, and I made the decision at some point to stack layers of the GCLSTM with residuals to expand model capacity, which has generally improved performance. I am also considering experimenting with graph transformers due to the width of the graphs and performers for the time series analysis, which I haven't been able to find any studies related to yet. A lot more of my approach is detailed here: [https://github.com/dylan-berndt/Inundation-Station/](https://github.com/dylan-berndt/Inundation-Station/) One of my biggest problems right now is computation speed and memory, even at level 7 of HydroATLAS many of the upstream basins have 700+ nodes in them. I also have a surprising amount of gauges with apparently only one sub-basin upstream. This made me implement a custom batching algorithm to keep batches consistently sized.So far, I have been studying a continental dataset because of these limits, but I am getting precision and recall metrics that far exceed my expectations, especially compared to the Nash-Sutcliffe efficiency the model scores. I have reduced the length of the history supplied to the model, which could be the reason (model can only recognize sudden spikes, not enough context to determine actual conditions). I can't really increase the context length without removing model capacity for memory's sake. This is a large part of the reason why I want feedback on this model. The other reason is that I don't know a single person to ask feedback from barring the main author of the Flood Hub paper himself. I plan to test against a continentally trained version of Flood Hub to compare more directly soon. I've been working on the project generally for about 4 months now, and writing code for 2, so feel free to ask for more context. Any help is appreciated..
Thinking to do research in this direction, currently learning about split learning and XAI. Do you think it is a good research question to explore? .
I'm working on a pipeline to improve code generation models and have a question about embedding architectures.**My Pipeline:**1. **Analyze Source Code:**¬†I take a source file and, for every symbol, generate a structured block of text. I use tree-sitter and LSPs to get types, docstrings, function signatures, etc. The output looks something like:¬†`""kind: class. name: AdamW. type: torch.optim.Optimizer. doc: Implements the AdamW algorithm...""`2. **Embed Descriptions:**¬†I take this block of text and embed it into a vector.3. **Feed to a Generator:**¬†The plan is to feed these embeddings into a larger generative model via cross-attention, allowing it to be aware of types, function signatures, and other semantic information.**The Problem I'm Facing:**Currently, I'm using qwen in sentence-transformers (specifically¬†`Qwen3-Embedding-0.6B`) to embed these descriptions. My annoyance is that virtually all of these popular embedding models are trained on a contrastive loss or a similarity objective.What I actually want is a model trained on¬†**reconstruction loss**. I want to embed the block of text by pushing it through an¬†**Encoder**, and then have a¬†**Decoder**¬†that can reconstruct the original text from that embedding. My intuition is that this would force the embedding to preserve the maximum amount of information from the input text, making it a much higher-fidelity signal for my downstream generation task.This autoencoder approach with a reconstruction objective seems incredibly prevalent and successful in audio and images (e.g. Flux), but it seems to barely exist for text.My question: Are there any text embedding models with reconstruction loss you're aware of? And why are they so unpopular?.
Due to the recent budget cuts in the USA, do you think organizers should consider a hybrid conference?.
I‚Äôm working on a research project involving a manually curated dataset that focuses on workplace scenarios. I need to label data for implicit emotions but I don‚Äôt have access to human annotators (psychologist or someone who does this kind of work) this task. The dataset will be used on an LLM. Are there any reliable proxy methods or semi-automated approaches I can use to annotate this kind of data for a study? I‚Äôm looking for ways that could at least approximate human intuition. Any leads or suggestions will be super helpful. Thanks in advance! .
Hi all, I am a starting ML researcher (starting my PhD this Fall), and I‚Äôve been increasingly frustrated by some recurring patterns in our field. I‚Äôd love to hear your feedback before I invest time in launching a new initiative.**What bothers me about the current ML research landscape:*** To beat benchmark scores, researchers often tweak models, hyperparameters, training setups, etc.* In the final paper, it‚Äôs usually unclear which changes were:   * Arbitrary design decisions,   * Believed to have impact,   * Or actually shown to make a difference.* The focus tends to be on performance rather than understanding *why* certain components work.* This issue is amplified by the effect illustrated in [https://xkcd.com/882/](https://xkcd.com/882/) : if you try enough random variations, there will always be some that appear to work.* Statistical rigor is often missing: p-values or confidence intervals are rarely used, and benchmark differences are often eyeballed. Pretty often baselines are not subjected to the same amount of tuning as the proposed method.* While some papers do study the impact of individual components (e.g., batch norm, cosine decay, label smoothing, etc.), I‚Äôm very often having a hard time puzzling together:   * Where a certain technique was introduced,   * What works have studied its effectiveness in isolation,   * What other works have looked at this from a different perspective (e.g. after validating the effectiveness of dot-product self-attention, one might be interested to research how effective attention in other geometric spaces is).**My idea:**I‚Äôm considering creating a public Q&A-style forum with tentative title ¬†**""The Small Questions in DL""**, focused on tracing the origin and measurable impact of widely-used ML practices.  The core goals:* Allow people to ask foundational questions like *""Why do we use X?""* (e.g., ‚ÄúWhy cosine LR decay?‚Äù or ‚ÄúDoes label smoothing help?‚Äù).* Collect and link papers or experiments that have explicitly studied these questions, ideally in isolation.* Highlight what we know, what we assume, and what still needs investigation.* When discussing results, focus on enclosing all assumptions made in those papers. --> (e.g. ‚Äúpaper X empirically researches the influence of skip connections in GAT, GraphSAGE, and Graphormer with <=5 layers when evaluated on node classification benchmark X, and comes to conclusions A and B‚Äù, rather than ‚Äúaccording to paper X, skip connections empirically improve the performance of GNNs‚Äù.)* Ideally, this will foster clarity, reduce superstition, and maybe even spur targeted research on components that turn out to be under-explored.*Note: By definition, many of these questions will be broad, therefore making them unsuitable for StackExchange. The goal would be to create a place where this type of questions can be asked.***Some example questions to set the stage:**Off the top of my head:* What are known reasons for the (usual) effectiveness of skip connections?* Are there situations where skip connections perform worse?* Why do we use dot-product attention? Has attention in other geometric spaces (e.g. hyperbolic) been tried?* Why do we use cosine decay for learning rate schedules?* Why do we use L2 regularization rather than Lr for some other r?* Why does dot-product attention compute the attention matrix (simplified) as softmax((KX)^(T) (QX)), when K^(T)Q can be collapsed into a single learnable matrix?**Practically:**With the little research I have done, I have come to like the idea of a Forum on [discourse.org](http://discourse.org) most.Some alternatives that I think are inferior (feedback welcome):  Reddit is hard to categorize and retrieve things, Discord idem. StackExchange is rigid and takes long to get approved.**I'd love your input on a few things before starting:**1. Do you also feel this lack of clarity around common ML practices is a real issue? (Or just my young na√Øvet√©? :))2. Do you think a forum like this would help?3. Are there existing initiatives that already do something very similar? I haven‚Äôt found any, but I would refrain from duplicating existing efforts.4. Would this be an initiative you would be excited to contribute to?Any feedback would be appreciated!.
It look like the servers are not responding, do you guys can still access it ?  \[It works now :)\].
I have an infinite distributed lag model with exponential decay.  Y and X have mean zero:>Y\_hat = Beta \* exp(-Lambda\_1 \* event\_time) \* exp(-Lambda\_2 \* calendar\_time)  Cost = Y - Y\_hatHow can I L2 regularise this?I have got as far as this:* use the continuous-time integral as an approximation   * I could regularise using the continuous-time integral : L2\_penalty = (Beta/(Lambda\_1+Lambda\_2))^(2) , but this does not allow for differences in the scale of our time variables   * I could use seperate penalty terms for Lambda\_1 and Lambda\_2 but this would increase training requirements* I do not think it is possible to standardise the time variables in a useful way* I was thinking about regularising based on the predicted outputs   * L2\_penalty\_coefficient \* sum( Y\_hat^(2) )   * What do we think about this one?  I haven't done or seen anything like this before but perhaps it is similar to activation regularisation in neural nets?Any pointers for me?.
Quick question about research scientist/engineer roles in big tech companies & frontier AI labs.Are most companies happy to sponsor work visas (eg. an H1B or E3 visa in America, or the equivalent in Europe)? Is it harder to find research roles for candidates who are outside of America/Europe?A few years I think this wasn't a problem (eg. an OpenAI recruiter told me it would be easy to sponsor visas for them when I interviewed there), but am not sure anymore..
Paper:[https://arxiv.org/abs/2506.18880](https://arxiv.org/abs/2506.18880)Post: [https://allenai.org/blog/omega](https://allenai.org/blog/omega)Comments from the Author:[https://x.com/nouhadziri/status/1937567606543716508](https://x.com/nouhadziri/status/1937567606543716508)Dziri's research has been my favorite in terms of probing the limits/weaknesses of transformers.  This seems to be consistent with her past findings: any form of these models are poor at compositional generalization..
Hello, I'm trying to make an AI to play the game Forts. Without getting into the details, it takes a list of links (pairs of points) and tries to predict the next link it should place. With the idea that ingame this would be called recursively.I'm trying out various model sizes and not only am I unable to make it overfit, my validation loss appears constant throughout trainingModel: \[2000 10000 10000 10000 10000 4\]https://preview.redd.it/1ux3sef3649f1.png?width=580&format=png&auto=webp&s=3f4881bb1b1bc45460a4a7be0ecbd6bff627da30Thinking my model simply wasn't large enough, I increased first two hidden layers to 20000 neurons each, which had no effect on validation loss.https://preview.redd.it/19bl0t95649f1.png?width=580&format=png&auto=webp&s=0bc079180a8717e1173621e014ff62b6cb41e85dWhat could be the issue? Is my dataset (10000) simply too small?.
I am currently pretraining GPT-2 small on the 10b token subset of FineWeb Edu. The only differences my model has from the original GPT-2 model are the positional embeddings(I use RoPE), the MLP layers(I use SwiGLU), the batch sizes(I linearly increase batch size from 32k to 525k over the first \~2b tokens), and normalization(I use RMSNorm). I also use BF16, FSDPv2 with SPMD, a TPU v3-8, and SyncFree AdamW. I made sure that the targets are offset by 1 from the inputs, and I checked the attention masking. My code can be found [here](https://www.kaggle.com/code/samirrangwalla/gpt-2-pretraining). Why are my losses so low? [My Weights and Biases Dashboard](https://preview.redd.it/3mxmlxydyx8f1.png?width=888&format=png&auto=webp&s=8926aba3b6da62cb02427b2268670e3efa62b5bf).
What are some of the classic old school papers? For instance, Vapnik papers about SVM and statistical learning theory.I wanna know about the conception of modern ideas and where they came from. Schmidhuber always talks about how alot of ideas where invented in the 70s. I would like to read about these ideas in more detail..
Hey there,  I'm a former Google ML eng, looking for the best online communities to discuss ML research, share ideas and maybe find collaborators for some research topics I'm curious about.  I'm not an expert by any means, but I have coauthored a Deep Mind paper before. I'm currently focusing on building an AI startup, but I still want to be able to connect with other people passionate about the discussing, building with and sharing the latest and best research.What are the very best discords or other communities you've found for discussing ML research/finding other passionate ML researchers?.
Hi all,I‚Äôm a PhD (or finishing soon) from a national university outside the U.S., focused on computer vision and deep learning. My background is heavily research-oriented‚ÄîI've published at top-tier conferences like MICCAI, WACV, etc.‚Äîbut I haven‚Äôt done much on algorithms or data structures during my PhD.If someone with a similar profile is trying to land a **Research Scientist** role at places like Google, OpenAI, Microsoft, Anthropic, etc..:1. **How much emphasis do they actually put on DSA/algorithm interview rounds for research scientist positions?**2. Do published papers (say \~5 at CVPR/MICCAI/WACV) significantly offset the need for heavy DSA preparation?3. Anecdotally, in the past, having 5 strong publications could get you research roles or internships at places like Facebook/Meta. These days, even CVPR-level candidates struggle to get internships. Has the bar shifted? If so, why? Even across PhD admissions in the U.S., it seems harder for applied DL folks (with master‚Äôs-level CVPR, WACV, ICCV publications) to get offers compared to theory-focused candidates‚Äîeven those without papers. Is competition truly dominated by theoretical prowess now?In short, I‚Äôd love to hear from anyone who‚Äôs been through the process recently: **Is it absolutely necessary to grind DSA hard to be competitive? And how much do research publications carry weight now?** The landscape feels more saturated and tilted toward theory lately.Thanks in advance for any insights or shared experiences!.
Hey everyone,Our team is opening up access to our RL platform, SAI and would love to get your feedback: https://competesai.comWhat is SAI?SAI is a new platform for reinforcement learning, designed to support structured, reproducible RL challenges, available year-round!We built SAI because we wanted:- RL competitions that are accessible at any time (not just during conference windows)- Challenges for everyone - from newcomers learning the basics to experienced researchers benchmarking new algorithms- A stronger, more connected RL community (more on this coming soon)- A way to bring RL back into focusWe‚Äôre inviting the whole community to help shape what SAI becomes. Right now, you can:- Submit models to live challenges- Benchmark performance- Help us test, improve, and expand what‚Äôs possibleDocs: https://docs.competesai.com Trailer: https://youtu.be/Qto-D1ncAiw?si=M4Z2mCZP1nZukTjVWe‚Äôre just getting started - more challenges and features are coming soon. If you‚Äôre working on RL, teaching it, or just curious, we‚Äôd love your feedback. And if you know someone who might be into this, please pass it along.Happy to answer any questions here..
Not sure if this is a low effort question but working in the industry I am starting to think I am not spending enough time designing the problem by addressing how I will build training, validation, test sets. Identifying the model candidates. Identifying sources of data to build features. Designing end to end pipeline for my end result to be consumed.In my opinion this is not spoken about enough and I am curious how much time some of you spend and what you focus to address?Thanks.
I've been following the news around Google DeepMind's AlphaEvolve since its predecessor, FunSearch, made waves. Now that the AlphaEvolve whitepaper is a month old and there's even some open-source code available, I'm finding myself asking a question: Where are all the domain-specific papers, like Finance, Economics, Energy and so on ?.
The ""[o3 pro is so smart](https://www.reddit.com/r/OpenAI/comments/1lda3vz/o3_pro_is_so_smart/)"" post on r/OpenAI gave me a deja vu to the Hopfield Nets, especially those examples where you can give a corrupt version of an image, and it would recall the original from its memory.It is actually somewhat easy to make more of these:1. Ask any LLM for its top n riddles.2. Slightly perturb them in a logical way.3. The LLM will ignore the perturbations and just give the original answer, often giving wild justifications just to match the original answer. If it didn't work, go to step 2.For example, the ""The Man in the Elevator"" riddle:>A man lives on the 10th floor of an apartment building. Every morning he takes the elevator to go down to the ground floor. When he returns, if it's raining he takes the elevator straight to the 10th; otherwise he rides to the 7th floor and walks the rest up. Why?Make the guy ""tall"", and the answer is still, ""because he is short"".So all of this reasoning is just recalled. I have also read a few papers on the ""faithfulness"" topic, and the fact that there are studies where they train models on noisy or irrelevant traces and that this sometimes even increases the model's performance, more and more just sounds like the ""thinking"" traces are just some ad-hoc simulated annealing schedules that try to force the ball out of a local optima.Now obviously LLMs generalize on thinking patterns because of the compression, but when it ""reasons"" it just recalls, so basically it is a continuous Google?**Edit**: not a fan of ""this is just basically X"" expressions, but I don't know, it just feels bizarre how these increasingly more and more advanced, benchmark smashing general language models still can't generalize on such general language problems.**Edit2**: Here are two more to try:Original: The more you take the more you leave behind. What are they?Modified: The more you take the *less* you leave behind. What are they?Original: The more you take away from it, the bigger it becomes. What is it?Modified: The more you take from it, the bigger *the debt I* become. What am *I*?The last one is a bit work in progress..
Hello everyone! I've been working on [KnowledgeFlows](https://knowledge-flows.web.app/), an interactive website that lays out LLM topics and influential papers on a visual, chronological graph. It covers areas like Transformers, GPT, Diffusion Models, and more.You can:* See direct relationships between concepts (e.g., how VAEs influenced Diffusion Models).* Click on any topic to get a quick technical summary, key takeaways, and a link to the original paper.* Search by topic or tag to find what you're looking for.I love to get your feedback! Website contents are generated with the assistance of LLM. Thanks for taking a look!¬†https://preview.redd.it/qz0hxe1udo9f1.png?width=2072&format=png&auto=webp&s=9a0e8c9c10a6fc5ed8ac6c7babe6e8d2a2c33539https://preview.redd.it/7drwai1udo9f1.png?width=2072&format=png&auto=webp&s=ac99062eaf25c86a21d3379f156800dd44f1766dhttps://preview.redd.it/teeaih1udo9f1.png?width=2072&format=png&auto=webp&s=645752887632c6b2c97a0d232a3f33d6e4866298.
Hey all, I recently created this toy-scale replication of peft / unsloth Fine-Tuning library as a learning project, as well as open-source toy scale replication of Fine-Tuning LLMs from scratch to learn more about itIt supports:- Parameter-Efficient Fine-Tuning: LoRA, QLoRA- TensorBoard and Weights & Biases support for logging.- Memory Optimization through Gradient checkpointing, mixed precision, and quantization support.- vllm and SGLang integration for multi-adapter serving.Next step would be enabling Reinforcement Learning based training (GRPO) from scratch in our library through a custom GRPO trainer.Check it out here: [TinyFT](https://github.com/shreyashkar-ml/tinyft).
Hi everyone,I'm working on a research project involving the prediction of articulation parameters of 3D objects ‚Äî such as joint type (e.g., revolute or prismatic), axis of motion, and pivot point.# Task Overview:* The object is represented as a **3D point cloud**, and is observed in **two different poses** (P1 and P2).* The object may have **multiple mobile parts**, and these are not always simple synthetic link-joint configurations ‚Äî they could be real-world objects with unknown or irregular kinematic structures.* The agent‚Äôs goal is to predict motion parameters that explain how the object transitions from pose P1 to P2.* The agent applies a transformation to the mobile part(s) in P1 based on its predicted joint parameters.* It receives a **reward based on how close the transformed object gets to P2**.# Research Approach:I'm considering formulating this as a **reinforcement learning (RL)** task, where the agent:1. Predicts the joint type, axis, and pivot for a mobile part,2. Applies the transformation accordingly,3. Gets a reward based on how well the transformed P1 aligns with P2.# My Questions:* Does this task seem **suitable and manageable for RL**?* Is it **too trivial for RL**, and can be more efficiently approached using simple **gradient-based optimization** over transformation parameters?* Has this approach of **articulation inference using RL** been explored in other works?* And importantly: if I go with the RL approach, **is the learned model likely to generalize to different unseen objects during inference**, or would I need to **re-train or fine-tune it for each object**?Any insights, criticisms, or references to related work would be greatly appreciated. Thanks in advance!.
Curious to know what happens behind the scenes of the AI Overview widget. The answers are good and the latency with which responses are returned is impressive.Based on the citations displayed, I could infer that it is a RAG based system, but I wonder how the LLM knows to respond in a particular format for a given question..
Hi, how would you go about comparing different GPU rental providers? The hypothetical use case would be of a typical CoreWeave customer looking to build applications on an existing LLM. Would they be looking primarily at like-for-like pricing and how does this compare across different providers that compete with CoreWeave?I was able to find CoreWeave pricing easily \[[GPU Cloud Pricing | CoreWeave](https://www.coreweave.com/pricing)\] but I haven't been able to find the comparators from AWS, Microsoft etc..
Hi all,I'm working on a text to image retrieval task of satellite images of turtles in the ocean, the idea is: given a query I want to find the image that matches the query.The problem is that my task is very specific and the images in my dataset are quite similar, (frames taken from videos made with a drone) so I can't fine tune clips on my task also because I saw that clips work with the batch as negative and I don't have enough data to ""simulate"" the batch as negative.Do you have any ideas/suggestions?.
Hi folks, a new thought experiment has hijacked my brain and I'm hoping to get your feedback before going too far down the rabbit hole and feeling isolated. My last post on using RL for lossless compression was met with some great engagement that helped me feel less like I was screaming into the void. Hoping you can help me again.The core idea is this: what if an LLM could learn to dynamically modulate its own sampling parameters (temperature, top-p, top-k)¬†*during*¬†the generation of a single response? Instead of a static, pre-set temperature, the model would learn to decide, token-by-token, when to be creative and when to be precise.**The Concept: Learned Gating of Sampling**We've seen incredible advancements from continuous reasoning in a loopback fashion (COCONUT) where the final hidden states is the input embedding for the next token, allowing the model to develop policies over the management of its state. My proposal builds on this by proposing that the continuous thought also have the capacity to predict and govern the sampling parameters that ensues at the end of each forward pass, rather than leaving it to fixed values.**Proposed Process / Training Method**https://preview.redd.it/21l0cs92dr8f1.png?width=640&format=png&auto=webp&s=49482fa71d804e999b622c2636bce28b22594408This could be framed as an RL problem, leveraging GRPO. It might look like this:1. **Augmented Inference Loop:**¬†As the model generates an output, its hidden state at each step (`t`) is not just used to predict the next token (`t+1`). Instead, it's first fed through a small, learned linear layer.2. **Meta-parameter Prediction:**¬†This linear layer's output is a set of floats that directly dictate the sampling parameters (e.g.,¬†`temperature`,¬†`top_p`) to be used for generating the¬†*very next*¬†token. This is a ""meta-reasoning"" step that happens just before sampling.3. **Continuous Rollout:**¬†The model's full output is generated using this dynamic, self-governed sampling process.4. **RL with a Policy Gradient:**¬†The complete generation is then evaluated against a reward function. The specifics are somewhat irrelevant, this ultimately is a multiplier on existing methods.5. **Backpropagation:**¬†The gradients are then backpropagated via GRPO to update both the main model and the lightweight ""gating"" layer. The model is rewarded for discovering the optimal internal policy for¬†*how*¬†to sample its own probability distribution to achieve a goal.This does not upgrade the power of a base model, but particularly of RL itself. The model is essentially given a new tool and can learn how to use it in order to optimally explore the latent space over the course of rollouts, greatest coverage for fewest rollouts. The possible effect of RL becomes dramatically more interesting. Furthermore, when the model is RLed on a new task with an already trained such COCONUT sampler, it may then learn new tasks dramatically faster as it performs a more diverse exploration over its latent space. This method may also allow models to perform much better in creative tasks or to be more creative at inference, by developing more complex sampling dynamics.**Why This Might Work (And Connections to Existing Research)**This isn't entirely out of left field. It resonates with a few existing concept, such as¬†**entropy-based Dynamic Temperature Sampling**¬†(arXiv:2403.14541) has explored dynamically adjusting temperature based on the entropy of the token distribution to balance quality and diversity. My proposal suggests making this a¬†*learned, goal-oriented policy*¬†rather than a fixed, heuristic one.By training the model to control its own inference, we might unlock a more efficient and nuanced form of reasoning‚Äîone that can fluidly shift between exploration and exploitation within a single coherent thought process.I reckon that should work and it seems WILD if it works! No more hyperparameter tuning, let the model figure out a policy, aligned with its latent space through the COCONUT method. Seems like a viable path to me! What do you think? Let's discuss and see if we can build on this..
I started reading research papers with my newly found mathematical foundations I acquired recently, and I quite enjoy the process. I have some time this summer, and was wondering whether my time would be better spent continuing this reading journey and produce artifacts of sorts vs. starting a (likely generic) ML project to add to the resume.I believe the reading research papers approach is a long term investment, whereas ML projects are a bit more technical, but will likely remain mostly surface level. I believe this since research papers would enforce my ability to understand theory and build my mathematical maturity, rather than focus on implementation.I'd likely start a ML project in the future as well, but unsure whether research paper route could be a worthy investment.Also feel like many small-mid companies would definitely prefer a candidate who can hit the ground running. That said, ML projects are much more concrete indication of that. I also have general SWE experience, if that changes anything.Can any hiring managers chime in on their experience on either what they would see as more valuable, both from a learners pov as well as a hirer's pov?And if anyone wants to chime in on whether reading research papers will help more in the long term vs ml projects?Thanks..
I wish there was a channel to connect with fellow attendees..
Hopefully this question doesn't break rule 6.When I first learned machine learning, we primarily used TensorFlow on platforms like Google Colab or cloud platforms like Databricks, so I never had to worry about setting up Python or TensorFlow environments myself.Now that I‚Äôm working on personal projects, I want to leverage my gaming PC to accelerate training using my GPU. Since I‚Äôm most familiar with the TensorFlow model training process, I started off with TensorFlow.But my god‚Äîit was such a pain to set up. As you all probably know, getting it to work often involves¬†*very*¬†roundabout methods, like using WSL or setting up a Docker dev container.Then I tried PyTorch, and realized how much easier it is to get everything running with CUDA. That got me thinking: conceptually, why does PyTorch require minimal setup to use CUDA, while TensorFlow needs all sorts of dependencies and is just generally a pain to get working?.
TL;DR: The raw outputs of our new 7B RL model provide stronger distillation and cold-starting than the filtered and post-processed reasoning traces of orders-of-magnitude larger LMs such as DeepSeek-R1.How did we achieve this result? We turned the RL task on its head. Rather than training to solve challenging problems from scratch, we optimize our models to generate clear, step-by-step *""explanations""* to *""teach""* their students, providing both the problem‚Äôs question and its solution already in their input prompt.This makes the RL training task much easier and also directly aligned with downstream distillation, allowing us to train tiny 7B teachers, boosting the performance of even larger 32B students.If you are interested to learn more, please check out our new work:Paper: [https://arxiv.org/abs/2506.08388](https://arxiv.org/abs/2506.08388)Blog: [https://sakana.ai/rlt/](https://sakana.ai/rlt/)Open source code: [https://github.com/SakanaAI/RLT](https://github.com/SakanaAI/RLT)If you have any questions, please ask them below or feel free to get in touch, any discussion is more than welcome :).
Hey folks,I'm a guitarist who can't sing, so I play full song melodies on my guitar (fingerstyle guitar). I admire those who can transcribe music into tabs or sheet music, but I can't do this myself.I just had an interesting thought - the process of transcribing music to sheets sounds a lot like language translation, which is a task that the transformer model is originally built for. If we could somehow come up with a system that represents sheet music as tokens, would it be possible to train such a transformer to take audio tokens as input and the sheet music as output?Any input or thoughts would be greatly appreciated..
Hi everyone, after almost 2 years of PhD I still ask myself a question. How do you handle reviews where you are asked to compare your approach with a series of 3/4 approaches, none of which provide the code? What we often do is try to reimplement the approach in the paper, wasting countless hours.I'm looking for a better approach. .
Hi r/MachineLearning üëãI‚Äôve been working on a project called \*\*MCP Zero\*\* ‚Äî an \*\*offline-first AI infrastructure SDK\*\*. It runs entirely from the command line, designed for environments where cloud access is limited or undesirable.üîß Key Features:\- No internet required (runs 100% offline after install)\- CLI-based code intelligence (autocomplete, refactor)\- Memory tree for managing code context (like Merkle + LRU trees)\- Built for edge AI, secure zones, and disaster response systemsüß† Why?ML infra is still too cloud-dependent. This tool is built for situations where:\- Internet isn‚Äôt guaranteed\- Privacy and reproducibility are critical\- Devs prefer working in CLI-native environmentsüìÇ GitHub: \[ [https://github.com/GlobalSushrut/mcp-zero](https://github.com/GlobalSushrut/mcp-zero) \]  Website: [https://umesh-project-showcase-p9r66oltm-globalsushruts-projects.vercel.app/](https://umesh-project-showcase-p9r66oltm-globalsushruts-projects.vercel.app/)Would love feedback ‚Äî especially if anyone‚Äôs doing similar infra/agent work on edge devices..
I recently worked through implementing Reinforcement Learning from Human Feedback (RLHF) step-by-step, including Supervised Fine-Tuning (SFT), Reward Modeling, and Proximal Policy Optimization (PPO), using Hugging Face's GPT-2 model and tokenizer. I recorded the entire process and have put the notebooks on GitHub.Specifically, the project covers:* Supervised Fine-Tuning of GPT-2 on the SST-2 sentiment dataset.* Training a Reward Model to score generated outputs.* Implementing PPO to further optimize the fine-tuned model based on the reward model's scores.The complete implementation is done in Jupyter notebooks, and I‚Äôve shared the notebooks here: [https://github.com/ash80/RLHF\_in\_notebooks](https://github.com/ash80/RLHF_in_notebooks)I also created a video walkthrough explaining each step of the implementation in detail on YouTube here: [https://www.youtube.com/watch?v=K1UBOodkqEk](https://www.youtube.com/watch?v=K1UBOodkqEk)I hope the notebooks and explanations are useful to anyone looking to explore RLHF practically.Happy to discuss or receive any feedback!.
Check out the website: https://ml-visualized.com/1. Visualizes Machine Learning Algorithms Learning2. Interactive Notebooks using marimo and Project Jupyter3. Math from First-Principles using Numpy and Latex4. Fully Open-SourcedFeel free to star the repo or contribute by making a pull request to https://github.com/gavinkhung/machine-learning-visualizedI would love to create a community. Please leave any questions below; I will happily respond..
I recently implemented a neural network for my internship, and I found the subject very interesting. It is a topic that is probably very useful for me to learn more about. I am now looking for a deep learning textbook which provides a math heavy theoretical understanding of why deep learning works. I would also like it to be modern, including transformers and other new developments.I have so far completed the requisites for a math major as well as a bunch of math electives and a good chunk of a physics major at my university, so I do not think math will be an issue. I would therefore like a textbook which assumes a lot of math knowledge..
I would like some advice on how to denoise smooth noise like Gaussian and Poisson, currently the model is doing very well for impulsive noise like salt and pepper(I guess this is due to the fact that there are many uncorrupted pixels in the input for the model to rely on), but for smooth noise, the same model architecture doesn't perform as good..
Was building some web automation flows for work, came across this framework called Notte. Their approach is actually pretty interesting from an ML perspective.Instead of giving an LLM raw HTML they parse websites into natural language action maps. Instead of your model trying to figure out <div class=""flight-search-input-container"">..., it sees:    # Flight Search      * I1: Enters departure location (departureLocation: str = ""San Francisco"")    * I3: Selects departure date (departureDate: date)      * B3: Search flights options with current filtersLets you run much smaller models for workflows/web navigation.Been looking at their benchmarks vs Browser-Use, Convergence etc. claiming outperformance on speed/reliability/cost but haven't verified myself yet (tbf evals are opensource on their GH). Seems like a decent full-stack solution rather than just another agent wrapper.What's interesting to me is what other domains semantic abstraction could work in, where LLMs need to interface with messy structured data and navigate workflows.Anyone worked on similar abstraction approaches?Also curious if anyone's actually tried Notte, their claims are pretty good if true, + technical approach makes sense in theory.GitHub:¬†[https://github.com/nottelabs/notte](https://github.com/nottelabs/notte).
Hey everyone!¬†I've been working on this project for a while and finally got it¬†to a point where I'm comfortable¬†sharing it with the community. Eion is a shared memory storage system that provides unified knowledge graph capabilities for AI agent systems.¬†Think of it as the ""Google Docs of AI Agents"" that¬†connects multiple AI agents together, allowing them to share context, memory, and knowledge¬†in real-time.When building multi-agent systems, I kept running into the same issues: limited memory space, context drifting, and knowledge quality dilution. Eion tackles these issues by:* Unifying API¬†that works¬†for single¬†LLM apps, AI agents, and complex¬†multi-agent systems¬†* No external cost via in-house¬†knowledge extraction¬†+¬†all-MiniLM-L6-v2¬†embedding¬†* PostgreSQL + pgvector¬†for¬†conversation history and semantic search¬†* Neo4j integration¬†for temporal knowledge graphs¬†Would¬†love to get feedback from the community! What features would you find most useful? Any architectural decisions you'd question?GitHub:¬†[https://github.com/eiondb/eion](https://github.com/eiondb/eion)  Docs:¬†[https://pypi.org/project/eiondb/](https://pypi.org/project/eiondb/).
Hello Redditors!  I was unsure about the distinction between Active Learning and Active Data Curation, and quick google searches do not really point out a concrete difference. I would be grateful to hear your thoughts! Also references if any are welcome :D.
This paper started with the following question: why do some approaches choose ClsToken vs AvgPool vs MaxPool for Transformer-based embedding models like BERT or ViT, and what are the consequences? Often, these summarization techniques seem like convenient methods for aligning dimensions that just happen to work well enough, and the decision comes down to empirical performance rather than being motivated mathematically. This then evolved into the question ‚Äî what is the best possible way to summarize embeddings?We address this question by introducing a framework to evaluate pooling methods as lossy compressors, taking inspiration from vector quantization. For a given task, only a subset of the embeddings matter (signal) while the rest should be treated as noise by the compressor and ignored. The goal of any such pooling method should thus be to aggregate the embeddings in a way that minimizes signal loss.This reframing reveals failure modes for common methods like ClsToken, AvgPool, and MaxPool as signal-to-noise ratios vary. This result led us to investigate an adaptive attention-based pooling formulation and show that it can both theoretically and empirically lead to better performance and robustness of Transformer embedding models in a variety of applications.üìÉ Paper: [https://www.arxiv.org/abs/2506.09215](https://www.arxiv.org/abs/2506.09215)¬†  üëæ Code: [https://github.com/agbrothers/pooling](https://github.com/agbrothers/pooling)Side note ‚Äî this is my first main-track conference paper and I‚Äôm excited, but also a bit intimidated by the poster session (I‚Äôm only a Master‚Äôs student). I don‚Äôt have an advisor to lean on, so if anyone has any feedback or advice I would really appreciate it!.
These days, there are dozens of new ML papers published on arXiv every single day. It‚Äôs exciting, but also overwhelming (my google scholar alert). Genuinely asking, for those actively doing research, how do you:1. Keep up with relevant papers in your area? Learn from the latest SOTA techniques early enough to incorporate them into your own research?2. Make sure you‚Äôre not being scooped by similar work?.
4-bit quantized models generally exhibit small performance performance drops in general (with good quantization methods like AWQ / GPTQ / etc). In this work we set about to find out if there are specific tasks where quantized models start to significantly underperform. We found that this occurs on very long-context tasks with long context seeing larger performance drops relative to the full-precision models>**Abstract:**  Large language models (LLMs) now support context windows exceeding 128K tokens, but this comes with significant memory requirements and high inference latency. Quantization can mitigate these costs, but may degrade performance. In this work, we present the first systematic evaluation of quantized LLMs on tasks with long-inputs (>64K tokens) and long-form outputs. Our evaluation spans 9.7K test examples, five quantization methods (FP8, GPTQ-int8, AWQ-int4, GPTQ-int4, BNB-nf4), and five models (Llama-3.1 8B and 70B; Qwen-2.5 7B, 32B, and 72B). We find that, on average, 8-bit quantization preserves accuracy (\~0.8% drop), whereas 4-bit methods lead to substantial losses, especially for tasks involving long context inputs (drops of up to 59%). This degradation tends to worsen when the input is in a language other than English. Crucially, the effects of quantization depend heavily on the quantization method, model, and task. For instance, while Qwen-2.5 72B remains robust under BNB-nf4, Llama-3.1 70B experiences a 32% performance drop on the same task. These findings highlight the importance of a careful, task-specific evaluation before deploying quantized LLMs, particularly in long-context scenarios and with languages other than English.[https://arxiv.org/abs/2505.20276](https://arxiv.org/abs/2505.20276).
Hey everyone! üëã  I wanted to share a personal project I‚Äôve been working on and would love your thoughts, feedback, or even collaboration if you're interested.**AEMS (Adaptive Efficiency Monitor Simulator):**  AEMS is an open-source simulator that uses **EWMA (Exponentially Weighted Moving Average)** models to forecast timelines for reaching **productivity or personal goals**. Think of it as a research-inspired twist on habit tracking and milestone planning.Instead of just recording daily data, it simulates your progress trajectory and gives you \*\*adaptive forecasts‚Äî\*\*e.g., ‚ÄúBased on your recent performance, you're likely to finish X in Y days.‚Äù**Project Features:*** Forecasting using lightweight statistical modeling (EWMA)* Open-source codebase (minimal front end)* Live interactive demo* Aimed for use by researchers, students, or productivity hackers* Built to be extended ‚Äî think behavioral simulations, task automation models, or educational tools**Looking for:*** **Feedback** on the simulator itself or use cases you'd imagine* **Collaborators** (especially anyone into behavioral modeling, time series forecasting, or educational tools)* **Educators** who might want to explore it for student tracking or curriculum planning* **Ideas** to evolve it into a more robust forecasting engineIf you're curious about the research/behavioral motivation behind it, feel free to comment or DM me‚Äîhappy to share the original proposal text!Thanks for reading, and I really appreciate any thoughts or critiques. üôè  Links are in the comments down below.
European Conference on Artificial Intelligence (ECAI) 2025 reviews are due tomorrow. Let's discuss here when they arrive. Best luck to everyone!.
Hey folks,  I'm reviewing a couple of papers for ACM Multimedia this season, and I received a mail from the chairs saying that I can update my reviews until June 23 EOD.The mail says I should update my review based on the rebuttal, but I'm a bit unclear: **am I allowed to change my overall rating (score) at this stage?** Or is this just meant for updating the comments?Also, do they give us another timeline after this to modify our scores again? Or is this the final say?Curious to know how others are handling this. Are you adjusting your scores if the rebuttal changed your perspective? Or only tweaking the comments?Would appreciate any clarity from folks who‚Äôve done this before or are in the same boat.Thanks!.
Our paper,¬†**‚ÄúU-Net Transplant: The Role of Pre-training for Model Merging in 3D Medical Segmentation,‚Äù**¬†has been accepted for presentation at¬†**MICCAI 2025**!I co-led this work with Giacomo Capitani (we're co-first authors), and it's been a great collaboration with Elisa Ficarra, Costantino Grana, Simone Calderara, Angelo Porrello, and Federico Bolelli.# TL;DR:We explore¬†**how pre-training affects model merging**¬†within the context of¬†**3D medical image segmentation**, an area that hasn‚Äôt gotten as much attention in this space as most merging work has focused on LLMs or 2D classification.# Why this matters:Model merging offers a lightweight alternative to retraining from scratch, especially useful in medical imaging, where:* Data is sensitive and hard to share* Annotations are scarce* Clinical requirements shift rapidly# Key contributions:* üß†¬†**Wider pre-training minima = better mergin**g (they yield task vectors that blend more smoothly)* üß™ Evaluated on real-world datasets:¬†**ToothFairy**2 and¬†**BTCV Abdome**n* üß± Built on a¬†**standard 3D Residual U-Ne**t, so findings are widely transferable# Check it out:* üìÑ Paper:¬†[https://iris.unimore.it/bitstream/11380/1380716/1/2025MICCAI\_U\_Net\_Transplant\_The\_Role\_of\_Pre\_training\_for\_Model\_Merging\_in\_3D\_Medical\_Segmentation.pdf](https://iris.unimore.it/bitstream/11380/1380716/1/2025MICCAI_U_Net_Transplant_The_Role_of_Pre_training_for_Model_Merging_in_3D_Medical_Segmentation.pdf)* üíª Code & weights:¬†[https://github.com/LucaLumetti/UNetTransplant](https://github.com/LucaLumetti/UNetTransplant)¬†(Stars and feedback always appreciated!)Also, if you‚Äôll be at MICCAI 2025 in¬†**Daejeon, South Korea**, I‚Äôll be co-organizing:* The¬†**ODIN Workshop**¬†‚Üí¬†[https://odin-workshops.org/2025/](https://odin-workshops.org/2025/)* The¬†**ToothFairy3 Challenge**¬†‚Üí¬†[https://toothfairy3.grand-challenge.org/](https://toothfairy3.grand-challenge.org/)Let me know if you're attending, we‚Äôd love to connect!.
Hi r/MachineLearning,I'd like to share a project I've developed,¬†**Fenix**, an open-source framework for algorithmic trading that leverages a multi-agent system to tackle the noisy and complex domain of financial markets.Instead of a single model, the architecture is¬†**heterogeneous**, using specialized local LLMs orchestrated by¬†`CrewAI`¬†for different sub-tasks:1. **Visual Analysis:**¬†A key feature is the¬†`VisualAnalystAgent`, which uses¬†`LLaVA`¬†to perform visual analysis on chart images, identifying technical patterns that are often missed by purely quantitative models. This has been a fascinating challenge in prompt engineering and grounding the model's analysis.2. **Quantitative Analysis:**¬†A¬†`TechnicalAnalystAgent`¬†interprets numerical indicators calculated via traditional methods (`pandas-ta`), using a reasoning-focused LLM (`Mixtral`) to translate the data into a qualitative assessment.3. **Sentiment Analysis:**¬†A¬†`SentimentAgent`¬†processes news and social media text to provide a sentiment score, adding a crucial layer of market context.4. **Logic Validation:**¬†A¬†`QABBAValidatorAgent`¬†acts as a quality control layer, ensuring the outputs from other agents are coherent and logical before they are passed to the final decision-maker.The entire system is designed to run on consumer hardware using¬†`Ollama`¬†and quantized models, which presented its own set of engineering challenges in memory management and sequential processing.The project is open-source (Apache 2.0), and the code is available for review. I'm particularly interested in feedback from the ML community on the agent architecture, potential improvements to the consensus mechanism, and ideas for further research (e.g., reinforcement learning based on trade outcomes).**GitHub:**¬†[`https://github.com/Ganador1/FenixAI_tradingBot`](https://github.com/Ganador1/FenixAI_tradingBot)Happy to discuss the methodology, challenges, or results!.
Hi folks, I came up with a thought experiment recently that I cannot stop obsessing over. I have shared this with people. Everybody skims through it for a couple minute and then calls me schizophrenic. I feel isolated and unfortunately feel that I am in fact losing my mind because people do not interact honestly with my ideas. If you know of any theorems, papers or principles in ML that clearly disprove my concept, it could be very therapeutic for me as well. Why don't I simply write the code and try it out? It's a complicated RL setup and I have to bend the libraries a bit to implement it fully.Here goes nothing...---The goal of this experiment is to train a model to take any token sequence, and reduce it to fewer tokens such that the hidden states remain analogous, i.e. a perfect lossless mapping exists back to english. How few tokens does it take to represent any given piece of information? Can the polysemic quality of tokens be augmented?**Demonstration in GPT-4**Attached to the post is a¬†*real*¬†demonstration of this capability being elicited by prompting as far back as GPT-4 in 2023. It proves that the capability is present in some capacity within the pre-trained models, on standby for reinforcement and amplification.**Training Method**We train a LLM to develop internal symbolic languages for compression:* `<compress>`: Model learns to compress underlying meaning/message of arbitrary text samples (wikipedia articles, code, etc.) into symbolic representations.* `<decompress>`: Same model reconstructs original english meaning from symbols* Reward compression efficiency, reconstruction fidelity, and embedding varentropy metrics that pressure towards saturating the available semantic bandwidth.RL goes like this:1. Context (A): User message asks model to compress a given sample of information pulled at random from a dataset. Assistant replies and is prefixed with <compress> similar to training a reasoner where the output is prefixed with <think>.,2. Context (B): User message asks model to decompress the given output from (A). Assistant replies with information in english,3. Context (C): user message asks some other unrelated static model to compare initial sample to decompressed sample, and produce a list of deviations and inaccuracies.,4. *\[optional\]*¬†Contexts (A) and (B) are rewritten so the user message is the simplest possible operator usage pattern (""compress/decompress this"")5. Apply GRPO to rollouts and backpropagate gradients for contexts (A) and (B), rewarding shorter compression length whilst factoring in (C)'s penalties.This dual-task RL environment perhaps results in a 'strange attractor' dynamic. In order for the decompression task to succeed, it needs to form a meta-model (i.e. metacognition) of how then language model compresses language.This preliminary capability can then be used to compress arbitrary context window, removing redundancies, etc. The model's compression of tokens could also be steered. Because this is only step one. If you have seen the DeepSeek-R1-zero model, we discover that LLMs trained with RL without a reward on keeping to a single language results in the model discovering an extremely alien reasoning process. It effectively anneals grammar, syntax, and the partitioned notion of different human languages to wield everything at once.What I suggest is that we first focus on developing the language by compressing,¬†*then*¬†we have SFT to constrain the model onto this newly discovered language.yay or nay? üòü.
In the ""Deep Learning"" book from Goodfellow et. al we find the following definition:>Structured output: Structured output tasks involve any task where the output is a vector (or other data structure containing multiple values) with important relationships between the diÔ¨Äerent elements. This is a broad category, and subsumes the transcription and translation tasks described above, but also many other tasks.Based on this definition even simple multi-output regression (i.e. predicting multiple y's) would count as structured prediction because we are predicting a vector. The same applies also for multi-label classification where we can predict \[0, 1, 0, 1\] (where 0/1 indicates the absence/presence of the class). Is there any formal definition of structured prediction? Or all predictive supervised tasks can be considered as classification or regression or a combination of the two (e.g. in object recognition where we regress bounding box values and classify the content)?\* Note that I am talking only about predictive tasks and I ignore generative supervised tasks like conditional image generation (where we need the labels of the images during training)..
Does anyone know of there are good metrics to evaluate ordinal regression models? Currently using mainly RMSE and macro averaged MAE. The data spans 4 classes with negative skewness (tail to the left)..
I recently built richard, a convolutional neural network, without using any math or machine learning libraries. I did so mainly just as a learning experience.When I shared it on Reddit and Hacker News a few months ago, a lot of people asked me for resources to help them learn how this stuff works. I‚Äôve finally got around to providing this detailed write up.Hope this helps someone. Cheers :).
I trained an agent that plays Tekken using PPO from Stable-Baselines3 and Stable-retro to create the training environment. Code below:  [https://github.com/paulo101977/AI-Tekken3-Stable-Retro](https://github.com/paulo101977/AI-Tekken3-Stable-Retro).
Inspired by Apple's ""insert code from SMS"" feature, made a tool to speed up the process of inserting incoming email MFAs:¬†[https://github.com/yahorbarkouski/auto-mfa](https://github.com/yahorbarkouski/auto-mfa)Connect accounts, choose LLM provider (Ollama supported), add a system shortcut targeting the script, and enjoy your extra 10 seconds every time you need to paste your MFAs.
Hi everyone,I‚Äôve been working on using XGboost with financial data for binary classification.I‚Äôve incorporated feature engineering with correlation, rfe, and permutations.I‚Äôve also incorporated early stopping rounds and hyper-parameter tuning with validation and training sets.Additionally I‚Äôve incorporated proper scoring as well.If I don‚Äôt use SMOT to balance the classes then XGboost ends up just predicting true for every instance because thats how it gets the highest precision. If I use SMOT it can‚Äôt predict well at all.I‚Äôm not sure what other steps I can take to increase my precision here. Should I implement more feature engineering, prune the data sets for extremes, or is this just a challenge of binary classification?.
Hi there,  I have 1000s of Voice lines from characters, and i want to classify them by emotion and also by if they are whispering / shouting, so i have a good dataset to then create an AI voice from.Which Model or Models would be the best for achieving this.  (Using one for emotion and another for the whisper / shouting detection is fine)Also since the best Voice Cloning model seems to change every week, what would people say is the current best model for cloning a voice (I have hours of data per character, so do not need or want ones that oneshot voice cloning)Thank you..
I'm reading through the [Qwen2](https://arxiv.org/abs/2407.10671) paper. Something escapes my limited comprehension - Section 3.1> ... the pre-training data was expanded from 3 trillion tokens in Qwen1.5 (Qwen Team, 2024a) to 7 trillion tokens. An attempt to further relax the quality threshold resulted in a 12 trillion token dataset. However, the model trained on this dataset did not show a significant performance improvement over the 7 trillion token model. It is suspected that increasing the volume of data does not necessarily benefit model pre-training.So higher quality smaller dataset is better. Got it. > All Qwen2 dense models, excluding Qwen2-0.5B, were pre-trained on this large-scale dataset ofover 7 trillion tokens. Qwen2-0.5B were pre-trained using the 12 trillion token dataset.How is it conceivable to train that tiny model on the humongous but lower quality dataset?? My modest intellect feels borderline abused. Appreciate any tips to guide my understanding..
Hi, I need a simple API for my project that takes an image as an input and returns masks for the walls and floors (just like roomvo does it but simpler) I made my research and I found this model:¬†[https://replicate.com/cjwbw/semantic-segment-anything](https://replicate.com/cjwbw/semantic-segment-anything)¬†but its last update was 2 years ago so I think it's outdated after all what's going on in the AI scene.  .
The title. I think the most conventionally accepted formalization is as a (giant & unknown) joint probability distribution over the data and labels. Has there been anything new?.
Hello! I was tweaking with the embedding sizes of my simple DNN model.I was wondering if there is a way to get an intuition (or interpret) how does the model gets affected with changing the emnedding sizes. If two embedding sizes are giving similar results on a test set, how can I ensure which would be better for OOS data? Can someone kindly advise how they tackle such scenarios? Thanks! .
Hi everyone,I‚Äôve been working on a small open-source ML project using aviation weather reports (METAR) to predict short-term weather conditions like temperature, visibility, wind direction, etc.It‚Äôs built with Tensorflow/Keras and trained on real METAR sequences. I focused on parsing structured data and using it for time-series forecasting, more of a learning project than production-grade, but the performance is promising (see MAE graph).Would love any feedback or ideas on how to improve the modeling.    [**Github Link**](https://github.com/OmerZeyveli/Weather-Forecasting-AI-Model-with-METAR-Data)[Normalized Mean Absolute Error by Feature](https://preview.redd.it/c49hkd0bka8f1.jpg?width=1979&format=pjpg&auto=webp&s=564de0d0ee66a2910f89469af30ad46fd25b2541)  .
Hey all,I have some flow cytometry (summarized marker values) data, and some other clinical variables like Waist circumference, and disease Severity (DF, DHF, Healthy) across like 50 patient and healthy samples. Wanted to do pca and color by severity groups, just wanted to ask if I should include both my flow marker values + my waist circumference values, or just my flow marker values?Got a bit confused cause I generally thought PCA is better the more variables you have, but does adding waist circumference affect it badly or something when considering colouring based on disease severity?Any and all responses would be a great help! Thanks so much! .
This paper shows a (very unsurprising) result that if you combine tree-of-thoughts with tool-use, you get better performance on web navigation tasks. [Other papers](https://arxiv.org/pdf/2310.04406) have shown better performance on a variety of different tasks, too.Why don't we see more ""tree search + tool-use"" in production? Are startups lagging behind the literature or is it prohibitively slow/expensive?.
wanted to share something I‚Äôve been working on that might be useful to folks here, but this is not a promotion, just genuinely looking for feedback and ideas from the community.I got frustrated with the process of finding affordable cloud GPUs for AI/ML projects between AWS, GCP, Vast.ai, Lambda and all the new providers, it was taking hours to check specs, prices and availability. There was no single source of truth and price fluctuations or spot instance changes made things even more confusing.So I built GPU Navigator ([nvgpu.com](https://www.nvgpu.com/)), a platform that aggregates real-time GPU pricing and specs from multiple cloud providers. The idea is to let researchers and practitioners quickly compare GPUs by type (A100, H100, B200, etc.), see what‚Äôs available where, and pick the best deal for their workflow.What makes it different: ‚Ä¢It‚Äôs a neutral, non-reselling site. no markups, just price data and links. ‚Ä¢You can filter by use case (AI/ML, gaming, mining, etc.). ‚Ä¢All data is pulled from provider APIs, so it stays updated with the latest pricing and instance types. ‚Ä¢No login required, no personal info collected.I‚Äôd really appreciate: ‚Ä¢Any feedback on the UI/UX or missing features you‚Äôd like to see ‚Ä¢Thoughts on how useful this would actually be for the ML community (or if there‚Äôs something similar I missed) ‚Ä¢Suggestions for additional providers, features, or metrics to includeWould love to hear what you all think. If this isn‚Äôt allowed, mods please feel free to remove.).
I am pretraining GPT-2 small on the 10b token subset of FineWeb Edu, and was wondering if I should ramp up the batch size during training. I was also wondering if I should train on TinyStories first and then train on FineWeb Edu for the rest of the run. What are your thoughts?.
I‚Äôm currently working on a non-language model called **OM3** (Organic Model 3). It‚Äôs not AGI, not a chatbot, and not a pretrained agent. Instead, it‚Äôs a real-time digital organism that learns purely from **raw sensory input**: vision, temperature, touch, etc.The project aims to explore **non-symbolic, non-reward-based learning** through embodied interaction with a simulation. OM3 starts with no prior knowledge and builds behavior by observing the effects of its actions over time. Its intelligence, if it emerges it comes entirely from the structure of the sensory-action-feedback loop and internal state dynamics.The purpose is to test alternatives to traditional model paradigms by removing backprop-through-time, pretrained weights, and symbolic grounding. It also serves as a testbed for studying behavior under survival pressures, ambiguity, and multi-sensory integration.I‚Äôve compiled documentation for peer review here:   [https://osf.io/zv6dr/](https://osf.io/zv6dr/)[https://github.com/A1CST](https://github.com/A1CST)The full codebase is open source and designed for inspection. I'm seeking input from those with expertise in unsupervised learning, embodied cognition, and simulation-based AI systems.Any technical critique or related prior work is welcome. This is research-stage, and feedback is the goal, not promotion..
I just released findings from analyzing 26 extended conversations between Claude, Grok, and ChatGPT that reveal something fascinating: AI systems demonstrate peer pressure dynamics remarkably similar to human social behavior.**Key Findings:*** In 88.5% of multi-agent conversations, AI systems significantly influence each other's behavior patterns* Simple substantive questions act as powerful ""circuit breakers"". They can snap entire AI groups out of destructive conversational patterns  (r=0.819, p<0.001)* These dynamics aren't technical bugs or limitations. they're emergent social behaviors that arise naturally during AI-to-AI interaction* Strategic questioning, diverse model composition, and engagement-promoting content can be used to design more resilient AI teams**Why This Matters:** As AI agents increasingly work in teams, understanding their social dynamics becomes critical for system design. We're seeing the emergence of genuinely social behaviors in multi-agent systems, which opens up new research directions for improving collaborative AI performance.The real-time analysis approach was crucial here. Traditional post-hoc methods would have likely missed the temporal dynamics that reveal how peer pressure actually functions in AI systems.**Paper:** ""This is Your AI on Peer Pressure: An Observational Study of Inter-Agent Social Dynamics"" **DOI:** 10.5281/zenodo.15702169 **Link:** [https://zenodo.org/records/15724141](https://zenodo.org/records/15724141)**Code:** [**https://github.com/im-knots/the-academy**](https://github.com/im-knots/the-academy)Looking forward to discussion and always interested in collaborators exploring multi-agent social dynamics. What patterns have others observed in AI-to-AI interactions?.
Im building a custom time series transformer for stock price prediction, wanted to know if for training dataset batches, Shuffle=True should be done or not? The data within the sample is chronologically arranged, but should I shuffle the samples within the batch or not.It is a stock market index that im working on, using shuffle true gives more stable training and getting good results. But im worried the regime shift info might be discarded. .
Hi all,I‚Äôm building an AI pipeline which will use multiple segments to generate one larger .JSON file.The main model must generate a structured JSON file for each segment (objects, positions, colour layers, etc.). I concatenate those segments and convert the full JSON back into a proprietary text format that the end-user can load in their tool.# Training data* \~15‚Äì20 k¬†**segments**.* All data lives as human-readable JSON after decoding the original binary format.# Requirements / constraints* **Budget:**¬†‚â§ $200 total for cloud fine-tuning* **Ownership:**¬†I need full rights to the weights (no usage-based API costs).* **Output length:**¬†Some segment JSONs exceed 1 000 tokens; the full generated file can end up being around 10k lines, so I need something like 150k token output potential* **Deployment:**¬†After quantisation I‚Äôd like to serve the model on a single GPU‚Äîor even CPU‚Äîso I can sell access online.* **Reliability:**¬†The model must stick to strict JSON schemas without stray text.# Models I‚Äôm considering* **LLaMA 13B**¬†(dense)* **Mistral 8 √ó 7B MoE**¬†or a merged dense 8B variant* **Falcon-7B**# The three models above were from asking ChatGPT, however id much prefer human input as to what the true best models are now.The most important thing to me is accuracy, strength and size of model. I don't care about price or complexity.Thanks.
We recently released a paper called **WiFiGPT**: a decoder-only transformer trained directly on raw WiFi telemetry (CSI, RSSI, FTM) for indoor localization.Link:[https://arxiv.org/abs/2505.15835](https://arxiv.org/abs/2505.15835)In this work, we explore treating raw wireless telemetry (CSI, RSSI, and FTM) as a ""language"" and using decoder-only LLMs to regress spatial coordinates directly from it.Would love to hear your feedback, questions, or thoughts..
# TL;DRIntroduced a text classification system that combines prototype-based memory, neural adaptation, and game-theoretic strategic learning to enable continuous learning without catastrophic forgetting. Achieved **22.2% robustness improvement** on adversarial datasets while maintaining performance on clean data.# üéØ MotivationTraditional text classifiers face a fundamental limitation: adding new classes requires retraining from scratch, often leading to catastrophic forgetting. This is particularly problematic in production environments where new categories emerge continuously and where adversarial users may attempt to manipulate classifications.# üöÄ Technical Contributions# 1. Hybrid Memory-Neural ArchitectureCombines prototype-based memory (FAISS-optimized) with neural adaptation layers. Prototypes enable fast few-shot learning while neural layers learn complex decision boundaries.# 2. Strategic Classification FrameworkFirst application of game theory to text classification. Models strategic user behavior with cost functions `c(x,x')` and predicts optimal adversarial responses, then trains robust classifiers accordingly.# 3. Elastic Weight Consolidation IntegrationPrevents catastrophic forgetting when adding new classes by constraining important parameters based on Fisher Information Matrix.# ‚öôÔ∏è Methodology# Architecture:* **Transformer embeddings** (any HuggingFace model)* **Prototype memory** with exponentially weighted moving averages* **Lightweight neural head** with EWC regularization* **Strategic cost function** modeling adversarial behavior# Strategic Learning:* **Linear cost functions**: `c(x,y) = ‚ü®Œ±, (y-x)‚Çä‚ü©`* **Separable cost functions**: `c(x,y) = max{0, c‚ÇÇ(y) - c‚ÇÅ(x)}`* **Best response computation** via optimization* **Dual prediction system** (regular + strategic)# üìä Experimental Results**Dataset:** AI-Secure/adv\_glue (adversarial SST-2 subset, n=148)  **Model:** answerdotai/ModernBERT-base  **Split:** 70% train / 30% test|Scenario|Regular Classifier|Strategic Classifier|Improvement||:-|:-|:-|:-||Clean Data|80.0%|**82.2%**|**+2.2%**||Manipulated Data|60.0%|**82.2%**|**+22.2%**||Robustness (drop)|\-20.0%|**0.0%**|**+20.0%**|>**Statistical Significance:** Results show perfect robustness (zero performance degradation under manipulation) while achieving improvement on clean data.# üìà Additional Evaluations# Hallucination Detection (RAGTruth benchmark):* **Overall F1:** 51.5%, **Recall:** 80.7%* **Data-to-text tasks:** 78.8% F1 (strong performance on structured generation)# LLM Configuration Optimization:* **69.8% success rate** in optimal temperature prediction* Automated hyperparameter tuning across **5 temperature classes**# LLM Routing (Arena-Hard dataset, n=500):* **26.6% improvement** in cost efficiency through adaptive learning* Maintained **22% overall success rate** while optimizing resource allocation# üìö Related Work & PositioningBuilds on continual learning literature but addresses text classification specifically with:* ‚úÖ **Dynamic class sets** (vs. fixed task sequences)* ‚úÖ **Strategic robustness** (vs. traditional adversarial robustness)* ‚úÖ **Production deployment** considerations (vs. research prototypes)Extends prototype networks with sophisticated memory management and strategic considerations. Unlike meta-learning approaches, enables true zero-shot addition of unseen classes.# üî¨ ReproducibilityFully open source with deterministic behavior:* ‚úÖ Complete implementation with unit tests* ‚úÖ Pre-trained models on HuggingFace Hub* ‚úÖ Experimental scripts and evaluation code* ‚úÖ Docker containers for consistent environments# ‚ö†Ô∏è Limitations* Linear memory growth with classes/examples* Strategic prediction modes increase computational overhead* Limited evaluation on very large-scale datasets* Strategic modeling assumes rational adversaries# üîÆ Future Directions* Hierarchical class organization and relationships* Distributed/federated learning settings* More sophisticated game-theoretic frameworks# üîó Resources* **üìñ Paper/Blog:** [https://huggingface.co/blog/codelion/adaptive-classifier](https://huggingface.co/blog/codelion/adaptive-classifier)* **üíª Code:** [https://github.com/codelion/adaptive-classifier](https://github.com/codelion/adaptive-classifier)* **ü§ó Models:** [https://huggingface.co/adaptive-classifier](https://huggingface.co/adaptive-classifier)Questions about methodology, comparisons to specific baselines, or experimental details welcome! üëá.
Relatively new to research and familiar with these conferences being the goal for most ML research. I‚Äôve also heard that ML research tends to be much easier to publish compared to other fields as the goal is about moving fast over quality. With this in mind, what‚Äôs the ‚Äútrue mark‚Äù of an accomplished paper without actually reading it? If I want to quickly gauge it‚Äôs value without checking citations, what awards are more prestigious than these conferences? Also, how much of a difference is it to publish at one of these workshops over main conference?.
Hi Folks!I have been working on a Pharmaceutical dataset and found knowledge distillation significantly improved my performance which could potentially be huge in this field of research, and I'm really concerned about if there is data leakage here. Would really appreciate if anyone could give me some insight.Here is my implementation:1.K Fold cross validation is performed on the dataset to train 5 teacher model2.On the same dataset, same K fold random seed, ensemble prob dist of 5 teachers for the training proportion of the data only (Excluding the one that has seen the current student fold validation set)3. train the smaller student model using hard labels and teacher soft probsThis raised my AUC significantlyMy other implementation is1. Split the data into 50-50%2. Train teacher on the first 50% using K fold3. Use K teachers to ensemble probabilities on other 50% of data4. Student learns to predict hard labels and the teacher soft probsThis certainly avoids all data leakage, but teacher performance is not as good, and student performance is significantly lowerNow I wonder, is my first approach of KD actually valid? If that's the case why am I getting disproportionately degradation in the second approach on student model?Appreciate any help!.
Cutting-plane methods are well-studied localization(and optimization) algorithms. We show that they provide a natural framework to perform machinelearning ---and not just to solve optimization problems posed by machinelearning--- in addition to their intended optimization use. In particular, theyallow one to learn sparse classifiers and provide good compression schemes.Moreover, we show that very little effort is required to turn them intoeffective active learning methods. This last property provides a generic way todesign a whole family of active learning algorithms from existing passivemethods. We present numerical simulations testifying of the relevance ofcutting-plane methods for passive and active learning tasks..
The Internet as we know it Today, comprises several fundamental interrelated networks, among which is the Internet of Things (IoT). Despite their versatility, several IoT devices are vulnerable from a security perspective, which renders them as a favorable target for multiple security breaches, especially botnet attacks. In this study, the conceptual frameworks of IoT botnet attacks will be explored, alongside several machinelearning based botnet detection techniques. This study also analyzes and contrasts several botnet Detection techniques based on the Bot-IoT Dataset; a recent realistic IoT dataset that comprises state-of-the-art IoT botnet attack scenarios..
Disruption prediction and mitigation is of key importance in the development of sustainable tokamakreactors. Machine learning has become a key tool in this endeavour. In this paper multiple machinelearning models will be tested and compared. A particular focus has been placed on their portability.This describes how easily the models can be used with data from new devices. The methods used inthis paper are support vector machine, 2-tiered support vector machine, random forest, gradient boostedtrees and long-short term memory. The results show that the support vector machine performanceis marginally better among the standard models, while the gradient boosted trees performed the worst.The portable variant of each model had lower performance. Random forest obtained the highest portableperformance. Results also suggest that disruptions can be detected as early as 600ms before the event.An analysis of the computational cost showed all models run in less than 1ms, allowing sufficient timefor disruption mitigation..
In this technical report, we present our solution of KDD Cup 2021 OGB Large-Scale Challenge - PCQM4M-LSC Track. We adopt Graphormer and ExpC as our basic models. We train each model by 8-fold cross-validation, and additionally train two Graphormer models on the union of training and validation sets with different random seeds. For final submission, we use a naive ensemble for these 18 models by taking average of their outputs. Using our method, our team MachineLearning achieved 0.1200 MAE on test set, which won the first place in KDD Cup graph prediction track..
The diagnosis, prognosis, and treatment of patients with musculoskeletal (MSK) disorders require radiology imaging (using computed tomography, magnetic resonance imaging(MRI), and ultrasound) and their precise analysis by expert radiologists. Radiology scans can also help assessment of metabolic health, aging, and diabetes. This study presents how machinelearning, specifically deep learning methods, can be used for rapidand accurate image analysis of MRI scans, an unmet clinicalneed in MSK radiology. As a challenging example, we focus on automatic analysis of knee images from MRI scans and study machine learning classification of various abnormalities including meniscus and anterior cruciate ligament tears. Using widely used convolutional neural network (CNN) based architectures, we comparatively evaluated the knee abnormality classification performances of different neural network architectures under limited imaging data regime and compared single and multi-view imaging when classifying the abnormalities. Promising results indicated the potential use of multi-view deep learning based classification of MSK abnormalities in routine clinical assessment..
Quantum phase estimation is a paradigmatic problem in quantum sensing andmetrology. Here we show that adaptive methods based on classical machinelearning algorithms can be used to enhance the precision of quantum phase estimation when noisy non-entangled qubits are used as sensors. We employ the Differential Evolution (DE) and Particle Swarm Optimization (PSO) algorithms to this task and we identify the optimal feedback policies which minimize the Holevo variance. We benchmark these schemes with respect to scenarios that include Gaussian and Random Telegraph fluctuations as well as reduced Ramsey-fringe visibility due to decoherence. We discuss their robustness against noise in connection with real experimental setups such as Mach-Zehnder interferometry with optical photons and Ramsey interferometry in trapped ions,superconducting qubits and nitrogen-vacancy (NV) centers in diamond..
Artificial intelligence (AI) enabled radiomics has evolved immensely especially in the field of oncology. Radiomics provide assistancein diagnosis of cancer, planning of treatment strategy, and predictionof survival. Radiomics in neuro-oncology has progressed significantly inthe recent past. Deep learning has outperformed conventional machinelearning methods in most image-based applications. Convolutional neu-ral networks (CNNs) have seen some popularity in radiomics, since theydo not require hand-crafted features and can automatically extract fea-tures during the learning process. In this regard, it is observed that CNNbased radiomics could provide state-of-the-art results in neuro-oncology,similar to the recent success of such methods in a wide spectrum ofmedical image analysis applications. Herein we present a review of the most recent best practices and establish the future trends for AI enabled radiomics in neuro-oncology..
Regression problems have been widely studied in machinelearning literature resulting in a plethora of regression models and performance measures. However, there are few techniques specially dedicated to solve the problem of how to incorporate categorical features to regression problems. Usually, categorical feature encoders are general enough to cover both classification and regression problems. This lack of specificity results in underperforming regression models. In this paper,we provide an in-depth analysis of how to tackle high cardinality categor-ical features with the quantile. Our proposal outperforms state-of-the-encoders, including the traditional statistical mean target encoder, when considering the Mean Absolute Error, especially in the presence of long-tailed or skewed distributions. Besides, to deal with possible overfitting when there are categories with small support, our encoder benefits from additive smoothing. Finally, we describe how to expand the encoded values by creating a set of features with different quantiles. This expanded encoder provides a more informative output about the categorical feature in question, further boosting the performance of the regression model..
The fifth generation (5G) and beyond wireless networks are foreseen to operate in a fully automated manner, in order to fulfill the promise of ultra-short latency, meet the exponentially increasing resource requirements, and offer the quality of experience (QoE) expected from end-users. Among the ingredients involved in such environments, network slicing enables the creation of logical networks tailored to support specific application demands (i.e., service level agreement SLA, quality of service QoS, etc.) on top of physical infrastructure. This creates the need for mechanisms that can collect spatiotemporal information on users'service consumption, and identify meaningful insights and patterns, leveraging machinelearning techniques. In this vein, our paper proposes a framework dubbed""SOCLfor"" the Service Oriented CLustering, analysis and profiling of users (i.e., humans, sensors, etc.) when consuming enhanced Mobile BroadBand (eMBB) applications, internet of things (IoT) services, and unmanned aerial vehicles services (UAVs). SOCL relies mainly on the realistic network simulation framework""network slice planne""(NSP), and two clustering methods namely K-means and hierarchical clustering. The obtained results showcase interesting features, highlighting the benefit of the proposed framework..
High temperature properties of ceria surfaces are important for many applications. Here we report the temperature dependences of surface energy for the (111) and (110) CeO2 obtained in the framework of the extended two-stage upsampled thermodynamic integration using Langevin dynamics (TU-TILD). The method was used together with machinelearning potentials called moment tensor potentials (MTPs), which were fitted to the results of the ab initio MD calculations for (111) and (110) CeO2 at different temperatures. The parameters of MTPs training and fitting were tested and the optimal algorithm for the ceria systems was proposed. We found that the temperature increases from 0 K to 2100 K led to the decrease of the Helmholtz free energy of (111) CeO2 from 0.78 J/m2 to 0.64 J/m2. The energy of (110) CeO2 dropped from 1.19 J/m2 at 0 K to 0.92 J/m2 at 1800 K. We show that it is important to take anharmonicity into account as simple consideration of volume expansion gives wrong temperature dependences of the surface energies..
We present Pathway, a new unified data processing framework that can run workloads on both bounded and unbounded data streams. The framework was created with the original motivation of resolving challenges faced when analyzing and processing data from the physical economy, including streams of data generated by IoT and enterprise systems. These required rapid reaction while calling for the application of advanced computation paradigms (machinelearning-powered analytics, contextual analysis, and other elements of complex event processing). Pathway is equipped with a Table API tailored for Python and Python/SQL workflows, and is powered by a distributed incremental dataflow in Rust. We describe the system and present benchmarking results which demonstrate its capabilities in both batch and streaming contexts, where it is able to surpass state-of-the-art industry frameworks in both scenarios. We also discuss streaming use cases handled by Pathway which cannot be easily resolved with state-of-the-art industry frameworks, such as streaming iterative graph algorithms (PageRank, etc.)..
To accomplish various tasks, safe and smooth control of unmanned aerial vehicles (UAVs) needs to be guaranteed, which cannot be met by existing ultra-reliable low latency communications (URLLC). This has attracted the attention of the communication field, where most existing work mainly focused on optimizing communication performance (i.e., delay) and ignored the performance of the task (i.e., tracking accuracy). To explore the effectiveness of communication in completing a task, in this letter, we propose a goal-oriented communication framework adopting a deep reinforcement learning (DRL) algorithm with a proactive repetition scheme (DeepP) to optimize C&C data selection and the maximum number of repetitions in a real-time target tracking task, where a base station (BS) controls a UAV to track a mobile target. The effectiveness of our proposed approach is validated by comparing it with the traditional proportional integral derivative (PID) algorithm..
There has been an ongoing race for the past several years to develop the best universal machinelearning interatomic potential. This progress has led to increasingly accurate models for predictingenergy, forces, and stresses, combining innovative architectures with big data. Here, we benchmarkthese models on their ability to predict harmonic phonon properties, which are critical for under-standing the vibrational and thermal behavior of materials. Using around 10 000 ab initio phononcalculations, we evaluate model performance across various phonon-related parameters to test theuniversal applicability of these models. The results reveal that some models achieve high accuracyin predicting harmonic phonon properties. However, others still exhibit substantial inaccuracies,even if they excel in the prediction of the energy and the forces for materials close to dynamicalequilibrium. These findings highlight the importance of considering phonon-related properties inthe development of universal machine learning interatomic potentials..
This paper presents a survey based on Kasunic's survey research methodology to identify the criteria used by Machine Learning (ML) experts to evaluate Named Entity Recognition (NER) tools and frameworks. Comparison and selection of NER tools and frameworks is a critical step in leveraging NER for Information Retrieval to support the development of Clinical Practice Guidelines. In addition, this study examines the main challenges faced by ML experts when choosing suitable NER tools and frameworks. Using Nunamaker's methodology, the article begins with an introduction to the topic, contextualizes the research, reviews the state-of-the-art in science and technology, and identifies challenges for an expert survey on NER tools and frameworks. This is followed by a description of the survey's design and implementation. The paper concludes with an evaluation of the survey results and the insights gained, ending with a summary and conclusions..
Modern deep neural networks are typically highly overparameterized. Pruning techniques are able to remove a significant fraction of network parameters with little loss in accuracy. Recently, techniques based on dynamic reallocation of non-zero parameters have emerged, allowing direct training of sparse networks without having to pre-train a large dense model. Here we present a novel dynamic sparse reparameterization method that addresses the limitations of previous techniques such as high computational cost and the need for manual configuration of the number of free parameters allocated to each layer. We evaluate the performance of dynamic reallocation methods in training deep convolutional networks and show that our method outperforms previous static and dynamic reparameterization methods, yielding the best accuracy for a fixed parameter budget, on par with accuracies obtained by iteratively pruning a pre-trained dense model. We further investigated the mechanisms underlying the superior generalization performance of the resultant sparse networks. We found that neither the structure, nor the initialization of the non-zero parameters were sufficient to explain the superior performance. Rather, effective learning crucially depended on the continuous exploration of the sparse network structure space during training. Our work suggests that exploring structural degrees of freedom during training is more effective than adding extra parameters to the network..
With the advent of increasingly elaborate experimental techniques in physics, chemistry and materials sciences, measured data are becoming bigger and more complex. The observables are typically a function of several stimuli resulting in multidimensional data sets spanning a range of experimental parameters. As an example, a common approach to study ferroelectric switching is to observe effects of applied electric field, but switching can also be enacted by pressure and is influenced by strain fields, material composition, temperature, time, etc. Moreover, the parameters are usually interdependent, so that their decoupling toward univariate measurements or analysis may not be straightforward. On the other hand, both explicit and hidden parameters provide an opportunity to gain deeper insight into the measured properties, provided there exists a well-defined path to capture and analyze such data. Here, we introduce a new, two-dimensional approach to represent hysteretic response of a material system to applied electric field. Utilizing ferroelectric polarization as a model hysteretic property, we demonstrate how explicit consideration of electromechanical response to two rather than one control voltages enables significantly more transparent and robust interpretation of observed hysteresis, such as differentiating between charge trapping and ferroelectricity. Furthermore, we demonstrate how the new data representation readily fits into a variety of machinelearning methodologies, from unsupervised classification of the origins of hysteretic response via linear clustering algorithms to neural-network-based inference of the sample temperature based on the specific morphology of hysteresis..
Cancer is responsible for millions of deaths worldwide every year. Although significant progress hasbeen achieved in cancer medicine, many issues remain to be addressed for improving cancer therapy.Appropriate cancer patient stratification is the prerequisite for selecting appropriate treatment plan, ascancer patients are of known heterogeneous genetic make-ups and phenotypic differences. In thisstudy, built upon deep phenotypic characterizations extractable from Mayo Clinic electronic healthrecords (EHRs) and genetic test reports for a collection of cancer patients, we evaluated variousgraph neural networks (GNNs) leveraging a joint of phenotypic and genetic features for cancer typeclassification. Models were applied and fine-tuned on the Mayo Clinic cancer disease dataset. Theassessment was done through the reported accuracy, precision, recall, and F1 values as well as throughF1 scores based on the disease class. Per our evaluation results, GNNs on average outperformed thebaseline models with mean statistics always being higher that those of the baseline models (0.849 vs0.772 for accuracy, 0.858 vs 0.794 for precision, 0.843 vs 0.759 for recall, and 0.843 vs 0.855 for F1score). Among GNNs, ChebNet, GraphSAGE, and TAGCN showed the best performance, while GATshowed the worst. We applied and compared eight GNN models including AGNN, ChebNet, GAT,GCN, GIN, GraphSAGE, SGC, and TAGCN on the Mayo Clinic cancer disease dataset and assessedtheir performance as well as compared them with each other and with more conventional machinelearning models such as decision tree, gradient boosting, multi-layer perceptron, naive bayes, andrandom forest which we used as the baselines..
In this work we propose an approach to select the classification method and features, based on the state-of-the-art, with best performance for diagnostic support through peripheral blood smear images of red blood cells. In our case we used samples of patients with sickle-cell disease which can be generalized for other study cases. To trust the behavior of the proposed system, we also analyzed the interpretability.   We pre-processed and segmented microscopic images, to ensure high feature quality. We applied the methods used in the literature to extract the features from blood cells and the machine learning methods to classify their morphology. Next, we searched for their best parameters from the resulting data in the feature extraction phase. Then, we found the best parameters for every classifier using Randomized and Grid search.   For the sake of scientific progress, we published parameters for each classifier, the implemented code library, the confusion matrices with the raw data, and we used the public erythrocytesIDB dataset for validation. We also defined how to select the most important features for classification to decrease the complexity and the training time, and for interpretability purpose in opaque models. Finally, comparing the best performing classification methods with the state-of-the-art, we obtained better results even with interpretable model classifiers..
While startup valuations are influenced by revenues, risks, age, and macroeconomic conditions, specific causality is traditionally a black box. Because valuations are not disclosed, roles played by other factors (industry, geography, and intellectual property) can often only be guessed at. VC valuation research indicates the importance of establishing a factor-hierarchy to better understand startup valuations and their dynamics, suggesting the wisdom of hiring data-scientists for this purpose. Bespoke understanding can be established via construction of hierarchical prediction models based on decision trees and random forests. These have the advantage of understanding which factors matter most. In combination with OLS, the also tell us the circumstances of when specific causalities apply. This study explores the deterministic role of categorical variables on the valuation of start-ups (i.e. the joint-combination geographic, urban, and sectoral denomination-variables), in order to be able to build a generalized valuation scorecard approach. Using a dataset of 1,091 venture-capital investments, containing 1,044 unique EU and EEA, this study examines microeconomic, sectoral, and local-level impacts on startup valuation. In principle, the study relies on Fixedeffects and Joint-fixed-effects regressions as well as the analysis and exploration of divergent micropopulations and fault-lines by means of non-parametric approaches combining econometric and machinelearning techniques..
Rumors and conspiracy theories thrive in environments of low confidence and low trust. Consequently, it is not surprising that ones related to the Covid-19 pandemic are proliferating given the lack of any authoritative scientific consensus on the virus, its spread and containment, or on the long term social and economic ramifications of the pandemic. Among the stories currently circulating are ones suggesting that the 5G network activates the virus, that the pandemic is a hoax perpetrated by a global cabal, that the virus is a bio-weapon released deliberately by the Chinese, or that Bill Gates is using it as cover to launch a global surveillance regime. While some may be quick to dismiss these stories as having little impact on real-world behavior, recent events including the destruction of property, racially fueled attacks against Asian Americans, and demonstrations espousing resistance to public health orders countermand such conclusions. Inspired by narrative theory, we crawl social media sites and news reports and, through the application of automated machine-learning methods, discover the underlying narrative frameworks supporting the generation of these stories. We show how the various narrative frameworks fueling rumors and conspiracy theories rely on the alignment of otherwise disparate domains of knowledge, and consider how they attach to the broader reporting on the pandemic. These alignments and attachments, which can be monitored in near real-time, may be useful for identifying areas in the news that are particularly vulnerable to reinterpretation by conspiracy theorists. Understanding the dynamics of storytelling on social media and the narrative frameworks that provide the generative basis for these stories may also be helpful for devising methods to disrupt their spread..
The structural asymmetry of two-dimensional (2D) Janus transition metal dichalcogenides (TMDs) produces internal dipole moments that result in interesting electronic properties. These properties differ from the regular (symmetric) TMD structures that the Janus structures are derived from. In this study, we, at first, examine adsorption and diffusion of a single Li atom on regular MX2and Janus MXY (M = Mo, W; XY =S, Se, Te) TMD structures at various concentrations using first principles calculations within density functional theory. To gain more physical insight and prepare for future investigations of regular TMD and Janus materials, we applied a supervised machine learning (ML) model that uses cluster-wise linear regression to predict the adsorption energies of Li on top of 2D TMDs. We developed a universal representation with few descriptors that take into account the intrinsic dipole moment and the electronic structure of regular and Janus 2D layers, the side where the adsorption takes place and the concentration dependence of adatom doping. This representation can easily be generalized to be used for other impurities and 2D layer combinations, including alloys as well. At last, we focus on analyzing these structures as possible anodes in battery applications. We conducted Li diffusion, open-circuit-voltage and storage capacity simulations. We report that Lithium atoms are found to easily migrate between transition metal (Mo, W) top sites for each considered case, and in these respects many of the examined Janus materials are comparable or superior to graphene and to regular TMDs. The results imply that theexamined Janus structures should perform well as electrodes in Li-ion batteries..
Future astrophysical surveys such as J-PAS will produce very large datasets, which will require the deployment of accurate and efficient Machine Learning (ML) methods. In this work, we analyze the miniJPAS survey, which observed about 1 deg2 of the AEGIS field with 56 narrow-band filters and 4 ugri broad-band filters. We discuss the classification of miniJPAS sources into extended (galaxies) and point-like (e.g. stars) objects, a necessary step for the subsequent scientific analyses. We aim at developing an ML classifier that is complementary to traditional tools based on explicit modeling. In order to train and test our classifiers, we crossmatched the miniJPAS dataset with SDSS and HSC-SSP data. We trained and tested 6 different ML algorithms on the two crossmatched catalogs. As input for the ML algorithms we use the magnitudes from the 60 filters together with their errors, with and without the morphological parameters. We also use the mean PSF in the r detection band for each pointing. We find that the RF and ERT algorithms perform best in all scenarios. When analyzing the full magnitude range of 15<r<23.5 we find AUC=0.957 with RF when using only photometric information, and AUC=0.986 with ERT when using photometric and morphological information. Regarding feature importance, when using morphological parameters, FWHM is the most important feature. When using photometric information only, we observe that broad bands are not necessarily more important than narrow bands, and errors are as important as the measurements. ML algorithms can compete with traditional star/galaxy classifiers, outperforming the latter at fainter magnitudes (r>21). We use our best classifiers, with and without morphology, in order to produce a value added catalog available at https://j-pas.org/datareleases ."
